
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../streamers/">
      
      
        <link rel="next" href="../../models/">
      
      
      <link rel="icon" href="../../../../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.44">
    
    
      
        <title>utils - MindNLP Docs</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mindnlp.transformers.generation.utils" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../../../" title="MindNLP Docs" class="md-header__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindNLP Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              utils
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="选择当前语言">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="../../../../../api/transformers/generation/utils/" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="./" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="分享" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindnlp" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../" class="md-tabs__link">
        
  
    
  
  主页

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../tutorials/quick_start/" class="md-tabs__link">
          
  
    
  
  教程

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../supported_models/" class="md-tabs__link">
        
  
    
  
  模型列表

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../contribute/" class="md-tabs__link">
        
  
    
  
  代码贡献

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../accelerate/" class="md-tabs__link">
          
  
    
  
  API文档

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../notes/changelog/" class="md-tabs__link">
          
  
    
  
  说明

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../" title="MindNLP Docs" class="md-nav__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    MindNLP Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindnlp" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    主页
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    教程
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            教程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/quick_start/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/data_preprocess/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Preprocess
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_mirror/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Mirror
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../supported_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    模型列表
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../contribute/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    代码贡献
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    API文档
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            API文档
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../accelerate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Accelerate
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../dataset/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Dataset
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            Dataset
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/load_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    load_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/BaseMapFunction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BaseMapFunction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/transforms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transforms
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" >
        
          
          <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Engine
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            Engine
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_1" >
        
          
          <label class="md-nav__link" for="__nav_5_4_1" id="__nav_5_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    train_args
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_1">
            <span class="md-nav__icon md-icon"></span>
            train_args
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/seq2seq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seq2seq
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_2" >
        
          
          <label class="md-nav__link" for="__nav_5_4_2" id="__nav_5_4_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_2">
            <span class="md-nav__icon md-icon"></span>
            trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/default_func/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    default_func
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/callbacks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    callbacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    export
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../modules/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modules
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../parallel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parallel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../peft/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_7" id="__nav_5_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7">
            <span class="md-nav__icon md-icon"></span>
            PEFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_2" >
        
          
          <label class="md-nav__link" for="__nav_5_7_2" id="__nav_5_7_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    tuners
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_2">
            <span class="md-nav__icon md-icon"></span>
            tuners
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adalora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AdaLoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adaption_prompt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adaption_Prompt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/ia3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IA3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lokr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoKr
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/prompt_tuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt tuning
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_3" >
        
          
          <label class="md-nav__link" for="__nav_5_7_3" id="__nav_5_7_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_3">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/utils/merge_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    merge_utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/mapping/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mapping
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/peft_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    peft_model
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sentence/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sentence
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Transformers
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9" id="__nav_5_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_9_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_2" checked>
        
          
          <label class="md-nav__link" for="__nav_5_9_2" id="__nav_5_9_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9_2">
            <span class="md-nav__icon md-icon"></span>
            generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../api/transforemrs/generation/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../beam_constraints/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_constraints
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../beam_search/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_search
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../logits_process/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    logits_process
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stopping_criteria/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stopping_criteria
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../streamers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    streamers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    utils
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils" class="md-nav__link">
    <span class="md-ellipsis">
      utils
    </span>
  </a>
  
    <nav class="md-nav" aria-label="utils">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.GenerateBeamDecoderOnlyOutput" class="md-nav__link">
    <span class="md-ellipsis">
      GenerateBeamDecoderOnlyOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.GenerateBeamEncoderDecoderOutput" class="md-nav__link">
    <span class="md-ellipsis">
      GenerateBeamEncoderDecoderOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.GenerateDecoderOnlyOutput" class="md-nav__link">
    <span class="md-ellipsis">
      GenerateDecoderOnlyOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.GenerateEncoderDecoderOutput" class="md-nav__link">
    <span class="md-ellipsis">
      GenerateEncoderDecoderOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.GenerationMixin" class="md-nav__link">
    <span class="md-ellipsis">
      GenerationMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GenerationMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.GenerationMixin.compute_transition_scores" class="md-nav__link">
    <span class="md-ellipsis">
      compute_transition_scores
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.GenerationMixin.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.GenerationMixin.heal_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      heal_tokens
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.stack_model_outputs" class="md-nav__link">
    <span class="md-ellipsis">
      stack_model_outputs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../models/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    models
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9_3" id="__nav_5_9_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_3">
            <span class="md-nav__icon md-icon"></span>
            models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/albert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    albert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/align/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    align
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/altclip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    altclip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/audio_spectrogram_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    audio_spectrogram_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/auto/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    auto
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    autoformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/baichuan/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    baichuan
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bark/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bark
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/barthez/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    barthez
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bartpho/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bartpho
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/beit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bert_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bert_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bertweet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bertweet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bge_m3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bge_m3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/big_bird/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    big_bird
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bigbird_pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bigbird_pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/biogpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    biogpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/blenderbot/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/blenderbot_small/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot_small
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/blip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/blip_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip_2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bloom/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bloom
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bridgetower/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bridgetower
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/bros/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bros
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/byt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    byt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/camembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    camembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/canine/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    canine
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/chatglm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/chatglm2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/chatglm3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/codegen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    codegen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cogvlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cogvlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cohere/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cohere
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/convbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/convnext/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convnext
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cpmant/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmant
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cpmbee/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmbee
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/ctrl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ctrl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cvt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cvt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/data2vec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    data2vec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/deberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/deberta_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/decision_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    decision_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/distilbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    distilbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/efficientformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/efficientnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/electra/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    electra
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/encodec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    encodec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/ernie/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/ernie_m/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie_m
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/esm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    esm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/falcon/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    falcon
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/flava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    flava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/funnel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    funnel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gemma/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gemma
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/git/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    git
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gpt_bigcode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_bigcode
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gpt_neo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gpt_neox/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gpt_neox_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gpt_pangu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_pangu
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/gptj/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gptj
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/graphormer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    graphormer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/groupvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    groupvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/hubert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    hubert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/imagegpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    imagegpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/internlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    internlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/jamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/jetmoe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jetmoe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/layoutlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/layoutlmv2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlmv2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/led/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    led
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/llama/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llama
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/llava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/llava_next/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava_next
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/longformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/longt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/luke/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    luke
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/marian/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    marian
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/maskformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    maskformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mbart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mbart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/megatron_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/megatron_gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/minicpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minicpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/minigpt4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minigpt4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mistral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mistral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mixtral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mixtral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mobilebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mobilevit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilevit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/moss/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    moss
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mpnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/musicgen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/musicgen_melody/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen_melody
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mvp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mvp
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/nezha/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nezha
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/nystromformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nystromformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/olmo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    olmo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/openelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    openelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/opt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    opt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/owlvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    owlvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/phi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/phi3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/poolformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    poolformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/pop2piano/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pop2piano
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/qwen2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/qwen2_moe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2_moe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/reformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    reformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/regnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    regnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/rembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/resnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    resnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/roc_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roc_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/rwkv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rwkv
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/sam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/seamless_m4t/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/seamless_m4t_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/segformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    segformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/seggpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seggpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/speech_encoder_decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_encoder_decoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/speech_to_text/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_to_text
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/squeezebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    squeezebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/stablelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stablelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/starcoder2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    starcoder2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/swiftformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    swiftformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/switch_transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    switch_transformers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/t5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    t5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/table_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    table_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/timesformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    timesformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/tinybert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tinybert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/van/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    van
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/vipllava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vipllava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/vision_text_dual_encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vision_text_dual_encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/visual_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    visual_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/wav2vec2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/wav2vec2_conformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_conformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/wav2vec2_with_lm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_with_lm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/wavlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wavlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/whisper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    whisper
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/x_clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    x_clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/xlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/xlm_roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/xlm_roberta_xl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta_xl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/xlnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlnet
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_4" >
        
          
          <label class="md-nav__link" for="__nav_5_9_4" id="__nav_5_9_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_4">
            <span class="md-nav__icon md-icon"></span>
            pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../api/transforemrs/pipeline/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/automatic_speech_recognition/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    automatic_speech_recognition
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/document_question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    document_question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/fill_mask/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    fill_mask
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text2text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text2text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/zero_shot_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    zero_shot_classification
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../configuration_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    configuration_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modeling_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    modeling_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils_base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils_fast/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_fast
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../trl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TRL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    说明
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            说明
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/changelog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    更新日志
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/code_of_conduct/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    行为准则
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    常见问题
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils" class="md-nav__link">
    <span class="md-ellipsis">
      utils
    </span>
  </a>
  
    <nav class="md-nav" aria-label="utils">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.GenerateBeamDecoderOnlyOutput" class="md-nav__link">
    <span class="md-ellipsis">
      GenerateBeamDecoderOnlyOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.GenerateBeamEncoderDecoderOutput" class="md-nav__link">
    <span class="md-ellipsis">
      GenerateBeamEncoderDecoderOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.GenerateDecoderOnlyOutput" class="md-nav__link">
    <span class="md-ellipsis">
      GenerateDecoderOnlyOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.GenerateEncoderDecoderOutput" class="md-nav__link">
    <span class="md-ellipsis">
      GenerateEncoderDecoderOutput
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.GenerationMixin" class="md-nav__link">
    <span class="md-ellipsis">
      GenerationMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GenerationMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.GenerationMixin.compute_transition_scores" class="md-nav__link">
    <span class="md-ellipsis">
      compute_transition_scores
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.GenerationMixin.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.GenerationMixin.heal_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      heal_tokens
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.generation.utils.stack_model_outputs" class="md-nav__link">
    <span class="md-ellipsis">
      stack_model_outputs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/mindspore-lab/mindnlp/edit/master/docs/en/api/transformers/generation/utils.md" title="编辑此页" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindnlp/raw/master/docs/en/api/transformers/generation/utils.md" title="查看本页的源代码" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


  <h1>utils</h1>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.generation.utils" class="doc doc-heading">
            <code>mindnlp.transformers.generation.utils</code>


<a href="#mindnlp.transformers.generation.utils" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>generation mixin</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.utils.GenerateBeamDecoderOnlyOutput" class="doc doc-heading">
            <code>mindnlp.transformers.generation.utils.GenerateBeamDecoderOnlyOutput</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.transformers.generation.utils.GenerateBeamDecoderOnlyOutput" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.utils.ModelOutput">ModelOutput</span></code></p>


        <p>Outputs of decoder-only generation models, when using beam methods.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sequences</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generated sequences. The second dimension (sequence_length) is either equal to <code>max_length</code> or shorter
if all batches finished early due to the <code>eos_token_id</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sequences_scores</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Final beam scores of the generated <code>sequences</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scores</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Beam transition scores for each vocabulary token at each generation step. Beam transition scores consisting
of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.
Tuple of <code>mindspore.Tensor</code> with up to <code>max_new_tokens</code> elements (one element for each generated token),
with each tensor of shape <code>(batch_size*num_beams, config.vocab_size)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logits</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)
at each generation step. Tuple of <code>mindspore.Tensor</code> with up to <code>max_new_tokens</code> elements (one element for
each generated token), with each tensor of shape <code>(batch_size, config.vocab_size)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>beam_indices</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Beam indices of generated token id at each generation step. <code>mindspore.Tensor</code> of shape
<code>(batch_size*num_return_sequences, sequence_length)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of
<code>mindspore.Tensor</code> of shape <code>(batch_size*num_beams, num_heads, generated_length, sequence_length)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of
<code>mindspore.Tensor</code> of shape <code>(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_key_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>NOTE: some models have a different <code>past_key_values</code> format, confirm with the model's documentation.
Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value
tensor). The first Tuple is of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads,
encoder_sequence_length, embed_size_per_head)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\utils.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">GenerateBeamDecoderOnlyOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Outputs of decoder-only generation models, when using beam methods.</span>

<span class="sd">    Args:</span>
<span class="sd">        sequences (`mindspore.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`):</span>
<span class="sd">            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter</span>
<span class="sd">            if all batches finished early due to the `eos_token_id`.</span>
<span class="sd">        sequences_scores (`mindspore.Tensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):</span>
<span class="sd">            Final beam scores of the generated `sequences`.</span>
<span class="sd">        scores (`tuple(mindspore.Tensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):</span>
<span class="sd">            Beam transition scores for each vocabulary token at each generation step. Beam transition scores consisting</span>
<span class="sd">            of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.</span>
<span class="sd">            Tuple of `mindspore.Tensor` with up to `max_new_tokens` elements (one element for each generated token),</span>
<span class="sd">            with each tensor of shape `(batch_size*num_beams, config.vocab_size)`.</span>
<span class="sd">        logits (`tuple(mindspore.Tensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):</span>
<span class="sd">            Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)</span>
<span class="sd">            at each generation step. Tuple of `mindspore.Tensor` with up to `max_new_tokens` elements (one element for</span>
<span class="sd">            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.</span>
<span class="sd">        beam_indices (`mindspore.Tensor`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):</span>
<span class="sd">            Beam indices of generated token id at each generation step. `mindspore.Tensor` of shape</span>
<span class="sd">            `(batch_size*num_return_sequences, sequence_length)`.</span>
<span class="sd">        attentions (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            `mindspore.Tensor` of shape `(batch_size*num_beams, num_heads, generated_length, sequence_length)`.</span>
<span class="sd">        hidden_states (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            `mindspore.Tensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.</span>
<span class="sd">        past_key_values (`tuple(tuple(mindspore.Tensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>
<span class="sd">            NOTE: some models have a different `past_key_values` format, confirm with the model&#39;s documentation.</span>
<span class="sd">            Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value</span>
<span class="sd">            tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape</span>
<span class="sd">            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if</span>
<span class="sd">            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,</span>
<span class="sd">            encoder_sequence_length, embed_size_per_head)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">sequences</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">sequences_scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">beam_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.utils.GenerateBeamEncoderDecoderOutput" class="doc doc-heading">
            <code>mindnlp.transformers.generation.utils.GenerateBeamEncoderDecoderOutput</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.transformers.generation.utils.GenerateBeamEncoderDecoderOutput" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.utils.ModelOutput">ModelOutput</span></code></p>


        <p>Outputs of encoder-decoder generation models, when using beam methods.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sequences</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generated sequences. The second dimension (sequence_length) is either equal to <code>max_length</code> or shorter
if all batches finished early due to the <code>eos_token_id</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sequences_scores</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Final beam scores of the generated <code>sequences</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scores</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Beam transition scores for each vocabulary token at each generation step. Beam transition scores consisting
of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.
Tuple of <code>mindspore.Tensor</code> with up to <code>max_new_tokens</code> elements (one element for each generated token),
with each tensor of shape <code>(batch_size*num_beams, config.vocab_size)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logits</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)
at each generation step. Tuple of <code>mindspore.Tensor</code> with up to <code>max_new_tokens</code> elements (one element for
each generated token), with each tensor of shape <code>(batch_size, config.vocab_size)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>beam_indices</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Beam indices of generated token id at each generation step. <code>mindspore.Tensor</code> of shape
<code>(batch_size*num_return_sequences, sequence_length)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple of <code>mindspore.Tensor</code> (one for each layer of the decoder) of shape <code>(batch_size, num_heads,
sequence_length, sequence_length)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple of <code>mindspore.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size*num_beams*num_return_sequences, sequence_length, hidden_size)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of
<code>mindspore.Tensor</code> of shape <code>(batch_size*num_beams*num_return_sequences, num_heads, generated_length,
sequence_length)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cross_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of
<code>mindspore.Tensor</code> of shape <code>(batch_size, num_heads, generated_length, sequence_length)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of
<code>mindspore.Tensor</code> of shape <code>(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_key_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>NOTE: some models have a different <code>past_key_values</code> format, confirm with the model's documentation.
Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value
tensor). The first Tuple is of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads,
encoder_sequence_length, embed_size_per_head)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\utils.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">GenerateBeamEncoderDecoderOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Outputs of encoder-decoder generation models, when using beam methods.</span>

<span class="sd">    Args:</span>
<span class="sd">        sequences (`mindspore.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`):</span>
<span class="sd">            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter</span>
<span class="sd">            if all batches finished early due to the `eos_token_id`.</span>
<span class="sd">        sequences_scores (`mindspore.Tensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):</span>
<span class="sd">            Final beam scores of the generated `sequences`.</span>
<span class="sd">        scores (`tuple(mindspore.Tensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):</span>
<span class="sd">            Beam transition scores for each vocabulary token at each generation step. Beam transition scores consisting</span>
<span class="sd">            of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.</span>
<span class="sd">            Tuple of `mindspore.Tensor` with up to `max_new_tokens` elements (one element for each generated token),</span>
<span class="sd">            with each tensor of shape `(batch_size*num_beams, config.vocab_size)`.</span>
<span class="sd">        logits (`tuple(mindspore.Tensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):</span>
<span class="sd">            Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)</span>
<span class="sd">            at each generation step. Tuple of `mindspore.Tensor` with up to `max_new_tokens` elements (one element for</span>
<span class="sd">            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.</span>
<span class="sd">        beam_indices (`mindspore.Tensor`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):</span>
<span class="sd">            Beam indices of generated token id at each generation step. `mindspore.Tensor` of shape</span>
<span class="sd">            `(batch_size*num_return_sequences, sequence_length)`.</span>
<span class="sd">        encoder_attentions (`tuple(mindspore.Tensor)`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):</span>
<span class="sd">            Tuple of `mindspore.Tensor` (one for each layer of the decoder) of shape `(batch_size, num_heads,</span>
<span class="sd">            sequence_length, sequence_length)`.</span>
<span class="sd">        encoder_hidden_states (`tuple(mindspore.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):</span>
<span class="sd">            Tuple of `mindspore.Tensor` (one for the output of the embeddings + one for the output of each layer) of</span>
<span class="sd">            shape `(batch_size*num_beams*num_return_sequences, sequence_length, hidden_size)`.</span>
<span class="sd">        decoder_attentions (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            `mindspore.Tensor` of shape `(batch_size*num_beams*num_return_sequences, num_heads, generated_length,</span>
<span class="sd">            sequence_length)`.</span>
<span class="sd">        cross_attentions (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            `mindspore.Tensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.</span>
<span class="sd">        decoder_hidden_states (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            `mindspore.Tensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.</span>
<span class="sd">        past_key_values (`tuple(tuple(mindspore.Tensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>
<span class="sd">            NOTE: some models have a different `past_key_values` format, confirm with the model&#39;s documentation.</span>
<span class="sd">            Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value</span>
<span class="sd">            tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape</span>
<span class="sd">            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if</span>
<span class="sd">            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,</span>
<span class="sd">            encoder_sequence_length, embed_size_per_head)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">sequences</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">sequences_scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">beam_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">encoder_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">decoder_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">cross_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">decoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.utils.GenerateDecoderOnlyOutput" class="doc doc-heading">
            <code>mindnlp.transformers.generation.utils.GenerateDecoderOnlyOutput</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.transformers.generation.utils.GenerateDecoderOnlyOutput" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.utils.ModelOutput">ModelOutput</span></code></p>


        <p>Outputs of decoder-only generation models, when using non-beam methods.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sequences</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generated sequences. The second dimension (sequence_length) is either equal to <code>max_length</code> or shorter
if all batches finished early due to the <code>eos_token_id</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scores</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)
at each generation step. Tuple of <code>mindspore.Tensor</code> with up to <code>max_new_tokens</code> elements (one element for
each generated token), with each tensor of shape <code>(batch_size, config.vocab_size)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logits</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)
at each generation step. Tuple of <code>mindspore.Tensor</code> with up to <code>max_new_tokens</code> elements (one element for
each generated token), with each tensor of shape <code>(batch_size, config.vocab_size)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of
<code>mindspore.Tensor</code> of shape <code>(batch_size, num_heads, generated_length, sequence_length)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of
<code>mindspore.Tensor</code> of shape <code>(batch_size, generated_length, hidden_size)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_key_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>NOTE: some models have a different <code>past_key_values</code> format, confirm with the model's documentation.
Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value
tensor). The first Tuple is of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads,
encoder_sequence_length, embed_size_per_head)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\utils.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">GenerateDecoderOnlyOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Outputs of decoder-only generation models, when using non-beam methods.</span>

<span class="sd">    Args:</span>
<span class="sd">        sequences (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter</span>
<span class="sd">            if all batches finished early due to the `eos_token_id`.</span>
<span class="sd">        scores (`tuple(mindspore.Tensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):</span>
<span class="sd">            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)</span>
<span class="sd">            at each generation step. Tuple of `mindspore.Tensor` with up to `max_new_tokens` elements (one element for</span>
<span class="sd">            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.</span>
<span class="sd">        logits (`tuple(mindspore.Tensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):</span>
<span class="sd">            Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)</span>
<span class="sd">            at each generation step. Tuple of `mindspore.Tensor` with up to `max_new_tokens` elements (one element for</span>
<span class="sd">            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.</span>
<span class="sd">        attentions (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            `mindspore.Tensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.</span>
<span class="sd">        hidden_states (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            `mindspore.Tensor` of shape `(batch_size, generated_length, hidden_size)`.</span>
<span class="sd">        past_key_values (`tuple(tuple(mindspore.Tensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>
<span class="sd">            NOTE: some models have a different `past_key_values` format, confirm with the model&#39;s documentation.</span>
<span class="sd">            Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value</span>
<span class="sd">            tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape</span>
<span class="sd">            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if</span>
<span class="sd">            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,</span>
<span class="sd">            encoder_sequence_length, embed_size_per_head)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">sequences</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">average_infer_time</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.utils.GenerateEncoderDecoderOutput" class="doc doc-heading">
            <code>mindnlp.transformers.generation.utils.GenerateEncoderDecoderOutput</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindnlp.transformers.generation.utils.GenerateEncoderDecoderOutput" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.utils.ModelOutput">ModelOutput</span></code></p>


        <p>Outputs of encoder-decoder generation models, when using non-beam methods.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sequences</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generated sequences. The second dimension (sequence_length) is either equal to <code>max_length</code> or shorter
if all batches finished early due to the <code>eos_token_id</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scores</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)
at each generation step. Tuple of <code>mindspore.Tensor</code> with up to <code>max_new_tokens</code> elements (one element for
each generated token), with each tensor of shape <code>(batch_size, config.vocab_size)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logits</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)
at each generation step. Tuple of <code>mindspore.Tensor</code> with up to <code>max_new_tokens</code> elements (one element for
each generated token), with each tensor of shape <code>(batch_size, config.vocab_size)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple of <code>mindspore.Tensor</code> (one for each layer of the decoder) of shape <code>(batch_size, num_heads,
sequence_length, sequence_length)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple of <code>mindspore.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of
<code>mindspore.Tensor</code> of shape <code>(batch_size, num_heads, generated_length, sequence_length)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cross_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of
<code>mindspore.Tensor</code> of shape <code>(batch_size, num_heads, generated_length, sequence_length)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>decoder_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of
<code>mindspore.Tensor</code> of shape <code>(batch_size, generated_length, hidden_size)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_key_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>NOTE: some models have a different <code>past_key_values</code> format, confirm with the model's documentation.
Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value
tensor). The first Tuple is of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads,
encoder_sequence_length, embed_size_per_head)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\utils.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">GenerateEncoderDecoderOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Outputs of encoder-decoder generation models, when using non-beam methods.</span>

<span class="sd">    Args:</span>
<span class="sd">        sequences (`mindspore.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`):</span>
<span class="sd">            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter</span>
<span class="sd">            if all batches finished early due to the `eos_token_id`.</span>
<span class="sd">        scores (`tuple(mindspore.Tensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):</span>
<span class="sd">            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)</span>
<span class="sd">            at each generation step. Tuple of `mindspore.Tensor` with up to `max_new_tokens` elements (one element for</span>
<span class="sd">            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.</span>
<span class="sd">        logits (`tuple(mindspore.Tensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):</span>
<span class="sd">            Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)</span>
<span class="sd">            at each generation step. Tuple of `mindspore.Tensor` with up to `max_new_tokens` elements (one element for</span>
<span class="sd">            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.</span>
<span class="sd">        encoder_attentions (`tuple(mindspore.Tensor)`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):</span>
<span class="sd">            Tuple of `mindspore.Tensor` (one for each layer of the decoder) of shape `(batch_size, num_heads,</span>
<span class="sd">            sequence_length, sequence_length)`.</span>
<span class="sd">        encoder_hidden_states (`tuple(mindspore.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):</span>
<span class="sd">            Tuple of `mindspore.Tensor` (one for the output of the embeddings + one for the output of each layer) of</span>
<span class="sd">            shape `(batch_size, sequence_length, hidden_size)`.</span>
<span class="sd">        decoder_attentions (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            `mindspore.Tensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.</span>
<span class="sd">        cross_attentions (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            `mindspore.Tensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.</span>
<span class="sd">        decoder_hidden_states (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            `mindspore.Tensor` of shape `(batch_size, generated_length, hidden_size)`.</span>
<span class="sd">        past_key_values (`tuple(tuple(mindspore.Tensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>
<span class="sd">            NOTE: some models have a different `past_key_values` format, confirm with the model&#39;s documentation.</span>
<span class="sd">            Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value</span>
<span class="sd">            tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape</span>
<span class="sd">            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if</span>
<span class="sd">            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,</span>
<span class="sd">            encoder_sequence_length, embed_size_per_head)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">sequences</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">encoder_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">decoder_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">cross_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">decoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">average_infer_time</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.generation.utils.GenerationMixin" class="doc doc-heading">
            <code>mindnlp.transformers.generation.utils.GenerationMixin</code>


<a href="#mindnlp.transformers.generation.utils.GenerationMixin" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">


        <p>A class containing all functions for auto-regressive text generation, to be used as a mixin in [<code>PreTrainedModel</code>].</p>
<p>The class exposes [<code>~generation.GenerationMixin.generate</code>], which can be used for:
    - <em>greedy decoding</em> if <code>num_beams=1</code> and <code>do_sample=False</code>
    - <em>contrastive search</em> if <code>penalty_alpha&gt;0</code> and <code>top_k&gt;1</code>
    - <em>multinomial sampling</em> if <code>num_beams=1</code> and <code>do_sample=True</code>
    - <em>beam-search decoding</em> if <code>num_beams&gt;1</code> and <code>do_sample=False</code>
    - <em>beam-search multinomial sampling</em> if <code>num_beams&gt;1</code> and <code>do_sample=True</code>
    - <em>diverse beam-search decoding</em> if <code>num_beams&gt;1</code> and <code>num_beam_groups&gt;1</code>
    - <em>constrained beam-search decoding</em> if <code>constraints!=None</code> or <code>force_words_ids!=None</code>
    - <em>assisted decoding</em> if <code>assistant_model</code> or <code>prompt_lookup_num_tokens</code> is passed to <code>.generate()</code></p>
<p>To learn more about decoding strategies refer to the <a href="../generation_strategies">text generation strategies guide</a>.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\generation\utils.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 357</span>
<span class="normal"> 358</span>
<span class="normal"> 359</span>
<span class="normal"> 360</span>
<span class="normal"> 361</span>
<span class="normal"> 362</span>
<span class="normal"> 363</span>
<span class="normal"> 364</span>
<span class="normal"> 365</span>
<span class="normal"> 366</span>
<span class="normal"> 367</span>
<span class="normal"> 368</span>
<span class="normal"> 369</span>
<span class="normal"> 370</span>
<span class="normal"> 371</span>
<span class="normal"> 372</span>
<span class="normal"> 373</span>
<span class="normal"> 374</span>
<span class="normal"> 375</span>
<span class="normal"> 376</span>
<span class="normal"> 377</span>
<span class="normal"> 378</span>
<span class="normal"> 379</span>
<span class="normal"> 380</span>
<span class="normal"> 381</span>
<span class="normal"> 382</span>
<span class="normal"> 383</span>
<span class="normal"> 384</span>
<span class="normal"> 385</span>
<span class="normal"> 386</span>
<span class="normal"> 387</span>
<span class="normal"> 388</span>
<span class="normal"> 389</span>
<span class="normal"> 390</span>
<span class="normal"> 391</span>
<span class="normal"> 392</span>
<span class="normal"> 393</span>
<span class="normal"> 394</span>
<span class="normal"> 395</span>
<span class="normal"> 396</span>
<span class="normal"> 397</span>
<span class="normal"> 398</span>
<span class="normal"> 399</span>
<span class="normal"> 400</span>
<span class="normal"> 401</span>
<span class="normal"> 402</span>
<span class="normal"> 403</span>
<span class="normal"> 404</span>
<span class="normal"> 405</span>
<span class="normal"> 406</span>
<span class="normal"> 407</span>
<span class="normal"> 408</span>
<span class="normal"> 409</span>
<span class="normal"> 410</span>
<span class="normal"> 411</span>
<span class="normal"> 412</span>
<span class="normal"> 413</span>
<span class="normal"> 414</span>
<span class="normal"> 415</span>
<span class="normal"> 416</span>
<span class="normal"> 417</span>
<span class="normal"> 418</span>
<span class="normal"> 419</span>
<span class="normal"> 420</span>
<span class="normal"> 421</span>
<span class="normal"> 422</span>
<span class="normal"> 423</span>
<span class="normal"> 424</span>
<span class="normal"> 425</span>
<span class="normal"> 426</span>
<span class="normal"> 427</span>
<span class="normal"> 428</span>
<span class="normal"> 429</span>
<span class="normal"> 430</span>
<span class="normal"> 431</span>
<span class="normal"> 432</span>
<span class="normal"> 433</span>
<span class="normal"> 434</span>
<span class="normal"> 435</span>
<span class="normal"> 436</span>
<span class="normal"> 437</span>
<span class="normal"> 438</span>
<span class="normal"> 439</span>
<span class="normal"> 440</span>
<span class="normal"> 441</span>
<span class="normal"> 442</span>
<span class="normal"> 443</span>
<span class="normal"> 444</span>
<span class="normal"> 445</span>
<span class="normal"> 446</span>
<span class="normal"> 447</span>
<span class="normal"> 448</span>
<span class="normal"> 449</span>
<span class="normal"> 450</span>
<span class="normal"> 451</span>
<span class="normal"> 452</span>
<span class="normal"> 453</span>
<span class="normal"> 454</span>
<span class="normal"> 455</span>
<span class="normal"> 456</span>
<span class="normal"> 457</span>
<span class="normal"> 458</span>
<span class="normal"> 459</span>
<span class="normal"> 460</span>
<span class="normal"> 461</span>
<span class="normal"> 462</span>
<span class="normal"> 463</span>
<span class="normal"> 464</span>
<span class="normal"> 465</span>
<span class="normal"> 466</span>
<span class="normal"> 467</span>
<span class="normal"> 468</span>
<span class="normal"> 469</span>
<span class="normal"> 470</span>
<span class="normal"> 471</span>
<span class="normal"> 472</span>
<span class="normal"> 473</span>
<span class="normal"> 474</span>
<span class="normal"> 475</span>
<span class="normal"> 476</span>
<span class="normal"> 477</span>
<span class="normal"> 478</span>
<span class="normal"> 479</span>
<span class="normal"> 480</span>
<span class="normal"> 481</span>
<span class="normal"> 482</span>
<span class="normal"> 483</span>
<span class="normal"> 484</span>
<span class="normal"> 485</span>
<span class="normal"> 486</span>
<span class="normal"> 487</span>
<span class="normal"> 488</span>
<span class="normal"> 489</span>
<span class="normal"> 490</span>
<span class="normal"> 491</span>
<span class="normal"> 492</span>
<span class="normal"> 493</span>
<span class="normal"> 494</span>
<span class="normal"> 495</span>
<span class="normal"> 496</span>
<span class="normal"> 497</span>
<span class="normal"> 498</span>
<span class="normal"> 499</span>
<span class="normal"> 500</span>
<span class="normal"> 501</span>
<span class="normal"> 502</span>
<span class="normal"> 503</span>
<span class="normal"> 504</span>
<span class="normal"> 505</span>
<span class="normal"> 506</span>
<span class="normal"> 507</span>
<span class="normal"> 508</span>
<span class="normal"> 509</span>
<span class="normal"> 510</span>
<span class="normal"> 511</span>
<span class="normal"> 512</span>
<span class="normal"> 513</span>
<span class="normal"> 514</span>
<span class="normal"> 515</span>
<span class="normal"> 516</span>
<span class="normal"> 517</span>
<span class="normal"> 518</span>
<span class="normal"> 519</span>
<span class="normal"> 520</span>
<span class="normal"> 521</span>
<span class="normal"> 522</span>
<span class="normal"> 523</span>
<span class="normal"> 524</span>
<span class="normal"> 525</span>
<span class="normal"> 526</span>
<span class="normal"> 527</span>
<span class="normal"> 528</span>
<span class="normal"> 529</span>
<span class="normal"> 530</span>
<span class="normal"> 531</span>
<span class="normal"> 532</span>
<span class="normal"> 533</span>
<span class="normal"> 534</span>
<span class="normal"> 535</span>
<span class="normal"> 536</span>
<span class="normal"> 537</span>
<span class="normal"> 538</span>
<span class="normal"> 539</span>
<span class="normal"> 540</span>
<span class="normal"> 541</span>
<span class="normal"> 542</span>
<span class="normal"> 543</span>
<span class="normal"> 544</span>
<span class="normal"> 545</span>
<span class="normal"> 546</span>
<span class="normal"> 547</span>
<span class="normal"> 548</span>
<span class="normal"> 549</span>
<span class="normal"> 550</span>
<span class="normal"> 551</span>
<span class="normal"> 552</span>
<span class="normal"> 553</span>
<span class="normal"> 554</span>
<span class="normal"> 555</span>
<span class="normal"> 556</span>
<span class="normal"> 557</span>
<span class="normal"> 558</span>
<span class="normal"> 559</span>
<span class="normal"> 560</span>
<span class="normal"> 561</span>
<span class="normal"> 562</span>
<span class="normal"> 563</span>
<span class="normal"> 564</span>
<span class="normal"> 565</span>
<span class="normal"> 566</span>
<span class="normal"> 567</span>
<span class="normal"> 568</span>
<span class="normal"> 569</span>
<span class="normal"> 570</span>
<span class="normal"> 571</span>
<span class="normal"> 572</span>
<span class="normal"> 573</span>
<span class="normal"> 574</span>
<span class="normal"> 575</span>
<span class="normal"> 576</span>
<span class="normal"> 577</span>
<span class="normal"> 578</span>
<span class="normal"> 579</span>
<span class="normal"> 580</span>
<span class="normal"> 581</span>
<span class="normal"> 582</span>
<span class="normal"> 583</span>
<span class="normal"> 584</span>
<span class="normal"> 585</span>
<span class="normal"> 586</span>
<span class="normal"> 587</span>
<span class="normal"> 588</span>
<span class="normal"> 589</span>
<span class="normal"> 590</span>
<span class="normal"> 591</span>
<span class="normal"> 592</span>
<span class="normal"> 593</span>
<span class="normal"> 594</span>
<span class="normal"> 595</span>
<span class="normal"> 596</span>
<span class="normal"> 597</span>
<span class="normal"> 598</span>
<span class="normal"> 599</span>
<span class="normal"> 600</span>
<span class="normal"> 601</span>
<span class="normal"> 602</span>
<span class="normal"> 603</span>
<span class="normal"> 604</span>
<span class="normal"> 605</span>
<span class="normal"> 606</span>
<span class="normal"> 607</span>
<span class="normal"> 608</span>
<span class="normal"> 609</span>
<span class="normal"> 610</span>
<span class="normal"> 611</span>
<span class="normal"> 612</span>
<span class="normal"> 613</span>
<span class="normal"> 614</span>
<span class="normal"> 615</span>
<span class="normal"> 616</span>
<span class="normal"> 617</span>
<span class="normal"> 618</span>
<span class="normal"> 619</span>
<span class="normal"> 620</span>
<span class="normal"> 621</span>
<span class="normal"> 622</span>
<span class="normal"> 623</span>
<span class="normal"> 624</span>
<span class="normal"> 625</span>
<span class="normal"> 626</span>
<span class="normal"> 627</span>
<span class="normal"> 628</span>
<span class="normal"> 629</span>
<span class="normal"> 630</span>
<span class="normal"> 631</span>
<span class="normal"> 632</span>
<span class="normal"> 633</span>
<span class="normal"> 634</span>
<span class="normal"> 635</span>
<span class="normal"> 636</span>
<span class="normal"> 637</span>
<span class="normal"> 638</span>
<span class="normal"> 639</span>
<span class="normal"> 640</span>
<span class="normal"> 641</span>
<span class="normal"> 642</span>
<span class="normal"> 643</span>
<span class="normal"> 644</span>
<span class="normal"> 645</span>
<span class="normal"> 646</span>
<span class="normal"> 647</span>
<span class="normal"> 648</span>
<span class="normal"> 649</span>
<span class="normal"> 650</span>
<span class="normal"> 651</span>
<span class="normal"> 652</span>
<span class="normal"> 653</span>
<span class="normal"> 654</span>
<span class="normal"> 655</span>
<span class="normal"> 656</span>
<span class="normal"> 657</span>
<span class="normal"> 658</span>
<span class="normal"> 659</span>
<span class="normal"> 660</span>
<span class="normal"> 661</span>
<span class="normal"> 662</span>
<span class="normal"> 663</span>
<span class="normal"> 664</span>
<span class="normal"> 665</span>
<span class="normal"> 666</span>
<span class="normal"> 667</span>
<span class="normal"> 668</span>
<span class="normal"> 669</span>
<span class="normal"> 670</span>
<span class="normal"> 671</span>
<span class="normal"> 672</span>
<span class="normal"> 673</span>
<span class="normal"> 674</span>
<span class="normal"> 675</span>
<span class="normal"> 676</span>
<span class="normal"> 677</span>
<span class="normal"> 678</span>
<span class="normal"> 679</span>
<span class="normal"> 680</span>
<span class="normal"> 681</span>
<span class="normal"> 682</span>
<span class="normal"> 683</span>
<span class="normal"> 684</span>
<span class="normal"> 685</span>
<span class="normal"> 686</span>
<span class="normal"> 687</span>
<span class="normal"> 688</span>
<span class="normal"> 689</span>
<span class="normal"> 690</span>
<span class="normal"> 691</span>
<span class="normal"> 692</span>
<span class="normal"> 693</span>
<span class="normal"> 694</span>
<span class="normal"> 695</span>
<span class="normal"> 696</span>
<span class="normal"> 697</span>
<span class="normal"> 698</span>
<span class="normal"> 699</span>
<span class="normal"> 700</span>
<span class="normal"> 701</span>
<span class="normal"> 702</span>
<span class="normal"> 703</span>
<span class="normal"> 704</span>
<span class="normal"> 705</span>
<span class="normal"> 706</span>
<span class="normal"> 707</span>
<span class="normal"> 708</span>
<span class="normal"> 709</span>
<span class="normal"> 710</span>
<span class="normal"> 711</span>
<span class="normal"> 712</span>
<span class="normal"> 713</span>
<span class="normal"> 714</span>
<span class="normal"> 715</span>
<span class="normal"> 716</span>
<span class="normal"> 717</span>
<span class="normal"> 718</span>
<span class="normal"> 719</span>
<span class="normal"> 720</span>
<span class="normal"> 721</span>
<span class="normal"> 722</span>
<span class="normal"> 723</span>
<span class="normal"> 724</span>
<span class="normal"> 725</span>
<span class="normal"> 726</span>
<span class="normal"> 727</span>
<span class="normal"> 728</span>
<span class="normal"> 729</span>
<span class="normal"> 730</span>
<span class="normal"> 731</span>
<span class="normal"> 732</span>
<span class="normal"> 733</span>
<span class="normal"> 734</span>
<span class="normal"> 735</span>
<span class="normal"> 736</span>
<span class="normal"> 737</span>
<span class="normal"> 738</span>
<span class="normal"> 739</span>
<span class="normal"> 740</span>
<span class="normal"> 741</span>
<span class="normal"> 742</span>
<span class="normal"> 743</span>
<span class="normal"> 744</span>
<span class="normal"> 745</span>
<span class="normal"> 746</span>
<span class="normal"> 747</span>
<span class="normal"> 748</span>
<span class="normal"> 749</span>
<span class="normal"> 750</span>
<span class="normal"> 751</span>
<span class="normal"> 752</span>
<span class="normal"> 753</span>
<span class="normal"> 754</span>
<span class="normal"> 755</span>
<span class="normal"> 756</span>
<span class="normal"> 757</span>
<span class="normal"> 758</span>
<span class="normal"> 759</span>
<span class="normal"> 760</span>
<span class="normal"> 761</span>
<span class="normal"> 762</span>
<span class="normal"> 763</span>
<span class="normal"> 764</span>
<span class="normal"> 765</span>
<span class="normal"> 766</span>
<span class="normal"> 767</span>
<span class="normal"> 768</span>
<span class="normal"> 769</span>
<span class="normal"> 770</span>
<span class="normal"> 771</span>
<span class="normal"> 772</span>
<span class="normal"> 773</span>
<span class="normal"> 774</span>
<span class="normal"> 775</span>
<span class="normal"> 776</span>
<span class="normal"> 777</span>
<span class="normal"> 778</span>
<span class="normal"> 779</span>
<span class="normal"> 780</span>
<span class="normal"> 781</span>
<span class="normal"> 782</span>
<span class="normal"> 783</span>
<span class="normal"> 784</span>
<span class="normal"> 785</span>
<span class="normal"> 786</span>
<span class="normal"> 787</span>
<span class="normal"> 788</span>
<span class="normal"> 789</span>
<span class="normal"> 790</span>
<span class="normal"> 791</span>
<span class="normal"> 792</span>
<span class="normal"> 793</span>
<span class="normal"> 794</span>
<span class="normal"> 795</span>
<span class="normal"> 796</span>
<span class="normal"> 797</span>
<span class="normal"> 798</span>
<span class="normal"> 799</span>
<span class="normal"> 800</span>
<span class="normal"> 801</span>
<span class="normal"> 802</span>
<span class="normal"> 803</span>
<span class="normal"> 804</span>
<span class="normal"> 805</span>
<span class="normal"> 806</span>
<span class="normal"> 807</span>
<span class="normal"> 808</span>
<span class="normal"> 809</span>
<span class="normal"> 810</span>
<span class="normal"> 811</span>
<span class="normal"> 812</span>
<span class="normal"> 813</span>
<span class="normal"> 814</span>
<span class="normal"> 815</span>
<span class="normal"> 816</span>
<span class="normal"> 817</span>
<span class="normal"> 818</span>
<span class="normal"> 819</span>
<span class="normal"> 820</span>
<span class="normal"> 821</span>
<span class="normal"> 822</span>
<span class="normal"> 823</span>
<span class="normal"> 824</span>
<span class="normal"> 825</span>
<span class="normal"> 826</span>
<span class="normal"> 827</span>
<span class="normal"> 828</span>
<span class="normal"> 829</span>
<span class="normal"> 830</span>
<span class="normal"> 831</span>
<span class="normal"> 832</span>
<span class="normal"> 833</span>
<span class="normal"> 834</span>
<span class="normal"> 835</span>
<span class="normal"> 836</span>
<span class="normal"> 837</span>
<span class="normal"> 838</span>
<span class="normal"> 839</span>
<span class="normal"> 840</span>
<span class="normal"> 841</span>
<span class="normal"> 842</span>
<span class="normal"> 843</span>
<span class="normal"> 844</span>
<span class="normal"> 845</span>
<span class="normal"> 846</span>
<span class="normal"> 847</span>
<span class="normal"> 848</span>
<span class="normal"> 849</span>
<span class="normal"> 850</span>
<span class="normal"> 851</span>
<span class="normal"> 852</span>
<span class="normal"> 853</span>
<span class="normal"> 854</span>
<span class="normal"> 855</span>
<span class="normal"> 856</span>
<span class="normal"> 857</span>
<span class="normal"> 858</span>
<span class="normal"> 859</span>
<span class="normal"> 860</span>
<span class="normal"> 861</span>
<span class="normal"> 862</span>
<span class="normal"> 863</span>
<span class="normal"> 864</span>
<span class="normal"> 865</span>
<span class="normal"> 866</span>
<span class="normal"> 867</span>
<span class="normal"> 868</span>
<span class="normal"> 869</span>
<span class="normal"> 870</span>
<span class="normal"> 871</span>
<span class="normal"> 872</span>
<span class="normal"> 873</span>
<span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span>
<span class="normal">1445</span>
<span class="normal">1446</span>
<span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span>
<span class="normal">2105</span>
<span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span>
<span class="normal">2109</span>
<span class="normal">2110</span>
<span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span>
<span class="normal">2141</span>
<span class="normal">2142</span>
<span class="normal">2143</span>
<span class="normal">2144</span>
<span class="normal">2145</span>
<span class="normal">2146</span>
<span class="normal">2147</span>
<span class="normal">2148</span>
<span class="normal">2149</span>
<span class="normal">2150</span>
<span class="normal">2151</span>
<span class="normal">2152</span>
<span class="normal">2153</span>
<span class="normal">2154</span>
<span class="normal">2155</span>
<span class="normal">2156</span>
<span class="normal">2157</span>
<span class="normal">2158</span>
<span class="normal">2159</span>
<span class="normal">2160</span>
<span class="normal">2161</span>
<span class="normal">2162</span>
<span class="normal">2163</span>
<span class="normal">2164</span>
<span class="normal">2165</span>
<span class="normal">2166</span>
<span class="normal">2167</span>
<span class="normal">2168</span>
<span class="normal">2169</span>
<span class="normal">2170</span>
<span class="normal">2171</span>
<span class="normal">2172</span>
<span class="normal">2173</span>
<span class="normal">2174</span>
<span class="normal">2175</span>
<span class="normal">2176</span>
<span class="normal">2177</span>
<span class="normal">2178</span>
<span class="normal">2179</span>
<span class="normal">2180</span>
<span class="normal">2181</span>
<span class="normal">2182</span>
<span class="normal">2183</span>
<span class="normal">2184</span>
<span class="normal">2185</span>
<span class="normal">2186</span>
<span class="normal">2187</span>
<span class="normal">2188</span>
<span class="normal">2189</span>
<span class="normal">2190</span>
<span class="normal">2191</span>
<span class="normal">2192</span>
<span class="normal">2193</span>
<span class="normal">2194</span>
<span class="normal">2195</span>
<span class="normal">2196</span>
<span class="normal">2197</span>
<span class="normal">2198</span>
<span class="normal">2199</span>
<span class="normal">2200</span>
<span class="normal">2201</span>
<span class="normal">2202</span>
<span class="normal">2203</span>
<span class="normal">2204</span>
<span class="normal">2205</span>
<span class="normal">2206</span>
<span class="normal">2207</span>
<span class="normal">2208</span>
<span class="normal">2209</span>
<span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span>
<span class="normal">2219</span>
<span class="normal">2220</span>
<span class="normal">2221</span>
<span class="normal">2222</span>
<span class="normal">2223</span>
<span class="normal">2224</span>
<span class="normal">2225</span>
<span class="normal">2226</span>
<span class="normal">2227</span>
<span class="normal">2228</span>
<span class="normal">2229</span>
<span class="normal">2230</span>
<span class="normal">2231</span>
<span class="normal">2232</span>
<span class="normal">2233</span>
<span class="normal">2234</span>
<span class="normal">2235</span>
<span class="normal">2236</span>
<span class="normal">2237</span>
<span class="normal">2238</span>
<span class="normal">2239</span>
<span class="normal">2240</span>
<span class="normal">2241</span>
<span class="normal">2242</span>
<span class="normal">2243</span>
<span class="normal">2244</span>
<span class="normal">2245</span>
<span class="normal">2246</span>
<span class="normal">2247</span>
<span class="normal">2248</span>
<span class="normal">2249</span>
<span class="normal">2250</span>
<span class="normal">2251</span>
<span class="normal">2252</span>
<span class="normal">2253</span>
<span class="normal">2254</span>
<span class="normal">2255</span>
<span class="normal">2256</span>
<span class="normal">2257</span>
<span class="normal">2258</span>
<span class="normal">2259</span>
<span class="normal">2260</span>
<span class="normal">2261</span>
<span class="normal">2262</span>
<span class="normal">2263</span>
<span class="normal">2264</span>
<span class="normal">2265</span>
<span class="normal">2266</span>
<span class="normal">2267</span>
<span class="normal">2268</span>
<span class="normal">2269</span>
<span class="normal">2270</span>
<span class="normal">2271</span>
<span class="normal">2272</span>
<span class="normal">2273</span>
<span class="normal">2274</span>
<span class="normal">2275</span>
<span class="normal">2276</span>
<span class="normal">2277</span>
<span class="normal">2278</span>
<span class="normal">2279</span>
<span class="normal">2280</span>
<span class="normal">2281</span>
<span class="normal">2282</span>
<span class="normal">2283</span>
<span class="normal">2284</span>
<span class="normal">2285</span>
<span class="normal">2286</span>
<span class="normal">2287</span>
<span class="normal">2288</span>
<span class="normal">2289</span>
<span class="normal">2290</span>
<span class="normal">2291</span>
<span class="normal">2292</span>
<span class="normal">2293</span>
<span class="normal">2294</span>
<span class="normal">2295</span>
<span class="normal">2296</span>
<span class="normal">2297</span>
<span class="normal">2298</span>
<span class="normal">2299</span>
<span class="normal">2300</span>
<span class="normal">2301</span>
<span class="normal">2302</span>
<span class="normal">2303</span>
<span class="normal">2304</span>
<span class="normal">2305</span>
<span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span>
<span class="normal">2313</span>
<span class="normal">2314</span>
<span class="normal">2315</span>
<span class="normal">2316</span>
<span class="normal">2317</span>
<span class="normal">2318</span>
<span class="normal">2319</span>
<span class="normal">2320</span>
<span class="normal">2321</span>
<span class="normal">2322</span>
<span class="normal">2323</span>
<span class="normal">2324</span>
<span class="normal">2325</span>
<span class="normal">2326</span>
<span class="normal">2327</span>
<span class="normal">2328</span>
<span class="normal">2329</span>
<span class="normal">2330</span>
<span class="normal">2331</span>
<span class="normal">2332</span>
<span class="normal">2333</span>
<span class="normal">2334</span>
<span class="normal">2335</span>
<span class="normal">2336</span>
<span class="normal">2337</span>
<span class="normal">2338</span>
<span class="normal">2339</span>
<span class="normal">2340</span>
<span class="normal">2341</span>
<span class="normal">2342</span>
<span class="normal">2343</span>
<span class="normal">2344</span>
<span class="normal">2345</span>
<span class="normal">2346</span>
<span class="normal">2347</span>
<span class="normal">2348</span>
<span class="normal">2349</span>
<span class="normal">2350</span>
<span class="normal">2351</span>
<span class="normal">2352</span>
<span class="normal">2353</span>
<span class="normal">2354</span>
<span class="normal">2355</span>
<span class="normal">2356</span>
<span class="normal">2357</span>
<span class="normal">2358</span>
<span class="normal">2359</span>
<span class="normal">2360</span>
<span class="normal">2361</span>
<span class="normal">2362</span>
<span class="normal">2363</span>
<span class="normal">2364</span>
<span class="normal">2365</span>
<span class="normal">2366</span>
<span class="normal">2367</span>
<span class="normal">2368</span>
<span class="normal">2369</span>
<span class="normal">2370</span>
<span class="normal">2371</span>
<span class="normal">2372</span>
<span class="normal">2373</span>
<span class="normal">2374</span>
<span class="normal">2375</span>
<span class="normal">2376</span>
<span class="normal">2377</span>
<span class="normal">2378</span>
<span class="normal">2379</span>
<span class="normal">2380</span>
<span class="normal">2381</span>
<span class="normal">2382</span>
<span class="normal">2383</span>
<span class="normal">2384</span>
<span class="normal">2385</span>
<span class="normal">2386</span>
<span class="normal">2387</span>
<span class="normal">2388</span>
<span class="normal">2389</span>
<span class="normal">2390</span>
<span class="normal">2391</span>
<span class="normal">2392</span>
<span class="normal">2393</span>
<span class="normal">2394</span>
<span class="normal">2395</span>
<span class="normal">2396</span>
<span class="normal">2397</span>
<span class="normal">2398</span>
<span class="normal">2399</span>
<span class="normal">2400</span>
<span class="normal">2401</span>
<span class="normal">2402</span>
<span class="normal">2403</span>
<span class="normal">2404</span>
<span class="normal">2405</span>
<span class="normal">2406</span>
<span class="normal">2407</span>
<span class="normal">2408</span>
<span class="normal">2409</span>
<span class="normal">2410</span>
<span class="normal">2411</span>
<span class="normal">2412</span>
<span class="normal">2413</span>
<span class="normal">2414</span>
<span class="normal">2415</span>
<span class="normal">2416</span>
<span class="normal">2417</span>
<span class="normal">2418</span>
<span class="normal">2419</span>
<span class="normal">2420</span>
<span class="normal">2421</span>
<span class="normal">2422</span>
<span class="normal">2423</span>
<span class="normal">2424</span>
<span class="normal">2425</span>
<span class="normal">2426</span>
<span class="normal">2427</span>
<span class="normal">2428</span>
<span class="normal">2429</span>
<span class="normal">2430</span>
<span class="normal">2431</span>
<span class="normal">2432</span>
<span class="normal">2433</span>
<span class="normal">2434</span>
<span class="normal">2435</span>
<span class="normal">2436</span>
<span class="normal">2437</span>
<span class="normal">2438</span>
<span class="normal">2439</span>
<span class="normal">2440</span>
<span class="normal">2441</span>
<span class="normal">2442</span>
<span class="normal">2443</span>
<span class="normal">2444</span>
<span class="normal">2445</span>
<span class="normal">2446</span>
<span class="normal">2447</span>
<span class="normal">2448</span>
<span class="normal">2449</span>
<span class="normal">2450</span>
<span class="normal">2451</span>
<span class="normal">2452</span>
<span class="normal">2453</span>
<span class="normal">2454</span>
<span class="normal">2455</span>
<span class="normal">2456</span>
<span class="normal">2457</span>
<span class="normal">2458</span>
<span class="normal">2459</span>
<span class="normal">2460</span>
<span class="normal">2461</span>
<span class="normal">2462</span>
<span class="normal">2463</span>
<span class="normal">2464</span>
<span class="normal">2465</span>
<span class="normal">2466</span>
<span class="normal">2467</span>
<span class="normal">2468</span>
<span class="normal">2469</span>
<span class="normal">2470</span>
<span class="normal">2471</span>
<span class="normal">2472</span>
<span class="normal">2473</span>
<span class="normal">2474</span>
<span class="normal">2475</span>
<span class="normal">2476</span>
<span class="normal">2477</span>
<span class="normal">2478</span>
<span class="normal">2479</span>
<span class="normal">2480</span>
<span class="normal">2481</span>
<span class="normal">2482</span>
<span class="normal">2483</span>
<span class="normal">2484</span>
<span class="normal">2485</span>
<span class="normal">2486</span>
<span class="normal">2487</span>
<span class="normal">2488</span>
<span class="normal">2489</span>
<span class="normal">2490</span>
<span class="normal">2491</span>
<span class="normal">2492</span>
<span class="normal">2493</span>
<span class="normal">2494</span>
<span class="normal">2495</span>
<span class="normal">2496</span>
<span class="normal">2497</span>
<span class="normal">2498</span>
<span class="normal">2499</span>
<span class="normal">2500</span>
<span class="normal">2501</span>
<span class="normal">2502</span>
<span class="normal">2503</span>
<span class="normal">2504</span>
<span class="normal">2505</span>
<span class="normal">2506</span>
<span class="normal">2507</span>
<span class="normal">2508</span>
<span class="normal">2509</span>
<span class="normal">2510</span>
<span class="normal">2511</span>
<span class="normal">2512</span>
<span class="normal">2513</span>
<span class="normal">2514</span>
<span class="normal">2515</span>
<span class="normal">2516</span>
<span class="normal">2517</span>
<span class="normal">2518</span>
<span class="normal">2519</span>
<span class="normal">2520</span>
<span class="normal">2521</span>
<span class="normal">2522</span>
<span class="normal">2523</span>
<span class="normal">2524</span>
<span class="normal">2525</span>
<span class="normal">2526</span>
<span class="normal">2527</span>
<span class="normal">2528</span>
<span class="normal">2529</span>
<span class="normal">2530</span>
<span class="normal">2531</span>
<span class="normal">2532</span>
<span class="normal">2533</span>
<span class="normal">2534</span>
<span class="normal">2535</span>
<span class="normal">2536</span>
<span class="normal">2537</span>
<span class="normal">2538</span>
<span class="normal">2539</span>
<span class="normal">2540</span>
<span class="normal">2541</span>
<span class="normal">2542</span>
<span class="normal">2543</span>
<span class="normal">2544</span>
<span class="normal">2545</span>
<span class="normal">2546</span>
<span class="normal">2547</span>
<span class="normal">2548</span>
<span class="normal">2549</span>
<span class="normal">2550</span>
<span class="normal">2551</span>
<span class="normal">2552</span>
<span class="normal">2553</span>
<span class="normal">2554</span>
<span class="normal">2555</span>
<span class="normal">2556</span>
<span class="normal">2557</span>
<span class="normal">2558</span>
<span class="normal">2559</span>
<span class="normal">2560</span>
<span class="normal">2561</span>
<span class="normal">2562</span>
<span class="normal">2563</span>
<span class="normal">2564</span>
<span class="normal">2565</span>
<span class="normal">2566</span>
<span class="normal">2567</span>
<span class="normal">2568</span>
<span class="normal">2569</span>
<span class="normal">2570</span>
<span class="normal">2571</span>
<span class="normal">2572</span>
<span class="normal">2573</span>
<span class="normal">2574</span>
<span class="normal">2575</span>
<span class="normal">2576</span>
<span class="normal">2577</span>
<span class="normal">2578</span>
<span class="normal">2579</span>
<span class="normal">2580</span>
<span class="normal">2581</span>
<span class="normal">2582</span>
<span class="normal">2583</span>
<span class="normal">2584</span>
<span class="normal">2585</span>
<span class="normal">2586</span>
<span class="normal">2587</span>
<span class="normal">2588</span>
<span class="normal">2589</span>
<span class="normal">2590</span>
<span class="normal">2591</span>
<span class="normal">2592</span>
<span class="normal">2593</span>
<span class="normal">2594</span>
<span class="normal">2595</span>
<span class="normal">2596</span>
<span class="normal">2597</span>
<span class="normal">2598</span>
<span class="normal">2599</span>
<span class="normal">2600</span>
<span class="normal">2601</span>
<span class="normal">2602</span>
<span class="normal">2603</span>
<span class="normal">2604</span>
<span class="normal">2605</span>
<span class="normal">2606</span>
<span class="normal">2607</span>
<span class="normal">2608</span>
<span class="normal">2609</span>
<span class="normal">2610</span>
<span class="normal">2611</span>
<span class="normal">2612</span>
<span class="normal">2613</span>
<span class="normal">2614</span>
<span class="normal">2615</span>
<span class="normal">2616</span>
<span class="normal">2617</span>
<span class="normal">2618</span>
<span class="normal">2619</span>
<span class="normal">2620</span>
<span class="normal">2621</span>
<span class="normal">2622</span>
<span class="normal">2623</span>
<span class="normal">2624</span>
<span class="normal">2625</span>
<span class="normal">2626</span>
<span class="normal">2627</span>
<span class="normal">2628</span>
<span class="normal">2629</span>
<span class="normal">2630</span>
<span class="normal">2631</span>
<span class="normal">2632</span>
<span class="normal">2633</span>
<span class="normal">2634</span>
<span class="normal">2635</span>
<span class="normal">2636</span>
<span class="normal">2637</span>
<span class="normal">2638</span>
<span class="normal">2639</span>
<span class="normal">2640</span>
<span class="normal">2641</span>
<span class="normal">2642</span>
<span class="normal">2643</span>
<span class="normal">2644</span>
<span class="normal">2645</span>
<span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span>
<span class="normal">2652</span>
<span class="normal">2653</span>
<span class="normal">2654</span>
<span class="normal">2655</span>
<span class="normal">2656</span>
<span class="normal">2657</span>
<span class="normal">2658</span>
<span class="normal">2659</span>
<span class="normal">2660</span>
<span class="normal">2661</span>
<span class="normal">2662</span>
<span class="normal">2663</span>
<span class="normal">2664</span>
<span class="normal">2665</span>
<span class="normal">2666</span>
<span class="normal">2667</span>
<span class="normal">2668</span>
<span class="normal">2669</span>
<span class="normal">2670</span>
<span class="normal">2671</span>
<span class="normal">2672</span>
<span class="normal">2673</span>
<span class="normal">2674</span>
<span class="normal">2675</span>
<span class="normal">2676</span>
<span class="normal">2677</span>
<span class="normal">2678</span>
<span class="normal">2679</span>
<span class="normal">2680</span>
<span class="normal">2681</span>
<span class="normal">2682</span>
<span class="normal">2683</span>
<span class="normal">2684</span>
<span class="normal">2685</span>
<span class="normal">2686</span>
<span class="normal">2687</span>
<span class="normal">2688</span>
<span class="normal">2689</span>
<span class="normal">2690</span>
<span class="normal">2691</span>
<span class="normal">2692</span>
<span class="normal">2693</span>
<span class="normal">2694</span>
<span class="normal">2695</span>
<span class="normal">2696</span>
<span class="normal">2697</span>
<span class="normal">2698</span>
<span class="normal">2699</span>
<span class="normal">2700</span>
<span class="normal">2701</span>
<span class="normal">2702</span>
<span class="normal">2703</span>
<span class="normal">2704</span>
<span class="normal">2705</span>
<span class="normal">2706</span>
<span class="normal">2707</span>
<span class="normal">2708</span>
<span class="normal">2709</span>
<span class="normal">2710</span>
<span class="normal">2711</span>
<span class="normal">2712</span>
<span class="normal">2713</span>
<span class="normal">2714</span>
<span class="normal">2715</span>
<span class="normal">2716</span>
<span class="normal">2717</span>
<span class="normal">2718</span>
<span class="normal">2719</span>
<span class="normal">2720</span>
<span class="normal">2721</span>
<span class="normal">2722</span>
<span class="normal">2723</span>
<span class="normal">2724</span>
<span class="normal">2725</span>
<span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span>
<span class="normal">2733</span>
<span class="normal">2734</span>
<span class="normal">2735</span>
<span class="normal">2736</span>
<span class="normal">2737</span>
<span class="normal">2738</span>
<span class="normal">2739</span>
<span class="normal">2740</span>
<span class="normal">2741</span>
<span class="normal">2742</span>
<span class="normal">2743</span>
<span class="normal">2744</span>
<span class="normal">2745</span>
<span class="normal">2746</span>
<span class="normal">2747</span>
<span class="normal">2748</span>
<span class="normal">2749</span>
<span class="normal">2750</span>
<span class="normal">2751</span>
<span class="normal">2752</span>
<span class="normal">2753</span>
<span class="normal">2754</span>
<span class="normal">2755</span>
<span class="normal">2756</span>
<span class="normal">2757</span>
<span class="normal">2758</span>
<span class="normal">2759</span>
<span class="normal">2760</span>
<span class="normal">2761</span>
<span class="normal">2762</span>
<span class="normal">2763</span>
<span class="normal">2764</span>
<span class="normal">2765</span>
<span class="normal">2766</span>
<span class="normal">2767</span>
<span class="normal">2768</span>
<span class="normal">2769</span>
<span class="normal">2770</span>
<span class="normal">2771</span>
<span class="normal">2772</span>
<span class="normal">2773</span>
<span class="normal">2774</span>
<span class="normal">2775</span>
<span class="normal">2776</span>
<span class="normal">2777</span>
<span class="normal">2778</span>
<span class="normal">2779</span>
<span class="normal">2780</span>
<span class="normal">2781</span>
<span class="normal">2782</span>
<span class="normal">2783</span>
<span class="normal">2784</span>
<span class="normal">2785</span>
<span class="normal">2786</span>
<span class="normal">2787</span>
<span class="normal">2788</span>
<span class="normal">2789</span>
<span class="normal">2790</span>
<span class="normal">2791</span>
<span class="normal">2792</span>
<span class="normal">2793</span>
<span class="normal">2794</span>
<span class="normal">2795</span>
<span class="normal">2796</span>
<span class="normal">2797</span>
<span class="normal">2798</span>
<span class="normal">2799</span>
<span class="normal">2800</span>
<span class="normal">2801</span>
<span class="normal">2802</span>
<span class="normal">2803</span>
<span class="normal">2804</span>
<span class="normal">2805</span>
<span class="normal">2806</span>
<span class="normal">2807</span>
<span class="normal">2808</span>
<span class="normal">2809</span>
<span class="normal">2810</span>
<span class="normal">2811</span>
<span class="normal">2812</span>
<span class="normal">2813</span>
<span class="normal">2814</span>
<span class="normal">2815</span>
<span class="normal">2816</span>
<span class="normal">2817</span>
<span class="normal">2818</span>
<span class="normal">2819</span>
<span class="normal">2820</span>
<span class="normal">2821</span>
<span class="normal">2822</span>
<span class="normal">2823</span>
<span class="normal">2824</span>
<span class="normal">2825</span>
<span class="normal">2826</span>
<span class="normal">2827</span>
<span class="normal">2828</span>
<span class="normal">2829</span>
<span class="normal">2830</span>
<span class="normal">2831</span>
<span class="normal">2832</span>
<span class="normal">2833</span>
<span class="normal">2834</span>
<span class="normal">2835</span>
<span class="normal">2836</span>
<span class="normal">2837</span>
<span class="normal">2838</span>
<span class="normal">2839</span>
<span class="normal">2840</span>
<span class="normal">2841</span>
<span class="normal">2842</span>
<span class="normal">2843</span>
<span class="normal">2844</span>
<span class="normal">2845</span>
<span class="normal">2846</span>
<span class="normal">2847</span>
<span class="normal">2848</span>
<span class="normal">2849</span>
<span class="normal">2850</span>
<span class="normal">2851</span>
<span class="normal">2852</span>
<span class="normal">2853</span>
<span class="normal">2854</span>
<span class="normal">2855</span>
<span class="normal">2856</span>
<span class="normal">2857</span>
<span class="normal">2858</span>
<span class="normal">2859</span>
<span class="normal">2860</span>
<span class="normal">2861</span>
<span class="normal">2862</span>
<span class="normal">2863</span>
<span class="normal">2864</span>
<span class="normal">2865</span>
<span class="normal">2866</span>
<span class="normal">2867</span>
<span class="normal">2868</span>
<span class="normal">2869</span>
<span class="normal">2870</span>
<span class="normal">2871</span>
<span class="normal">2872</span>
<span class="normal">2873</span>
<span class="normal">2874</span>
<span class="normal">2875</span>
<span class="normal">2876</span>
<span class="normal">2877</span>
<span class="normal">2878</span>
<span class="normal">2879</span>
<span class="normal">2880</span>
<span class="normal">2881</span>
<span class="normal">2882</span>
<span class="normal">2883</span>
<span class="normal">2884</span>
<span class="normal">2885</span>
<span class="normal">2886</span>
<span class="normal">2887</span>
<span class="normal">2888</span>
<span class="normal">2889</span>
<span class="normal">2890</span>
<span class="normal">2891</span>
<span class="normal">2892</span>
<span class="normal">2893</span>
<span class="normal">2894</span>
<span class="normal">2895</span>
<span class="normal">2896</span>
<span class="normal">2897</span>
<span class="normal">2898</span>
<span class="normal">2899</span>
<span class="normal">2900</span>
<span class="normal">2901</span>
<span class="normal">2902</span>
<span class="normal">2903</span>
<span class="normal">2904</span>
<span class="normal">2905</span>
<span class="normal">2906</span>
<span class="normal">2907</span>
<span class="normal">2908</span>
<span class="normal">2909</span>
<span class="normal">2910</span>
<span class="normal">2911</span>
<span class="normal">2912</span>
<span class="normal">2913</span>
<span class="normal">2914</span>
<span class="normal">2915</span>
<span class="normal">2916</span>
<span class="normal">2917</span>
<span class="normal">2918</span>
<span class="normal">2919</span>
<span class="normal">2920</span>
<span class="normal">2921</span>
<span class="normal">2922</span>
<span class="normal">2923</span>
<span class="normal">2924</span>
<span class="normal">2925</span>
<span class="normal">2926</span>
<span class="normal">2927</span>
<span class="normal">2928</span>
<span class="normal">2929</span>
<span class="normal">2930</span>
<span class="normal">2931</span>
<span class="normal">2932</span>
<span class="normal">2933</span>
<span class="normal">2934</span>
<span class="normal">2935</span>
<span class="normal">2936</span>
<span class="normal">2937</span>
<span class="normal">2938</span>
<span class="normal">2939</span>
<span class="normal">2940</span>
<span class="normal">2941</span>
<span class="normal">2942</span>
<span class="normal">2943</span>
<span class="normal">2944</span>
<span class="normal">2945</span>
<span class="normal">2946</span>
<span class="normal">2947</span>
<span class="normal">2948</span>
<span class="normal">2949</span>
<span class="normal">2950</span>
<span class="normal">2951</span>
<span class="normal">2952</span>
<span class="normal">2953</span>
<span class="normal">2954</span>
<span class="normal">2955</span>
<span class="normal">2956</span>
<span class="normal">2957</span>
<span class="normal">2958</span>
<span class="normal">2959</span>
<span class="normal">2960</span>
<span class="normal">2961</span>
<span class="normal">2962</span>
<span class="normal">2963</span>
<span class="normal">2964</span>
<span class="normal">2965</span>
<span class="normal">2966</span>
<span class="normal">2967</span>
<span class="normal">2968</span>
<span class="normal">2969</span>
<span class="normal">2970</span>
<span class="normal">2971</span>
<span class="normal">2972</span>
<span class="normal">2973</span>
<span class="normal">2974</span>
<span class="normal">2975</span>
<span class="normal">2976</span>
<span class="normal">2977</span>
<span class="normal">2978</span>
<span class="normal">2979</span>
<span class="normal">2980</span>
<span class="normal">2981</span>
<span class="normal">2982</span>
<span class="normal">2983</span>
<span class="normal">2984</span>
<span class="normal">2985</span>
<span class="normal">2986</span>
<span class="normal">2987</span>
<span class="normal">2988</span>
<span class="normal">2989</span>
<span class="normal">2990</span>
<span class="normal">2991</span>
<span class="normal">2992</span>
<span class="normal">2993</span>
<span class="normal">2994</span>
<span class="normal">2995</span>
<span class="normal">2996</span>
<span class="normal">2997</span>
<span class="normal">2998</span>
<span class="normal">2999</span>
<span class="normal">3000</span>
<span class="normal">3001</span>
<span class="normal">3002</span>
<span class="normal">3003</span>
<span class="normal">3004</span>
<span class="normal">3005</span>
<span class="normal">3006</span>
<span class="normal">3007</span>
<span class="normal">3008</span>
<span class="normal">3009</span>
<span class="normal">3010</span>
<span class="normal">3011</span>
<span class="normal">3012</span>
<span class="normal">3013</span>
<span class="normal">3014</span>
<span class="normal">3015</span>
<span class="normal">3016</span>
<span class="normal">3017</span>
<span class="normal">3018</span>
<span class="normal">3019</span>
<span class="normal">3020</span>
<span class="normal">3021</span>
<span class="normal">3022</span>
<span class="normal">3023</span>
<span class="normal">3024</span>
<span class="normal">3025</span>
<span class="normal">3026</span>
<span class="normal">3027</span>
<span class="normal">3028</span>
<span class="normal">3029</span>
<span class="normal">3030</span>
<span class="normal">3031</span>
<span class="normal">3032</span>
<span class="normal">3033</span>
<span class="normal">3034</span>
<span class="normal">3035</span>
<span class="normal">3036</span>
<span class="normal">3037</span>
<span class="normal">3038</span>
<span class="normal">3039</span>
<span class="normal">3040</span>
<span class="normal">3041</span>
<span class="normal">3042</span>
<span class="normal">3043</span>
<span class="normal">3044</span>
<span class="normal">3045</span>
<span class="normal">3046</span>
<span class="normal">3047</span>
<span class="normal">3048</span>
<span class="normal">3049</span>
<span class="normal">3050</span>
<span class="normal">3051</span>
<span class="normal">3052</span>
<span class="normal">3053</span>
<span class="normal">3054</span>
<span class="normal">3055</span>
<span class="normal">3056</span>
<span class="normal">3057</span>
<span class="normal">3058</span>
<span class="normal">3059</span>
<span class="normal">3060</span>
<span class="normal">3061</span>
<span class="normal">3062</span>
<span class="normal">3063</span>
<span class="normal">3064</span>
<span class="normal">3065</span>
<span class="normal">3066</span>
<span class="normal">3067</span>
<span class="normal">3068</span>
<span class="normal">3069</span>
<span class="normal">3070</span>
<span class="normal">3071</span>
<span class="normal">3072</span>
<span class="normal">3073</span>
<span class="normal">3074</span>
<span class="normal">3075</span>
<span class="normal">3076</span>
<span class="normal">3077</span>
<span class="normal">3078</span>
<span class="normal">3079</span>
<span class="normal">3080</span>
<span class="normal">3081</span>
<span class="normal">3082</span>
<span class="normal">3083</span>
<span class="normal">3084</span>
<span class="normal">3085</span>
<span class="normal">3086</span>
<span class="normal">3087</span>
<span class="normal">3088</span>
<span class="normal">3089</span>
<span class="normal">3090</span>
<span class="normal">3091</span>
<span class="normal">3092</span>
<span class="normal">3093</span>
<span class="normal">3094</span>
<span class="normal">3095</span>
<span class="normal">3096</span>
<span class="normal">3097</span>
<span class="normal">3098</span>
<span class="normal">3099</span>
<span class="normal">3100</span>
<span class="normal">3101</span>
<span class="normal">3102</span>
<span class="normal">3103</span>
<span class="normal">3104</span>
<span class="normal">3105</span>
<span class="normal">3106</span>
<span class="normal">3107</span>
<span class="normal">3108</span>
<span class="normal">3109</span>
<span class="normal">3110</span>
<span class="normal">3111</span>
<span class="normal">3112</span>
<span class="normal">3113</span>
<span class="normal">3114</span>
<span class="normal">3115</span>
<span class="normal">3116</span>
<span class="normal">3117</span>
<span class="normal">3118</span>
<span class="normal">3119</span>
<span class="normal">3120</span>
<span class="normal">3121</span>
<span class="normal">3122</span>
<span class="normal">3123</span>
<span class="normal">3124</span>
<span class="normal">3125</span>
<span class="normal">3126</span>
<span class="normal">3127</span>
<span class="normal">3128</span>
<span class="normal">3129</span>
<span class="normal">3130</span>
<span class="normal">3131</span>
<span class="normal">3132</span>
<span class="normal">3133</span>
<span class="normal">3134</span>
<span class="normal">3135</span>
<span class="normal">3136</span>
<span class="normal">3137</span>
<span class="normal">3138</span>
<span class="normal">3139</span>
<span class="normal">3140</span>
<span class="normal">3141</span>
<span class="normal">3142</span>
<span class="normal">3143</span>
<span class="normal">3144</span>
<span class="normal">3145</span>
<span class="normal">3146</span>
<span class="normal">3147</span>
<span class="normal">3148</span>
<span class="normal">3149</span>
<span class="normal">3150</span>
<span class="normal">3151</span>
<span class="normal">3152</span>
<span class="normal">3153</span>
<span class="normal">3154</span>
<span class="normal">3155</span>
<span class="normal">3156</span>
<span class="normal">3157</span>
<span class="normal">3158</span>
<span class="normal">3159</span>
<span class="normal">3160</span>
<span class="normal">3161</span>
<span class="normal">3162</span>
<span class="normal">3163</span>
<span class="normal">3164</span>
<span class="normal">3165</span>
<span class="normal">3166</span>
<span class="normal">3167</span>
<span class="normal">3168</span>
<span class="normal">3169</span>
<span class="normal">3170</span>
<span class="normal">3171</span>
<span class="normal">3172</span>
<span class="normal">3173</span>
<span class="normal">3174</span>
<span class="normal">3175</span>
<span class="normal">3176</span>
<span class="normal">3177</span>
<span class="normal">3178</span>
<span class="normal">3179</span>
<span class="normal">3180</span>
<span class="normal">3181</span>
<span class="normal">3182</span>
<span class="normal">3183</span>
<span class="normal">3184</span>
<span class="normal">3185</span>
<span class="normal">3186</span>
<span class="normal">3187</span>
<span class="normal">3188</span>
<span class="normal">3189</span>
<span class="normal">3190</span>
<span class="normal">3191</span>
<span class="normal">3192</span>
<span class="normal">3193</span>
<span class="normal">3194</span>
<span class="normal">3195</span>
<span class="normal">3196</span>
<span class="normal">3197</span>
<span class="normal">3198</span>
<span class="normal">3199</span>
<span class="normal">3200</span>
<span class="normal">3201</span>
<span class="normal">3202</span>
<span class="normal">3203</span>
<span class="normal">3204</span>
<span class="normal">3205</span>
<span class="normal">3206</span>
<span class="normal">3207</span>
<span class="normal">3208</span>
<span class="normal">3209</span>
<span class="normal">3210</span>
<span class="normal">3211</span>
<span class="normal">3212</span>
<span class="normal">3213</span>
<span class="normal">3214</span>
<span class="normal">3215</span>
<span class="normal">3216</span>
<span class="normal">3217</span>
<span class="normal">3218</span>
<span class="normal">3219</span>
<span class="normal">3220</span>
<span class="normal">3221</span>
<span class="normal">3222</span>
<span class="normal">3223</span>
<span class="normal">3224</span>
<span class="normal">3225</span>
<span class="normal">3226</span>
<span class="normal">3227</span>
<span class="normal">3228</span>
<span class="normal">3229</span>
<span class="normal">3230</span>
<span class="normal">3231</span>
<span class="normal">3232</span>
<span class="normal">3233</span>
<span class="normal">3234</span>
<span class="normal">3235</span>
<span class="normal">3236</span>
<span class="normal">3237</span>
<span class="normal">3238</span>
<span class="normal">3239</span>
<span class="normal">3240</span>
<span class="normal">3241</span>
<span class="normal">3242</span>
<span class="normal">3243</span>
<span class="normal">3244</span>
<span class="normal">3245</span>
<span class="normal">3246</span>
<span class="normal">3247</span>
<span class="normal">3248</span>
<span class="normal">3249</span>
<span class="normal">3250</span>
<span class="normal">3251</span>
<span class="normal">3252</span>
<span class="normal">3253</span>
<span class="normal">3254</span>
<span class="normal">3255</span>
<span class="normal">3256</span>
<span class="normal">3257</span>
<span class="normal">3258</span>
<span class="normal">3259</span>
<span class="normal">3260</span>
<span class="normal">3261</span>
<span class="normal">3262</span>
<span class="normal">3263</span>
<span class="normal">3264</span>
<span class="normal">3265</span>
<span class="normal">3266</span>
<span class="normal">3267</span>
<span class="normal">3268</span>
<span class="normal">3269</span>
<span class="normal">3270</span>
<span class="normal">3271</span>
<span class="normal">3272</span>
<span class="normal">3273</span>
<span class="normal">3274</span>
<span class="normal">3275</span>
<span class="normal">3276</span>
<span class="normal">3277</span>
<span class="normal">3278</span>
<span class="normal">3279</span>
<span class="normal">3280</span>
<span class="normal">3281</span>
<span class="normal">3282</span>
<span class="normal">3283</span>
<span class="normal">3284</span>
<span class="normal">3285</span>
<span class="normal">3286</span>
<span class="normal">3287</span>
<span class="normal">3288</span>
<span class="normal">3289</span>
<span class="normal">3290</span>
<span class="normal">3291</span>
<span class="normal">3292</span>
<span class="normal">3293</span>
<span class="normal">3294</span>
<span class="normal">3295</span>
<span class="normal">3296</span>
<span class="normal">3297</span>
<span class="normal">3298</span>
<span class="normal">3299</span>
<span class="normal">3300</span>
<span class="normal">3301</span>
<span class="normal">3302</span>
<span class="normal">3303</span>
<span class="normal">3304</span>
<span class="normal">3305</span>
<span class="normal">3306</span>
<span class="normal">3307</span>
<span class="normal">3308</span>
<span class="normal">3309</span>
<span class="normal">3310</span>
<span class="normal">3311</span>
<span class="normal">3312</span>
<span class="normal">3313</span>
<span class="normal">3314</span>
<span class="normal">3315</span>
<span class="normal">3316</span>
<span class="normal">3317</span>
<span class="normal">3318</span>
<span class="normal">3319</span>
<span class="normal">3320</span>
<span class="normal">3321</span>
<span class="normal">3322</span>
<span class="normal">3323</span>
<span class="normal">3324</span>
<span class="normal">3325</span>
<span class="normal">3326</span>
<span class="normal">3327</span>
<span class="normal">3328</span>
<span class="normal">3329</span>
<span class="normal">3330</span>
<span class="normal">3331</span>
<span class="normal">3332</span>
<span class="normal">3333</span>
<span class="normal">3334</span>
<span class="normal">3335</span>
<span class="normal">3336</span>
<span class="normal">3337</span>
<span class="normal">3338</span>
<span class="normal">3339</span>
<span class="normal">3340</span>
<span class="normal">3341</span>
<span class="normal">3342</span>
<span class="normal">3343</span>
<span class="normal">3344</span>
<span class="normal">3345</span>
<span class="normal">3346</span>
<span class="normal">3347</span>
<span class="normal">3348</span>
<span class="normal">3349</span>
<span class="normal">3350</span>
<span class="normal">3351</span>
<span class="normal">3352</span>
<span class="normal">3353</span>
<span class="normal">3354</span>
<span class="normal">3355</span>
<span class="normal">3356</span>
<span class="normal">3357</span>
<span class="normal">3358</span>
<span class="normal">3359</span>
<span class="normal">3360</span>
<span class="normal">3361</span>
<span class="normal">3362</span>
<span class="normal">3363</span>
<span class="normal">3364</span>
<span class="normal">3365</span>
<span class="normal">3366</span>
<span class="normal">3367</span>
<span class="normal">3368</span>
<span class="normal">3369</span>
<span class="normal">3370</span>
<span class="normal">3371</span>
<span class="normal">3372</span>
<span class="normal">3373</span>
<span class="normal">3374</span>
<span class="normal">3375</span>
<span class="normal">3376</span>
<span class="normal">3377</span>
<span class="normal">3378</span>
<span class="normal">3379</span>
<span class="normal">3380</span>
<span class="normal">3381</span>
<span class="normal">3382</span>
<span class="normal">3383</span>
<span class="normal">3384</span>
<span class="normal">3385</span>
<span class="normal">3386</span>
<span class="normal">3387</span>
<span class="normal">3388</span>
<span class="normal">3389</span>
<span class="normal">3390</span>
<span class="normal">3391</span>
<span class="normal">3392</span>
<span class="normal">3393</span>
<span class="normal">3394</span>
<span class="normal">3395</span>
<span class="normal">3396</span>
<span class="normal">3397</span>
<span class="normal">3398</span>
<span class="normal">3399</span>
<span class="normal">3400</span>
<span class="normal">3401</span>
<span class="normal">3402</span>
<span class="normal">3403</span>
<span class="normal">3404</span>
<span class="normal">3405</span>
<span class="normal">3406</span>
<span class="normal">3407</span>
<span class="normal">3408</span>
<span class="normal">3409</span>
<span class="normal">3410</span>
<span class="normal">3411</span>
<span class="normal">3412</span>
<span class="normal">3413</span>
<span class="normal">3414</span>
<span class="normal">3415</span>
<span class="normal">3416</span>
<span class="normal">3417</span>
<span class="normal">3418</span>
<span class="normal">3419</span>
<span class="normal">3420</span>
<span class="normal">3421</span>
<span class="normal">3422</span>
<span class="normal">3423</span>
<span class="normal">3424</span>
<span class="normal">3425</span>
<span class="normal">3426</span>
<span class="normal">3427</span>
<span class="normal">3428</span>
<span class="normal">3429</span>
<span class="normal">3430</span>
<span class="normal">3431</span>
<span class="normal">3432</span>
<span class="normal">3433</span>
<span class="normal">3434</span>
<span class="normal">3435</span>
<span class="normal">3436</span>
<span class="normal">3437</span>
<span class="normal">3438</span>
<span class="normal">3439</span>
<span class="normal">3440</span>
<span class="normal">3441</span>
<span class="normal">3442</span>
<span class="normal">3443</span>
<span class="normal">3444</span>
<span class="normal">3445</span>
<span class="normal">3446</span>
<span class="normal">3447</span>
<span class="normal">3448</span>
<span class="normal">3449</span>
<span class="normal">3450</span>
<span class="normal">3451</span>
<span class="normal">3452</span>
<span class="normal">3453</span>
<span class="normal">3454</span>
<span class="normal">3455</span>
<span class="normal">3456</span>
<span class="normal">3457</span>
<span class="normal">3458</span>
<span class="normal">3459</span>
<span class="normal">3460</span>
<span class="normal">3461</span>
<span class="normal">3462</span>
<span class="normal">3463</span>
<span class="normal">3464</span>
<span class="normal">3465</span>
<span class="normal">3466</span>
<span class="normal">3467</span>
<span class="normal">3468</span>
<span class="normal">3469</span>
<span class="normal">3470</span>
<span class="normal">3471</span>
<span class="normal">3472</span>
<span class="normal">3473</span>
<span class="normal">3474</span>
<span class="normal">3475</span>
<span class="normal">3476</span>
<span class="normal">3477</span>
<span class="normal">3478</span>
<span class="normal">3479</span>
<span class="normal">3480</span>
<span class="normal">3481</span>
<span class="normal">3482</span>
<span class="normal">3483</span>
<span class="normal">3484</span>
<span class="normal">3485</span>
<span class="normal">3486</span>
<span class="normal">3487</span>
<span class="normal">3488</span>
<span class="normal">3489</span>
<span class="normal">3490</span>
<span class="normal">3491</span>
<span class="normal">3492</span>
<span class="normal">3493</span>
<span class="normal">3494</span>
<span class="normal">3495</span>
<span class="normal">3496</span>
<span class="normal">3497</span>
<span class="normal">3498</span>
<span class="normal">3499</span>
<span class="normal">3500</span>
<span class="normal">3501</span>
<span class="normal">3502</span>
<span class="normal">3503</span>
<span class="normal">3504</span>
<span class="normal">3505</span>
<span class="normal">3506</span>
<span class="normal">3507</span>
<span class="normal">3508</span>
<span class="normal">3509</span>
<span class="normal">3510</span>
<span class="normal">3511</span>
<span class="normal">3512</span>
<span class="normal">3513</span>
<span class="normal">3514</span>
<span class="normal">3515</span>
<span class="normal">3516</span>
<span class="normal">3517</span>
<span class="normal">3518</span>
<span class="normal">3519</span>
<span class="normal">3520</span>
<span class="normal">3521</span>
<span class="normal">3522</span>
<span class="normal">3523</span>
<span class="normal">3524</span>
<span class="normal">3525</span>
<span class="normal">3526</span>
<span class="normal">3527</span>
<span class="normal">3528</span>
<span class="normal">3529</span>
<span class="normal">3530</span>
<span class="normal">3531</span>
<span class="normal">3532</span>
<span class="normal">3533</span>
<span class="normal">3534</span>
<span class="normal">3535</span>
<span class="normal">3536</span>
<span class="normal">3537</span>
<span class="normal">3538</span>
<span class="normal">3539</span>
<span class="normal">3540</span>
<span class="normal">3541</span>
<span class="normal">3542</span>
<span class="normal">3543</span>
<span class="normal">3544</span>
<span class="normal">3545</span>
<span class="normal">3546</span>
<span class="normal">3547</span>
<span class="normal">3548</span>
<span class="normal">3549</span>
<span class="normal">3550</span>
<span class="normal">3551</span>
<span class="normal">3552</span>
<span class="normal">3553</span>
<span class="normal">3554</span>
<span class="normal">3555</span>
<span class="normal">3556</span>
<span class="normal">3557</span>
<span class="normal">3558</span>
<span class="normal">3559</span>
<span class="normal">3560</span>
<span class="normal">3561</span>
<span class="normal">3562</span>
<span class="normal">3563</span>
<span class="normal">3564</span>
<span class="normal">3565</span>
<span class="normal">3566</span>
<span class="normal">3567</span>
<span class="normal">3568</span>
<span class="normal">3569</span>
<span class="normal">3570</span>
<span class="normal">3571</span>
<span class="normal">3572</span>
<span class="normal">3573</span>
<span class="normal">3574</span>
<span class="normal">3575</span>
<span class="normal">3576</span>
<span class="normal">3577</span>
<span class="normal">3578</span>
<span class="normal">3579</span>
<span class="normal">3580</span>
<span class="normal">3581</span>
<span class="normal">3582</span>
<span class="normal">3583</span>
<span class="normal">3584</span>
<span class="normal">3585</span>
<span class="normal">3586</span>
<span class="normal">3587</span>
<span class="normal">3588</span>
<span class="normal">3589</span>
<span class="normal">3590</span>
<span class="normal">3591</span>
<span class="normal">3592</span>
<span class="normal">3593</span>
<span class="normal">3594</span>
<span class="normal">3595</span>
<span class="normal">3596</span>
<span class="normal">3597</span>
<span class="normal">3598</span>
<span class="normal">3599</span>
<span class="normal">3600</span>
<span class="normal">3601</span>
<span class="normal">3602</span>
<span class="normal">3603</span>
<span class="normal">3604</span>
<span class="normal">3605</span>
<span class="normal">3606</span>
<span class="normal">3607</span>
<span class="normal">3608</span>
<span class="normal">3609</span>
<span class="normal">3610</span>
<span class="normal">3611</span>
<span class="normal">3612</span>
<span class="normal">3613</span>
<span class="normal">3614</span>
<span class="normal">3615</span>
<span class="normal">3616</span>
<span class="normal">3617</span>
<span class="normal">3618</span>
<span class="normal">3619</span>
<span class="normal">3620</span>
<span class="normal">3621</span>
<span class="normal">3622</span>
<span class="normal">3623</span>
<span class="normal">3624</span>
<span class="normal">3625</span>
<span class="normal">3626</span>
<span class="normal">3627</span>
<span class="normal">3628</span>
<span class="normal">3629</span>
<span class="normal">3630</span>
<span class="normal">3631</span>
<span class="normal">3632</span>
<span class="normal">3633</span>
<span class="normal">3634</span>
<span class="normal">3635</span>
<span class="normal">3636</span>
<span class="normal">3637</span>
<span class="normal">3638</span>
<span class="normal">3639</span>
<span class="normal">3640</span>
<span class="normal">3641</span>
<span class="normal">3642</span>
<span class="normal">3643</span>
<span class="normal">3644</span>
<span class="normal">3645</span>
<span class="normal">3646</span>
<span class="normal">3647</span>
<span class="normal">3648</span>
<span class="normal">3649</span>
<span class="normal">3650</span>
<span class="normal">3651</span>
<span class="normal">3652</span>
<span class="normal">3653</span>
<span class="normal">3654</span>
<span class="normal">3655</span>
<span class="normal">3656</span>
<span class="normal">3657</span>
<span class="normal">3658</span>
<span class="normal">3659</span>
<span class="normal">3660</span>
<span class="normal">3661</span>
<span class="normal">3662</span>
<span class="normal">3663</span>
<span class="normal">3664</span>
<span class="normal">3665</span>
<span class="normal">3666</span>
<span class="normal">3667</span>
<span class="normal">3668</span>
<span class="normal">3669</span>
<span class="normal">3670</span>
<span class="normal">3671</span>
<span class="normal">3672</span>
<span class="normal">3673</span>
<span class="normal">3674</span>
<span class="normal">3675</span>
<span class="normal">3676</span>
<span class="normal">3677</span>
<span class="normal">3678</span>
<span class="normal">3679</span>
<span class="normal">3680</span>
<span class="normal">3681</span>
<span class="normal">3682</span>
<span class="normal">3683</span>
<span class="normal">3684</span>
<span class="normal">3685</span>
<span class="normal">3686</span>
<span class="normal">3687</span>
<span class="normal">3688</span>
<span class="normal">3689</span>
<span class="normal">3690</span>
<span class="normal">3691</span>
<span class="normal">3692</span>
<span class="normal">3693</span>
<span class="normal">3694</span>
<span class="normal">3695</span>
<span class="normal">3696</span>
<span class="normal">3697</span>
<span class="normal">3698</span>
<span class="normal">3699</span>
<span class="normal">3700</span>
<span class="normal">3701</span>
<span class="normal">3702</span>
<span class="normal">3703</span>
<span class="normal">3704</span>
<span class="normal">3705</span>
<span class="normal">3706</span>
<span class="normal">3707</span>
<span class="normal">3708</span>
<span class="normal">3709</span>
<span class="normal">3710</span>
<span class="normal">3711</span>
<span class="normal">3712</span>
<span class="normal">3713</span>
<span class="normal">3714</span>
<span class="normal">3715</span>
<span class="normal">3716</span>
<span class="normal">3717</span>
<span class="normal">3718</span>
<span class="normal">3719</span>
<span class="normal">3720</span>
<span class="normal">3721</span>
<span class="normal">3722</span>
<span class="normal">3723</span>
<span class="normal">3724</span>
<span class="normal">3725</span>
<span class="normal">3726</span>
<span class="normal">3727</span>
<span class="normal">3728</span>
<span class="normal">3729</span>
<span class="normal">3730</span>
<span class="normal">3731</span>
<span class="normal">3732</span>
<span class="normal">3733</span>
<span class="normal">3734</span>
<span class="normal">3735</span>
<span class="normal">3736</span>
<span class="normal">3737</span>
<span class="normal">3738</span>
<span class="normal">3739</span>
<span class="normal">3740</span>
<span class="normal">3741</span>
<span class="normal">3742</span>
<span class="normal">3743</span>
<span class="normal">3744</span>
<span class="normal">3745</span>
<span class="normal">3746</span>
<span class="normal">3747</span>
<span class="normal">3748</span>
<span class="normal">3749</span>
<span class="normal">3750</span>
<span class="normal">3751</span>
<span class="normal">3752</span>
<span class="normal">3753</span>
<span class="normal">3754</span>
<span class="normal">3755</span>
<span class="normal">3756</span>
<span class="normal">3757</span>
<span class="normal">3758</span>
<span class="normal">3759</span>
<span class="normal">3760</span>
<span class="normal">3761</span>
<span class="normal">3762</span>
<span class="normal">3763</span>
<span class="normal">3764</span>
<span class="normal">3765</span>
<span class="normal">3766</span>
<span class="normal">3767</span>
<span class="normal">3768</span>
<span class="normal">3769</span>
<span class="normal">3770</span>
<span class="normal">3771</span>
<span class="normal">3772</span>
<span class="normal">3773</span>
<span class="normal">3774</span>
<span class="normal">3775</span>
<span class="normal">3776</span>
<span class="normal">3777</span>
<span class="normal">3778</span>
<span class="normal">3779</span>
<span class="normal">3780</span>
<span class="normal">3781</span>
<span class="normal">3782</span>
<span class="normal">3783</span>
<span class="normal">3784</span>
<span class="normal">3785</span>
<span class="normal">3786</span>
<span class="normal">3787</span>
<span class="normal">3788</span>
<span class="normal">3789</span>
<span class="normal">3790</span>
<span class="normal">3791</span>
<span class="normal">3792</span>
<span class="normal">3793</span>
<span class="normal">3794</span>
<span class="normal">3795</span>
<span class="normal">3796</span>
<span class="normal">3797</span>
<span class="normal">3798</span>
<span class="normal">3799</span>
<span class="normal">3800</span>
<span class="normal">3801</span>
<span class="normal">3802</span>
<span class="normal">3803</span>
<span class="normal">3804</span>
<span class="normal">3805</span>
<span class="normal">3806</span>
<span class="normal">3807</span>
<span class="normal">3808</span>
<span class="normal">3809</span>
<span class="normal">3810</span>
<span class="normal">3811</span>
<span class="normal">3812</span>
<span class="normal">3813</span>
<span class="normal">3814</span>
<span class="normal">3815</span>
<span class="normal">3816</span>
<span class="normal">3817</span>
<span class="normal">3818</span>
<span class="normal">3819</span>
<span class="normal">3820</span>
<span class="normal">3821</span>
<span class="normal">3822</span>
<span class="normal">3823</span>
<span class="normal">3824</span>
<span class="normal">3825</span>
<span class="normal">3826</span>
<span class="normal">3827</span>
<span class="normal">3828</span>
<span class="normal">3829</span>
<span class="normal">3830</span>
<span class="normal">3831</span>
<span class="normal">3832</span>
<span class="normal">3833</span>
<span class="normal">3834</span>
<span class="normal">3835</span>
<span class="normal">3836</span>
<span class="normal">3837</span>
<span class="normal">3838</span>
<span class="normal">3839</span>
<span class="normal">3840</span>
<span class="normal">3841</span>
<span class="normal">3842</span>
<span class="normal">3843</span>
<span class="normal">3844</span>
<span class="normal">3845</span>
<span class="normal">3846</span>
<span class="normal">3847</span>
<span class="normal">3848</span>
<span class="normal">3849</span>
<span class="normal">3850</span>
<span class="normal">3851</span>
<span class="normal">3852</span>
<span class="normal">3853</span>
<span class="normal">3854</span>
<span class="normal">3855</span>
<span class="normal">3856</span>
<span class="normal">3857</span>
<span class="normal">3858</span>
<span class="normal">3859</span>
<span class="normal">3860</span>
<span class="normal">3861</span>
<span class="normal">3862</span>
<span class="normal">3863</span>
<span class="normal">3864</span>
<span class="normal">3865</span>
<span class="normal">3866</span>
<span class="normal">3867</span>
<span class="normal">3868</span>
<span class="normal">3869</span>
<span class="normal">3870</span>
<span class="normal">3871</span>
<span class="normal">3872</span>
<span class="normal">3873</span>
<span class="normal">3874</span>
<span class="normal">3875</span>
<span class="normal">3876</span>
<span class="normal">3877</span>
<span class="normal">3878</span>
<span class="normal">3879</span>
<span class="normal">3880</span>
<span class="normal">3881</span>
<span class="normal">3882</span>
<span class="normal">3883</span>
<span class="normal">3884</span>
<span class="normal">3885</span>
<span class="normal">3886</span>
<span class="normal">3887</span>
<span class="normal">3888</span>
<span class="normal">3889</span>
<span class="normal">3890</span>
<span class="normal">3891</span>
<span class="normal">3892</span>
<span class="normal">3893</span>
<span class="normal">3894</span>
<span class="normal">3895</span>
<span class="normal">3896</span>
<span class="normal">3897</span>
<span class="normal">3898</span>
<span class="normal">3899</span>
<span class="normal">3900</span>
<span class="normal">3901</span>
<span class="normal">3902</span>
<span class="normal">3903</span>
<span class="normal">3904</span>
<span class="normal">3905</span>
<span class="normal">3906</span>
<span class="normal">3907</span>
<span class="normal">3908</span>
<span class="normal">3909</span>
<span class="normal">3910</span>
<span class="normal">3911</span>
<span class="normal">3912</span>
<span class="normal">3913</span>
<span class="normal">3914</span>
<span class="normal">3915</span>
<span class="normal">3916</span>
<span class="normal">3917</span>
<span class="normal">3918</span>
<span class="normal">3919</span>
<span class="normal">3920</span>
<span class="normal">3921</span>
<span class="normal">3922</span>
<span class="normal">3923</span>
<span class="normal">3924</span>
<span class="normal">3925</span>
<span class="normal">3926</span>
<span class="normal">3927</span>
<span class="normal">3928</span>
<span class="normal">3929</span>
<span class="normal">3930</span>
<span class="normal">3931</span>
<span class="normal">3932</span>
<span class="normal">3933</span>
<span class="normal">3934</span>
<span class="normal">3935</span>
<span class="normal">3936</span>
<span class="normal">3937</span>
<span class="normal">3938</span>
<span class="normal">3939</span>
<span class="normal">3940</span>
<span class="normal">3941</span>
<span class="normal">3942</span>
<span class="normal">3943</span>
<span class="normal">3944</span>
<span class="normal">3945</span>
<span class="normal">3946</span>
<span class="normal">3947</span>
<span class="normal">3948</span>
<span class="normal">3949</span>
<span class="normal">3950</span>
<span class="normal">3951</span>
<span class="normal">3952</span>
<span class="normal">3953</span>
<span class="normal">3954</span>
<span class="normal">3955</span>
<span class="normal">3956</span>
<span class="normal">3957</span>
<span class="normal">3958</span>
<span class="normal">3959</span>
<span class="normal">3960</span>
<span class="normal">3961</span>
<span class="normal">3962</span>
<span class="normal">3963</span>
<span class="normal">3964</span>
<span class="normal">3965</span>
<span class="normal">3966</span>
<span class="normal">3967</span>
<span class="normal">3968</span>
<span class="normal">3969</span>
<span class="normal">3970</span>
<span class="normal">3971</span>
<span class="normal">3972</span>
<span class="normal">3973</span>
<span class="normal">3974</span>
<span class="normal">3975</span>
<span class="normal">3976</span>
<span class="normal">3977</span>
<span class="normal">3978</span>
<span class="normal">3979</span>
<span class="normal">3980</span>
<span class="normal">3981</span>
<span class="normal">3982</span>
<span class="normal">3983</span>
<span class="normal">3984</span>
<span class="normal">3985</span>
<span class="normal">3986</span>
<span class="normal">3987</span>
<span class="normal">3988</span>
<span class="normal">3989</span>
<span class="normal">3990</span>
<span class="normal">3991</span>
<span class="normal">3992</span>
<span class="normal">3993</span>
<span class="normal">3994</span>
<span class="normal">3995</span>
<span class="normal">3996</span>
<span class="normal">3997</span>
<span class="normal">3998</span>
<span class="normal">3999</span>
<span class="normal">4000</span>
<span class="normal">4001</span>
<span class="normal">4002</span>
<span class="normal">4003</span>
<span class="normal">4004</span>
<span class="normal">4005</span>
<span class="normal">4006</span>
<span class="normal">4007</span>
<span class="normal">4008</span>
<span class="normal">4009</span>
<span class="normal">4010</span>
<span class="normal">4011</span>
<span class="normal">4012</span>
<span class="normal">4013</span>
<span class="normal">4014</span>
<span class="normal">4015</span>
<span class="normal">4016</span>
<span class="normal">4017</span>
<span class="normal">4018</span>
<span class="normal">4019</span>
<span class="normal">4020</span>
<span class="normal">4021</span>
<span class="normal">4022</span>
<span class="normal">4023</span>
<span class="normal">4024</span>
<span class="normal">4025</span>
<span class="normal">4026</span>
<span class="normal">4027</span>
<span class="normal">4028</span>
<span class="normal">4029</span>
<span class="normal">4030</span>
<span class="normal">4031</span>
<span class="normal">4032</span>
<span class="normal">4033</span>
<span class="normal">4034</span>
<span class="normal">4035</span>
<span class="normal">4036</span>
<span class="normal">4037</span>
<span class="normal">4038</span>
<span class="normal">4039</span>
<span class="normal">4040</span>
<span class="normal">4041</span>
<span class="normal">4042</span>
<span class="normal">4043</span>
<span class="normal">4044</span>
<span class="normal">4045</span>
<span class="normal">4046</span>
<span class="normal">4047</span>
<span class="normal">4048</span>
<span class="normal">4049</span>
<span class="normal">4050</span>
<span class="normal">4051</span>
<span class="normal">4052</span>
<span class="normal">4053</span>
<span class="normal">4054</span>
<span class="normal">4055</span>
<span class="normal">4056</span>
<span class="normal">4057</span>
<span class="normal">4058</span>
<span class="normal">4059</span>
<span class="normal">4060</span>
<span class="normal">4061</span>
<span class="normal">4062</span>
<span class="normal">4063</span>
<span class="normal">4064</span>
<span class="normal">4065</span>
<span class="normal">4066</span>
<span class="normal">4067</span>
<span class="normal">4068</span>
<span class="normal">4069</span>
<span class="normal">4070</span>
<span class="normal">4071</span>
<span class="normal">4072</span>
<span class="normal">4073</span>
<span class="normal">4074</span>
<span class="normal">4075</span>
<span class="normal">4076</span>
<span class="normal">4077</span>
<span class="normal">4078</span>
<span class="normal">4079</span>
<span class="normal">4080</span>
<span class="normal">4081</span>
<span class="normal">4082</span>
<span class="normal">4083</span>
<span class="normal">4084</span>
<span class="normal">4085</span>
<span class="normal">4086</span>
<span class="normal">4087</span>
<span class="normal">4088</span>
<span class="normal">4089</span>
<span class="normal">4090</span>
<span class="normal">4091</span>
<span class="normal">4092</span>
<span class="normal">4093</span>
<span class="normal">4094</span>
<span class="normal">4095</span>
<span class="normal">4096</span>
<span class="normal">4097</span>
<span class="normal">4098</span>
<span class="normal">4099</span>
<span class="normal">4100</span>
<span class="normal">4101</span>
<span class="normal">4102</span>
<span class="normal">4103</span>
<span class="normal">4104</span>
<span class="normal">4105</span>
<span class="normal">4106</span>
<span class="normal">4107</span>
<span class="normal">4108</span>
<span class="normal">4109</span>
<span class="normal">4110</span>
<span class="normal">4111</span>
<span class="normal">4112</span>
<span class="normal">4113</span>
<span class="normal">4114</span>
<span class="normal">4115</span>
<span class="normal">4116</span>
<span class="normal">4117</span>
<span class="normal">4118</span>
<span class="normal">4119</span>
<span class="normal">4120</span>
<span class="normal">4121</span>
<span class="normal">4122</span>
<span class="normal">4123</span>
<span class="normal">4124</span>
<span class="normal">4125</span>
<span class="normal">4126</span>
<span class="normal">4127</span>
<span class="normal">4128</span>
<span class="normal">4129</span>
<span class="normal">4130</span>
<span class="normal">4131</span>
<span class="normal">4132</span>
<span class="normal">4133</span>
<span class="normal">4134</span>
<span class="normal">4135</span>
<span class="normal">4136</span>
<span class="normal">4137</span>
<span class="normal">4138</span>
<span class="normal">4139</span>
<span class="normal">4140</span>
<span class="normal">4141</span>
<span class="normal">4142</span>
<span class="normal">4143</span>
<span class="normal">4144</span>
<span class="normal">4145</span>
<span class="normal">4146</span>
<span class="normal">4147</span>
<span class="normal">4148</span>
<span class="normal">4149</span>
<span class="normal">4150</span>
<span class="normal">4151</span>
<span class="normal">4152</span>
<span class="normal">4153</span>
<span class="normal">4154</span>
<span class="normal">4155</span>
<span class="normal">4156</span>
<span class="normal">4157</span>
<span class="normal">4158</span>
<span class="normal">4159</span>
<span class="normal">4160</span>
<span class="normal">4161</span>
<span class="normal">4162</span>
<span class="normal">4163</span>
<span class="normal">4164</span>
<span class="normal">4165</span>
<span class="normal">4166</span>
<span class="normal">4167</span>
<span class="normal">4168</span>
<span class="normal">4169</span>
<span class="normal">4170</span>
<span class="normal">4171</span>
<span class="normal">4172</span>
<span class="normal">4173</span>
<span class="normal">4174</span>
<span class="normal">4175</span>
<span class="normal">4176</span>
<span class="normal">4177</span>
<span class="normal">4178</span>
<span class="normal">4179</span>
<span class="normal">4180</span>
<span class="normal">4181</span>
<span class="normal">4182</span>
<span class="normal">4183</span>
<span class="normal">4184</span>
<span class="normal">4185</span>
<span class="normal">4186</span>
<span class="normal">4187</span>
<span class="normal">4188</span>
<span class="normal">4189</span>
<span class="normal">4190</span>
<span class="normal">4191</span>
<span class="normal">4192</span>
<span class="normal">4193</span>
<span class="normal">4194</span>
<span class="normal">4195</span>
<span class="normal">4196</span>
<span class="normal">4197</span>
<span class="normal">4198</span>
<span class="normal">4199</span>
<span class="normal">4200</span>
<span class="normal">4201</span>
<span class="normal">4202</span>
<span class="normal">4203</span>
<span class="normal">4204</span>
<span class="normal">4205</span>
<span class="normal">4206</span>
<span class="normal">4207</span>
<span class="normal">4208</span>
<span class="normal">4209</span>
<span class="normal">4210</span>
<span class="normal">4211</span>
<span class="normal">4212</span>
<span class="normal">4213</span>
<span class="normal">4214</span>
<span class="normal">4215</span>
<span class="normal">4216</span>
<span class="normal">4217</span>
<span class="normal">4218</span>
<span class="normal">4219</span>
<span class="normal">4220</span>
<span class="normal">4221</span>
<span class="normal">4222</span>
<span class="normal">4223</span>
<span class="normal">4224</span>
<span class="normal">4225</span>
<span class="normal">4226</span>
<span class="normal">4227</span>
<span class="normal">4228</span>
<span class="normal">4229</span>
<span class="normal">4230</span>
<span class="normal">4231</span>
<span class="normal">4232</span>
<span class="normal">4233</span>
<span class="normal">4234</span>
<span class="normal">4235</span>
<span class="normal">4236</span>
<span class="normal">4237</span>
<span class="normal">4238</span>
<span class="normal">4239</span>
<span class="normal">4240</span>
<span class="normal">4241</span>
<span class="normal">4242</span>
<span class="normal">4243</span>
<span class="normal">4244</span>
<span class="normal">4245</span>
<span class="normal">4246</span>
<span class="normal">4247</span>
<span class="normal">4248</span>
<span class="normal">4249</span>
<span class="normal">4250</span>
<span class="normal">4251</span>
<span class="normal">4252</span>
<span class="normal">4253</span>
<span class="normal">4254</span>
<span class="normal">4255</span>
<span class="normal">4256</span>
<span class="normal">4257</span>
<span class="normal">4258</span>
<span class="normal">4259</span>
<span class="normal">4260</span>
<span class="normal">4261</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">GenerationMixin</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class containing all functions for auto-regressive text generation, to be used as a mixin in [`PreTrainedModel`].</span>

<span class="sd">    The class exposes [`~generation.GenerationMixin.generate`], which can be used for:</span>
<span class="sd">        - *greedy decoding* if `num_beams=1` and `do_sample=False`</span>
<span class="sd">        - *contrastive search* if `penalty_alpha&gt;0` and `top_k&gt;1`</span>
<span class="sd">        - *multinomial sampling* if `num_beams=1` and `do_sample=True`</span>
<span class="sd">        - *beam-search decoding* if `num_beams&gt;1` and `do_sample=False`</span>
<span class="sd">        - *beam-search multinomial sampling* if `num_beams&gt;1` and `do_sample=True`</span>
<span class="sd">        - *diverse beam-search decoding* if `num_beams&gt;1` and `num_beam_groups&gt;1`</span>
<span class="sd">        - *constrained beam-search decoding* if `constraints!=None` or `force_words_ids!=None`</span>
<span class="sd">        - *assisted decoding* if `assistant_model` or `prompt_lookup_num_tokens` is passed to `.generate()`</span>

<span class="sd">    To learn more about decoding strategies refer to the [text generation strategies guide](../generation_strategies).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;A model class needs to define a `prepare_inputs_for_generation` method in order to use `.generate()`.&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_prepare_model_inputs</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function extracts the model-specific `inputs` for generation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># 1. retrieve all kwargs that are non-None or non-model input related.</span>
        <span class="c1"># some encoder-decoder models have different names for model and encoder</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
            <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;encoder&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">main_input_name</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">main_input_name</span>
        <span class="p">):</span>
            <span class="n">input_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">main_input_name</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">main_input_name</span>

        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">k</span> <span class="o">!=</span> <span class="n">input_name</span><span class="p">}</span>

        <span class="c1"># 2. check whether model_input_name is passed as kwarg</span>
        <span class="c1"># if yes and `inputs` is None use kwarg inputs</span>
        <span class="n">inputs_kwarg</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">input_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">inputs_kwarg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`inputs`: </span><span class="si">{</span><span class="n">inputs</span><span class="si">}</span><span class="s2">` were passed alongside </span><span class="si">{</span><span class="n">input_name</span><span class="si">}</span><span class="s2"> which is not allowed. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Make sure to either pass </span><span class="si">{</span><span class="n">inputs</span><span class="si">}</span><span class="s2"> or </span><span class="si">{</span><span class="n">input_name</span><span class="si">}</span><span class="s2">=...&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">inputs_kwarg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs_kwarg</span>

        <span class="c1"># 3. In the presence of `inputs_embeds` for text models:</span>
        <span class="c1"># - decoder-only models should complain if the user attempts to pass `inputs_embeds`, but the model</span>
        <span class="c1"># doesn&#39;t have its forwarding implemented. `inputs_embeds` is kept in `model_kwargs` and can coexist with</span>
        <span class="c1"># input_ids (`inputs_embeds` will be used in the 1st generation step, as opposed to `input_ids`)</span>
        <span class="c1"># - encoder-decoder models should complain if the user attempts to pass `inputs_embeds` and `input_ids`, and</span>
        <span class="c1"># pull the former to inputs. It will be used in place of `input_ids` to get the encoder hidden states.</span>
        <span class="k">if</span> <span class="n">input_name</span> <span class="o">==</span> <span class="s2">&quot;input_ids&quot;</span> <span class="ow">and</span> <span class="s2">&quot;inputs_embeds&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                <span class="n">has_inputs_embeds_forwarding</span> <span class="o">=</span> <span class="s2">&quot;inputs_embeds&quot;</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span>
                    <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">has_inputs_embeds_forwarding</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;You passed `inputs_embeds` to `.generate()`, but the model class </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> &quot;</span>
                        <span class="s2">&quot;doesn&#39;t have its forwarding implemented. See the GPT2 implementation for an example &quot;</span>
                        <span class="s2">&quot;(https://github.com/huggingface/transformers/pull/21405), and feel free to open a PR with it!&quot;</span>
                    <span class="p">)</span>
                <span class="c1"># In this case, `input_ids` is moved to the `model_kwargs`, so a few automations (like the creation of</span>
                <span class="c1"># the attention mask) can rely on the actual model input.</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_initialize_input_ids_for_generation</span><span class="p">(</span>
                    <span class="n">inputs</span><span class="p">,</span> <span class="n">bos_token_id</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You passed `inputs_embeds` and `input_ids` to `.generate()`. Please pick one.&quot;</span><span class="p">)</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">input_name</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">],</span> <span class="s2">&quot;inputs_embeds&quot;</span>

        <span class="c1"># 4. if `inputs` is still None, try to create `input_ids` from BOS token</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_initialize_input_ids_for_generation</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">bos_token_id</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">input_name</span><span class="p">,</span> <span class="n">model_kwargs</span>

    <span class="k">def</span> <span class="nf">_maybe_initialize_input_ids_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes input ids for generation, if necessary.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">inputs</span>

        <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">and</span> <span class="n">encoder_outputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># make dummy input_ids with value -100, as a sanity check ensuring that they won&#39;t be used for encoding</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mi">100</span>

        <span class="c1"># If there is some tensor in `model_kwargs`, we can infer the batch size from it. This is helpful with</span>
        <span class="c1"># soft-prompting or in multimodal implementations built on top of decoder-only language models.</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">batch_size</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">break</span>

        <span class="k">if</span> <span class="s2">&quot;inputs_embeds&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
            <span class="c1"># avoid some operators do not support dim size 0</span>
            <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">bos_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`bos_token_id` has to be defined when no `input_ids` are provided.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span> <span class="o">*</span> <span class="n">bos_token_id</span>

    <span class="k">def</span> <span class="nf">_prepare_attention_mask_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># No information for attention mask inference -&gt; return default attention mask</span>
        <span class="n">default_attention_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">default_attention_mask</span>

        <span class="n">is_input_ids</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_input_ids</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">default_attention_mask</span>

        <span class="c1"># Otherwise we have may have information -&gt; try to infer the attention mask</span>

        <span class="n">is_pad_token_in_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">elements</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">test_elements</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
        <span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">is_pad_token_not_equal_to_eos_token_id</span> <span class="o">=</span> <span class="p">(</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="o">~</span><span class="p">(</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">elements</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">test_elements</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
        <span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">can_infer_attention_mask</span> <span class="o">=</span> <span class="n">is_pad_token_in_inputs</span> <span class="ow">and</span> <span class="n">is_pad_token_not_equal_to_eos_token_id</span>
        <span class="n">attention_mask_from_padding</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

        <span class="n">attention_mask</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">attention_mask_from_padding</span> <span class="o">*</span> <span class="n">can_infer_attention_mask</span> <span class="o">+</span> <span class="n">default_attention_mask</span> <span class="o">*</span> <span class="p">(</span><span class="ow">not</span> <span class="n">can_infer_attention_mask</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">attention_mask</span>

    <span class="k">def</span> <span class="nf">_prepare_encoder_decoder_kwargs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs_tensor</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">model_kwargs</span><span class="p">,</span>
        <span class="n">model_input_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="c1"># 1. get encoder</span>
        <span class="n">encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">()</span>
        <span class="c1"># Compatibility with Accelerate big model inference: we need the encoder to outputs stuff on the same device</span>
        <span class="c1"># as the inputs.</span>
        <span class="c1"># if hasattr(self, &quot;hf_device_map&quot;):</span>
        <span class="c1">#     if hasattr(encoder, &quot;_hf_hook&quot;):</span>
        <span class="c1">#         encoder._hf_hook.io_same_device = True</span>
        <span class="c1">#     else:</span>
        <span class="c1">#         add_hook_to_module(encoder, AlignDevicesHook(io_same_device=True))</span>

        <span class="c1"># 2. Prepare encoder args and encoder kwargs from model kwargs and generation config.</span>
        <span class="n">irrelevant_prefix</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;decoder_&quot;</span><span class="p">,</span> <span class="s2">&quot;cross_attn&quot;</span><span class="p">,</span> <span class="s2">&quot;use_cache&quot;</span><span class="p">]</span>
        <span class="n">encoder_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">argument</span><span class="p">:</span> <span class="n">value</span>
            <span class="k">for</span> <span class="n">argument</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">argument</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">irrelevant_prefix</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="n">encoder_signature</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>
        <span class="n">encoder_accepts_wildcard</span> <span class="o">=</span> <span class="s2">&quot;kwargs&quot;</span> <span class="ow">in</span> <span class="n">encoder_signature</span> <span class="ow">or</span> <span class="s2">&quot;model_kwargs&quot;</span> <span class="ow">in</span> <span class="n">encoder_signature</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">encoder_accepts_wildcard</span><span class="p">:</span>
            <span class="n">encoder_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">argument</span><span class="p">:</span> <span class="n">value</span> <span class="k">for</span> <span class="n">argument</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">encoder_kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">argument</span> <span class="ow">in</span> <span class="n">encoder_signature</span>
            <span class="p">}</span>
        <span class="n">encoder_kwargs</span><span class="p">[</span><span class="s2">&quot;output_attentions&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">encoder_kwargs</span><span class="p">[</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_hidden_states</span>

        <span class="c1"># 3. make sure that encoder returns `ModelOutput`</span>
        <span class="n">model_input_name</span> <span class="o">=</span> <span class="n">model_input_name</span> <span class="k">if</span> <span class="n">model_input_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">main_input_name</span>
        <span class="n">encoder_kwargs</span><span class="p">[</span><span class="s2">&quot;return_dict&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">encoder_kwargs</span><span class="p">[</span><span class="n">model_input_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs_tensor</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]:</span> <span class="n">ModelOutput</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="o">**</span><span class="n">encoder_kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">model_kwargs</span>

    <span class="k">def</span> <span class="nf">_prepare_decoder_input_ids_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">model_input_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">decoder_start_token_id</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Prepares `decoder_input_ids` for generation with encoder-decoder models&quot;&quot;&quot;</span>
        <span class="c1"># 1. Check whether the user has defined `decoder_input_ids` manually. To facilitate in terms of input naming,</span>
        <span class="c1"># we also allow the user to pass it under `input_ids`, if the encoder does not use it as the main input.</span>
        <span class="k">if</span> <span class="n">model_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;decoder_input_ids&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
            <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="s2">&quot;input_ids&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span> <span class="ow">and</span> <span class="n">model_input_name</span> <span class="o">!=</span> <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span>
            <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;input_ids&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># 2. `decoder_start_token_id` must have shape (batch_size, 1)</span>
        <span class="k">if</span> <span class="n">decoder_start_token_id</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">decoder_start_token_id</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">batch_size</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;`decoder_start_token_id` expected to have length </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="n">decoder_start_token_id</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="n">decoder_start_token_id</span> <span class="o">=</span> <span class="n">decoder_start_token_id</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">decoder_start_token_id</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span> <span class="o">*</span> <span class="n">decoder_start_token_id</span>
            <span class="p">)</span>

        <span class="c1"># 3. Encoder-decoder models expect the `decoder_input_ids` to start with a special token. Let&#39;s ensure that.</span>
        <span class="c1"># no user input -&gt; use decoder_start_token_id as decoder_input_ids</span>
        <span class="k">if</span> <span class="n">decoder_input_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">decoder_start_token_id</span>
        <span class="c1"># exception: Donut checkpoints have task-specific decoder starts and don&#39;t expect a BOS token. Note that the</span>
        <span class="c1"># original checkpoints can&#39;t be detected through `self.__class__.__name__.lower()`, needing custom logic.</span>
        <span class="c1"># See: https://github.com/huggingface/transformers/pull/31470</span>
        <span class="k">elif</span> <span class="s2">&quot;donut&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;vision-encoder-decoder&quot;</span> <span class="ow">and</span> <span class="s2">&quot;donut&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">model_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="k">pass</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;whisper&quot;</span><span class="p">]:</span>
            <span class="k">pass</span>
        <span class="c1"># user input but doesn&#39;t start with decoder_start_token_id -&gt; prepend decoder_start_token_id (and adjust</span>
        <span class="c1"># decoder_attention_mask if provided)</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">decoder_input_ids</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">decoder_start_token_id</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">():</span>
            <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">decoder_start_token_id</span><span class="p">,</span> <span class="n">decoder_input_ids</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;decoder_attention_mask&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
                <span class="n">decoder_attention_mask</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;decoder_attention_mask&quot;</span><span class="p">]</span>
                <span class="n">decoder_attention_mask</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                    <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">decoder_attention_mask</span><span class="p">)[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="n">decoder_attention_mask</span><span class="p">),</span>
                    <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;decoder_attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">decoder_attention_mask</span>

        <span class="k">return</span> <span class="n">decoder_input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_expand_inputs_for_generation</span><span class="p">(</span>
        <span class="n">expand_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">is_encoder_decoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Expands tensors from [batch_size, ...] to [batch_size * expand_size, ...]&quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">_expand_dict_for_generation</span><span class="p">(</span><span class="n">dict_to_expand</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">dict_to_expand</span><span class="p">:</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">key</span> <span class="o">!=</span> <span class="s2">&quot;cache_position&quot;</span>
                    <span class="ow">and</span> <span class="n">dict_to_expand</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dict_to_expand</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                <span class="p">):</span>
                    <span class="n">dict_to_expand</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">dict_to_expand</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">expand_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">dict_to_expand</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="mi">0</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">expand_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">expand_size</span><span class="p">,</span> <span class="o">*</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="n">_expand_dict_for_generation</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.&quot;</span><span class="p">)</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_expand_dict_for_generation</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span>

    <span class="k">def</span> <span class="nf">_extract_past_from_model_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">ModelOutput</span><span class="p">):</span>
        <span class="n">past_key_values</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">cache_name</span> <span class="o">=</span> <span class="s2">&quot;past_key_values&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;past_key_values&quot;</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
            <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">past_key_values</span>
        <span class="k">elif</span> <span class="s2">&quot;mems&quot;</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
            <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">mems</span>
        <span class="k">elif</span> <span class="s2">&quot;past_buckets_states&quot;</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
            <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">past_buckets_states</span>
        <span class="k">elif</span> <span class="s2">&quot;cache_params&quot;</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
            <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">cache_params</span>
            <span class="n">cache_name</span> <span class="o">=</span> <span class="s2">&quot;cache_params&quot;</span>

        <span class="k">return</span> <span class="n">cache_name</span><span class="p">,</span> <span class="n">past_key_values</span>

    <span class="k">def</span> <span class="nf">_update_model_kwargs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">ModelOutput</span><span class="p">,</span>
        <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">is_encoder_decoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">num_new_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="c1"># update past_key_values keeping its naming used in model code</span>
        <span class="n">cache_name</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_past_from_model_output</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="n">cache_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">cache</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="s2">&quot;state&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">state</span>

        <span class="c1"># update token_type_ids with last value</span>
        <span class="k">if</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
            <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">token_type_ids</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="c1"># update attention mask</span>
            <span class="k">if</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">dtype</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># update decoder attention mask</span>
            <span class="k">if</span> <span class="s2">&quot;decoder_attention_mask&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
                <span class="n">decoder_attention_mask</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;decoder_attention_mask&quot;</span><span class="p">]</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;decoder_attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">decoder_attention_mask</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">decoder_attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">decoder_attention_mask</span><span class="o">.</span><span class="n">dtype</span><span class="p">)],</span>
                    <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;use_cache&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;cache_position&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;cache_position&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="n">num_new_tokens</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">past_positions</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_position&quot;</span><span class="p">)</span>
            <span class="n">new_positions</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
                <span class="n">past_positions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">past_positions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">+</span> <span class="n">num_new_tokens</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">past_positions</span><span class="o">.</span><span class="n">dtype</span>
            <span class="p">)</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;cache_position&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">past_positions</span><span class="p">,</span> <span class="n">new_positions</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">model_kwargs</span>

    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">past_key_values</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Make sure that a `_reorder_cache` function is correctly implemented in </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__module__</span><span class="si">}</span><span class="s2"> to&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; enable beam search for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_candidate_generator</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">inputs_tensor</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">assistant_model</span><span class="p">:</span> <span class="s2">&quot;PreTrainedModel&quot;</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">LogitsProcessorList</span><span class="p">,</span>
        <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CandidateGenerator</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the candidate generator to be used in `assisted_generation`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">prompt_lookup_num_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">candidate_generator</span> <span class="o">=</span> <span class="n">PromptLookupCandidateGenerator</span><span class="p">(</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span><span class="p">,</span>
                <span class="n">num_output_tokens</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">prompt_lookup_num_tokens</span><span class="p">,</span>
                <span class="n">max_matching_ngram_size</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_matching_ngram_size</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">candidate_generator</span> <span class="o">=</span> <span class="n">AssistedCandidateGenerator</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span>
                <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
                <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
                <span class="n">inputs_tensor</span><span class="o">=</span><span class="n">inputs_tensor</span><span class="p">,</span>
                <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">candidate_generator</span>

    <span class="k">def</span> <span class="nf">_get_logits_warper</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LogitsProcessorList</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This class returns a [`LogitsProcessorList`] list object that contains all relevant [`LogitsWarper`] instances</span>
<span class="sd">        used for multinomial sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># instantiate warpers list</span>
        <span class="n">warpers</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>

        <span class="c1"># In beam methods, we need to keep at least one non-eos token to explore continuations that might have a</span>
        <span class="c1"># better score (i.e. keep len(list(generation_config._eos_token_tensor)) + 1)</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="n">min_tokens_to_keep</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">min_tokens_to_keep</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">min_tokens_to_keep</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">min_tokens_to_keep</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="c1"># the following idea is largely copied from this PR: https://github.com/huggingface/transformers/pull/5420/files</span>
        <span class="c1"># all samplers can be found in `generation_utils_samplers.py`</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">temperature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">temperature</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="n">warpers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">TemperatureLogitsWarper</span><span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">temperature</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">top_k</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">warpers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">TopKLogitsWarper</span><span class="p">(</span><span class="n">top_k</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">top_k</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="n">min_tokens_to_keep</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">top_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">top_p</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="n">warpers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">TopPLogitsWarper</span><span class="p">(</span><span class="n">top_p</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">top_p</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="n">min_tokens_to_keep</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">min_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Applied after temperature scaling (see https://github.com/ggerganov/llama.cpp/pull/3841#issuecomment-2073826084)</span>
            <span class="n">warpers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">MinPLogitsWarper</span><span class="p">(</span><span class="n">min_p</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">min_p</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="n">min_tokens_to_keep</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">typical_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">typical_p</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="n">warpers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">TypicalLogitsWarper</span><span class="p">(</span><span class="n">mass</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">typical_p</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="n">min_tokens_to_keep</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">epsilon_cutoff</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="mf">0.0</span> <span class="o">&lt;</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">epsilon_cutoff</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="n">warpers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">EpsilonLogitsWarper</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">epsilon_cutoff</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="n">min_tokens_to_keep</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">eta_cutoff</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="mf">0.0</span> <span class="o">&lt;</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">eta_cutoff</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="n">warpers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">EtaLogitsWarper</span><span class="p">(</span>
                    <span class="n">epsilon</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">eta_cutoff</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="n">min_tokens_to_keep</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="c1"># `LogitNormalization` should always be the last logit processor, when present</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">renormalize_logits</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">warpers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LogitNormalization</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">warpers</span>

    <span class="k">def</span> <span class="nf">_get_logits_processor</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">,</span>
        <span class="n">input_ids_seq_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">encoder_input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">prefix_allowed_tokens_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">],</span>
        <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">negative_prompt_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">negative_prompt_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LogitsProcessorList</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This class returns a [`LogitsProcessorList`] list object that contains all relevant [`LogitsProcessor`]</span>
<span class="sd">        instances used to modify the scores of the language model head.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># instantiate processors list</span>
        <span class="n">processors</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">guidance_scale</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">guidance_scale</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">UnbatchedClassifierFreeGuidanceLogitsProcessor</span><span class="p">(</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">guidance_scale</span><span class="p">,</span>
                    <span class="bp">self</span><span class="p">,</span>
                    <span class="n">unconditional_ids</span><span class="o">=</span><span class="n">negative_prompt_ids</span><span class="p">,</span>
                    <span class="n">unconditional_attention_mask</span><span class="o">=</span><span class="n">negative_prompt_attention_mask</span><span class="p">,</span>
                    <span class="n">use_cache</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;use_cache&quot;</span><span class="p">],</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">sequence_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">SequenceBiasLogitsProcessor</span><span class="p">(</span><span class="n">sequence_bias</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">sequence_bias</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">diversity_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">diversity_penalty</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">HammingDiversityLogitsProcessor</span><span class="p">(</span>
                    <span class="n">diversity_penalty</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">diversity_penalty</span><span class="p">,</span>
                    <span class="n">num_beams</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
                    <span class="n">num_beam_groups</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beam_groups</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">generation_config</span><span class="o">.</span><span class="n">encoder_repetition_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">encoder_repetition_penalty</span> <span class="o">!=</span> <span class="mf">1.0</span>
        <span class="p">):</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">EncoderRepetitionPenaltyLogitsProcessor</span><span class="p">(</span>
                    <span class="n">penalty</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">encoder_repetition_penalty</span><span class="p">,</span>
                    <span class="n">encoder_input_ids</span><span class="o">=</span><span class="n">encoder_input_ids</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">repetition_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">repetition_penalty</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">RepetitionPenaltyLogitsProcessor</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">repetition_penalty</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">no_repeat_ngram_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">no_repeat_ngram_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">NoRepeatNGramLogitsProcessor</span><span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">no_repeat_ngram_size</span><span class="p">))</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">generation_config</span><span class="o">.</span><span class="n">encoder_no_repeat_ngram_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">encoder_no_repeat_ngram_size</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">EncoderNoRepeatNGramLogitsProcessor</span><span class="p">(</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">encoder_no_repeat_ngram_size</span><span class="p">,</span>
                    <span class="n">encoder_input_ids</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">bad_words_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">NoBadWordsLogitsProcessor</span><span class="p">(</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">bad_words_ids</span><span class="p">,</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">generation_config</span><span class="o">.</span><span class="n">min_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">min_length</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">MinLengthLogitsProcessor</span><span class="p">(</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">min_length</span><span class="p">,</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">generation_config</span><span class="o">.</span><span class="n">min_new_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">min_new_tokens</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">MinNewTokensLengthLogitsProcessor</span><span class="p">(</span>
                    <span class="n">input_ids_seq_length</span><span class="p">,</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">min_new_tokens</span><span class="p">,</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">prefix_allowed_tokens_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">PrefixConstrainedLogitsProcessor</span><span class="p">(</span>
                    <span class="n">prefix_allowed_tokens_fn</span><span class="p">,</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">//</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">num_beam_groups</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">forced_bos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">ForcedBOSTokenLogitsProcessor</span><span class="p">(</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">forced_bos_token_id</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">forced_eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">ForcedEOSTokenLogitsProcessor</span><span class="p">(</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">forced_eos_token_id</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">remove_invalid_values</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">InfNanRemoveLogitsProcessor</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">exponential_decay_length_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">ExponentialDecayLengthPenalty</span><span class="p">(</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">exponential_decay_length_penalty</span><span class="p">,</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span><span class="p">,</span>
                    <span class="n">input_ids_seq_length</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">suppress_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">SuppressTokensLogitsProcessor</span><span class="p">(</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">suppress_tokens</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">begin_suppress_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">begin_index</span> <span class="o">=</span> <span class="n">input_ids_seq_length</span>
            <span class="n">begin_index</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">begin_index</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">input_ids_seq_length</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">forced_bos_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">begin_index</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">forced_decoder_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># generation starts after the last token that is forced</span>
                <span class="n">begin_index</span> <span class="o">+=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">forced_decoder_ids</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">SuppressTokensAtBeginLogitsProcessor</span><span class="p">(</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">begin_suppress_tokens</span><span class="p">,</span>
                    <span class="n">begin_index</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">forced_decoder_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;You have explicitly specified `forced_decoder_ids`. &quot;</span>
                <span class="s2">&quot;This functionality has been deprecated and will throw an error. &quot;</span>
                <span class="s2">&quot;Please remove the `forced_decoder_ids` argument in favour of `input_ids` or `decoder_input_ids` respectively.&quot;</span><span class="p">,</span>
                <span class="ne">FutureWarning</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ForceTokensLogitsProcessor</span><span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">forced_decoder_ids</span><span class="p">,</span> <span class="n">_has_warned</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">watermarking_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">WatermarkLogitsProcessor</span><span class="p">(</span>
                    <span class="n">vocab_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span>
                    <span class="n">greenlist_ratio</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">watermarking_config</span><span class="o">.</span><span class="n">greenlist_ratio</span><span class="p">,</span>
                    <span class="n">bias</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">watermarking_config</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                    <span class="n">hashing_key</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">watermarking_config</span><span class="o">.</span><span class="n">hashing_key</span><span class="p">,</span>
                    <span class="n">seeding_scheme</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">watermarking_config</span><span class="o">.</span><span class="n">seeding_scheme</span><span class="p">,</span>
                    <span class="n">context_width</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">watermarking_config</span><span class="o">.</span><span class="n">context_width</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="n">processors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merge_criteria_processor_list</span><span class="p">(</span><span class="n">processors</span><span class="p">,</span> <span class="n">logits_processor</span><span class="p">)</span>
        <span class="c1"># `LogitNormalization` should always be the last logit processor, when present</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">renormalize_logits</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LogitNormalization</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">processors</span>

    <span class="k">def</span> <span class="nf">_get_stopping_criteria</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">StoppingCriteriaList</span><span class="p">],</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;PreTrainedTokenizerBase&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">StoppingCriteriaList</span><span class="p">:</span>
        <span class="n">criteria</span> <span class="o">=</span> <span class="n">StoppingCriteriaList</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;max_position_embeddings&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">criteria</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">MaxLengthCriteria</span><span class="p">(</span>
                    <span class="n">max_length</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
                    <span class="n">max_position_embeddings</span><span class="o">=</span><span class="n">max_position_embeddings</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_time</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">criteria</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">MaxTimeCriteria</span><span class="p">(</span><span class="n">max_time</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_time</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">stop_strings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">tokenizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;There are one or more stop strings, either in the arguments to `generate` or in the &quot;</span>
                    <span class="s2">&quot;model&#39;s generation config, but we could not locate a tokenizer. When generating with &quot;</span>
                    <span class="s2">&quot;stop strings, you must pass the model&#39;s tokenizer to the `tokenizer` argument of `generate`.&quot;</span>
                <span class="p">)</span>
            <span class="n">criteria</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">StopStringCriteria</span><span class="p">(</span><span class="n">stop_strings</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">stop_strings</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">criteria</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EosTokenCriteria</span><span class="p">(</span><span class="n">eos_token_id</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span><span class="p">))</span>
        <span class="n">criteria</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merge_criteria_processor_list</span><span class="p">(</span><span class="n">criteria</span><span class="p">,</span> <span class="n">stopping_criteria</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">criteria</span>

    <span class="k">def</span> <span class="nf">_merge_criteria_processor_list</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">default_list</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">,</span> <span class="n">StoppingCriteriaList</span><span class="p">],</span>
        <span class="n">custom_list</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">,</span> <span class="n">StoppingCriteriaList</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">,</span> <span class="n">StoppingCriteriaList</span><span class="p">]:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">custom_list</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">default_list</span>
        <span class="k">for</span> <span class="n">default</span> <span class="ow">in</span> <span class="n">default_list</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">custom</span> <span class="ow">in</span> <span class="n">custom_list</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">custom</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">type</span><span class="p">(</span><span class="n">default</span><span class="p">):</span>
                    <span class="n">object_type</span> <span class="o">=</span> <span class="s2">&quot;stopping criteria&quot;</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">custom</span><span class="p">,</span> <span class="n">StoppingCriteria</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;logits processor&quot;</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;A custom </span><span class="si">{</span><span class="n">object_type</span><span class="si">}</span><span class="s2"> of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">custom</span><span class="p">)</span><span class="si">}</span><span class="s2"> with values </span><span class="si">{</span><span class="n">custom</span><span class="si">}</span><span class="s2"> has been passed to&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot; `.generate()`, but it has already been created with the values </span><span class="si">{</span><span class="n">default</span><span class="si">}</span><span class="s2">. </span><span class="si">{</span><span class="n">default</span><span class="si">}</span><span class="s2"> has been&quot;</span>
                        <span class="s2">&quot; created by passing the corresponding arguments to generate or by the model&#39;s config default&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot; values. If you just want to change the default values of </span><span class="si">{</span><span class="n">object_type</span><span class="si">}</span><span class="s2"> consider passing&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot; them as arguments to `.generate()` instead of using a custom </span><span class="si">{</span><span class="n">object_type</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>
        <span class="n">default_list</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">custom_list</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">default_list</span>

    <span class="k">def</span> <span class="nf">compute_transition_scores</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sequences</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">scores</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">beam_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">normalize_logits</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the transition scores of sequences given the generation scores (and beam indices, if beam search was</span>
<span class="sd">        used). This is a convenient method to quicky obtain the scores of the selected tokens at generation time.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            sequences (`mindspore.Tensor`):</span>
<span class="sd">                The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or</span>
<span class="sd">                shorter if all batches finished early due to the `eos_token_id`.</span>
<span class="sd">            scores (`tuple(mindspore.Tensor)`):</span>
<span class="sd">                Transition scores for each vocabulary token at each generation step. Beam transition scores consisting</span>
<span class="sd">                of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.</span>
<span class="sd">                Tuple of `mindspore.Tensor` with up to `max_new_tokens` elements (one element for each generated token),</span>
<span class="sd">                with each tensor of shape `(batch_size*num_beams, config.vocab_size)`.</span>
<span class="sd">            beam_indices (`mindspore.Tensor`, *optional*):</span>
<span class="sd">                Beam indices of generated token id at each generation step. `mindspore.Tensor` of shape</span>
<span class="sd">                `(batch_size*num_return_sequences, sequence_length)`. Only required if a `num_beams&gt;1` at</span>
<span class="sd">                generate-time.</span>
<span class="sd">            normalize_logits (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to normalize the logits (which, for legacy reasons, may be unnormalized).</span>

<span class="sd">        Return:</span>
<span class="sd">            `mindspore.Tensor`: A `mindspore.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)` containing</span>
<span class="sd">                the transition scores (logits)</span>

<span class="sd">        Examples:</span>

<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; from transformers import GPT2Tokenizer, AutoModelForCausalLM</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>

<span class="sd">        &gt;&gt;&gt; tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)</span>
<span class="sd">        &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;openai-community/gpt2&quot;)</span>
<span class="sd">        &gt;&gt;&gt; tokenizer.pad_token_id = tokenizer.eos_token_id</span>
<span class="sd">        &gt;&gt;&gt; inputs = tokenizer([&quot;Today is&quot;], return_tensors=&quot;ms&quot;)</span>

<span class="sd">        &gt;&gt;&gt; # Example 1: Print the scores for each token generated with Greedy Search</span>
<span class="sd">        &gt;&gt;&gt; outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)</span>
<span class="sd">        &gt;&gt;&gt; transition_scores = model.compute_transition_scores(</span>
<span class="sd">        ...     outputs.sequences, outputs.scores, normalize_logits=True</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; # input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for</span>
<span class="sd">        &gt;&gt;&gt; # encoder-decoder models, like BART or T5.</span>
<span class="sd">        &gt;&gt;&gt; input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]</span>
<span class="sd">        &gt;&gt;&gt; generated_tokens = outputs.sequences[:, input_length:]</span>
<span class="sd">        &gt;&gt;&gt; for tok, score in zip(generated_tokens[0], transition_scores[0]):</span>
<span class="sd">        ...     # | token | token string | log probability | probability</span>
<span class="sd">        ...     print(f&quot;| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}&quot;)</span>
<span class="sd">        |   262 |  the     | -1.414 | 24.33%</span>
<span class="sd">        |  1110 |  day     | -2.609 | 7.36%</span>
<span class="sd">        |   618 |  when    | -2.010 | 13.40%</span>
<span class="sd">        |   356 |  we      | -1.859 | 15.58%</span>
<span class="sd">        |   460 |  can     | -2.508 | 8.14%</span>

<span class="sd">        &gt;&gt;&gt; # Example 2: Reconstruct the sequence scores from Beam Search</span>
<span class="sd">        &gt;&gt;&gt; outputs = model.generate(</span>
<span class="sd">        ...     **inputs,</span>
<span class="sd">        ...     max_new_tokens=5,</span>
<span class="sd">        ...     num_beams=4,</span>
<span class="sd">        ...     num_return_sequences=4,</span>
<span class="sd">        ...     return_dict_in_generate=True,</span>
<span class="sd">        ...     output_scores=True,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; transition_scores = model.compute_transition_scores(</span>
<span class="sd">        ...     outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; # If you sum the generated tokens&#39; scores and apply the length penalty, you&#39;ll get the sequence scores.</span>
<span class="sd">        &gt;&gt;&gt; # Tip 1: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the</span>
<span class="sd">        &gt;&gt;&gt; # use case, you might want to recompute it with `normalize_logits=True`.</span>
<span class="sd">        &gt;&gt;&gt; # Tip 2: the output length does NOT include the input length</span>
<span class="sd">        &gt;&gt;&gt; output_length = np.sum(transition_scores.numpy() &lt; 0, axis=1)</span>
<span class="sd">        &gt;&gt;&gt; length_penalty = model.generation_config.length_penalty</span>
<span class="sd">        &gt;&gt;&gt; reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)</span>
<span class="sd">        &gt;&gt;&gt; print(np.allclose(outputs.sequences_scores, reconstructed_scores))</span>
<span class="sd">        True</span>
<span class="sd">        ```&quot;&quot;&quot;</span>
        <span class="c1"># 1. In absence of `beam_indices`, we can assume that we come from e.g. greedy search, which is equivalent</span>
        <span class="c1"># to a beam search approach were the first (and only) beam is always selected</span>
        <span class="k">if</span> <span class="n">beam_indices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">beam_indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">beam_indices</span> <span class="o">=</span> <span class="n">beam_indices</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>

        <span class="c1"># 2. reshape scores as [batch_size*vocab_size, # generation steps] with # generation steps being</span>
        <span class="c1"># seq_len - input_length</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 3. Optionally normalize the logits (across the vocab dimension)</span>
        <span class="k">if</span> <span class="n">normalize_logits</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># 4. cut beam_indices to longest beam length</span>
        <span class="n">beam_indices_mask</span> <span class="o">=</span> <span class="n">beam_indices</span> <span class="o">&lt;</span> <span class="mi">0</span>
        <span class="n">max_beam_length</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beam_indices_mask</span><span class="o">.</span><span class="n">long</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">beam_indices</span> <span class="o">=</span> <span class="n">beam_indices</span><span class="p">[:,</span> <span class="p">:</span><span class="n">max_beam_length</span><span class="p">]</span>
        <span class="n">beam_indices_mask</span> <span class="o">=</span> <span class="n">beam_indices_mask</span><span class="p">[:,</span> <span class="p">:</span><span class="n">max_beam_length</span><span class="p">]</span>

        <span class="c1"># 5. Set indices of beams that finished early to 0; such indices will be masked correctly afterwards</span>
        <span class="n">beam_indices</span><span class="p">[</span><span class="n">beam_indices_mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># 6. multiply beam_indices with vocab size to gather correctly from scores</span>
        <span class="n">beam_sequence_indices</span> <span class="o">=</span> <span class="n">beam_indices</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>

        <span class="c1"># 7. Define which indices contributed to scores</span>
        <span class="n">cut_idx</span> <span class="o">=</span> <span class="n">sequences</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">max_beam_length</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:,</span> <span class="n">cut_idx</span><span class="p">:]</span> <span class="o">+</span> <span class="n">beam_sequence_indices</span>

        <span class="c1"># 8. Compute scores</span>
        <span class="n">transition_scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>

        <span class="c1"># 9. Mask out transition_scores of beams that stopped early</span>
        <span class="n">transition_scores</span><span class="p">[</span><span class="n">beam_indices_mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">return</span> <span class="n">transition_scores</span>

    <span class="k">def</span> <span class="nf">_validate_model_class</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Confirms that the model class is compatible with generation. If not, raises an exception that points to the</span>
<span class="sd">        right class to use.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">can_generate</span><span class="p">():</span>
            <span class="n">generate_compatible_mappings</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">MODEL_FOR_CAUSAL_LM_MAPPING</span><span class="p">,</span>
                <span class="n">MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING</span><span class="p">,</span>
                <span class="n">MODEL_FOR_VISION_2_SEQ_MAPPING</span><span class="p">,</span>
                <span class="n">MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING</span><span class="p">,</span>
                <span class="n">MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING</span><span class="p">,</span>
            <span class="p">]</span>
            <span class="n">generate_compatible_classes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">model_mapping</span> <span class="ow">in</span> <span class="n">generate_compatible_mappings</span><span class="p">:</span>
                <span class="n">supported_models</span> <span class="o">=</span> <span class="n">model_mapping</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">),</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">supported_models</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">generate_compatible_classes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">supported_models</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
            <span class="n">exception_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The current model class (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">) is not compatible with `.generate()`, as &quot;</span>
                <span class="s2">&quot;it doesn&#39;t have a language model head.&quot;</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">generate_compatible_classes</span><span class="p">:</span>
                <span class="n">exception_message</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot; Please use one of the following classes instead: </span><span class="si">{</span><span class="n">generate_compatible_classes</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">exception_message</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_assistant</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">assistant_model</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">assistant_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">assistant_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">attributes_to_check</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;encoder_attention_heads&quot;</span><span class="p">,</span> <span class="s2">&quot;encoder_ffn_dim&quot;</span><span class="p">,</span> <span class="s2">&quot;encoder_layers&quot;</span><span class="p">]</span>
            <span class="n">attributes_to_check</span> <span class="o">=</span> <span class="p">[</span><span class="n">attr</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">assistant_model</span><span class="o">.</span><span class="n">config</span><span class="p">)</span> <span class="k">if</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attributes_to_check</span><span class="p">]</span>
            <span class="n">are_equal</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span> <span class="o">==</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">assistant_model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">attributes_to_check</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">are_equal</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The main model and the assistant don&#39;t have compatible encoder-dependent input shapes. &quot;</span>
                    <span class="s2">&quot;Ensure you load the assistant with the correct encoder-decoder class, e.g. `AutoModelForSpeechSeq2Seq` for Whisper.&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">==</span> <span class="n">assistant_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Make sure the main and assistant model use the same tokenizer&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_model_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Validates model kwargs for generation. Generate argument typos will also be caught here.&quot;&quot;&quot;</span>
        <span class="c1"># If a `Cache` instance is passed, checks whether the model is compatible with it</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">Cache</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supports_cache_class</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> does not support an instance of `Cache` as `past_key_values`. Please &quot;</span>
                <span class="s2">&quot;check the model documentation for supported cache formats.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Excludes arguments that are handled before calling any model function</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">]:</span>
                <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="n">unused_model_args</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">model_args</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>
        <span class="c1"># `kwargs`/`model_kwargs` is often used to handle optional forward pass inputs like `attention_mask`. If</span>
        <span class="c1"># `prepare_inputs_for_generation` doesn&#39;t accept them, then a stricter check can be made ;)</span>
        <span class="k">if</span> <span class="s2">&quot;kwargs&quot;</span> <span class="ow">in</span> <span class="n">model_args</span> <span class="ow">or</span> <span class="s2">&quot;model_kwargs&quot;</span> <span class="ow">in</span> <span class="n">model_args</span><span class="p">:</span>
            <span class="n">model_args</span> <span class="o">|=</span> <span class="nb">set</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>

        <span class="c1"># Encoder-Decoder models may also need Encoder arguments from `model_kwargs`</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">base_model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

            <span class="c1"># allow encoder kwargs</span>
            <span class="n">encoder</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="c1"># `MusicgenForConditionalGeneration` has `text_encoder` and `audio_encoder`.</span>
            <span class="c1"># Also, it has `base_model_prefix = &quot;encoder_decoder&quot;` but there is no `self.encoder_decoder`</span>
            <span class="c1"># TODO: A better way to handle this.</span>
            <span class="k">if</span> <span class="n">encoder</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">base_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_model_args</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>
                <span class="n">model_args</span> <span class="o">|=</span> <span class="n">encoder_model_args</span>

            <span class="c1"># allow decoder kwargs</span>
            <span class="n">decoder</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;decoder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">decoder</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">base_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="s2">&quot;decoder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">decoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_model_args</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>
                <span class="n">model_args</span> <span class="o">|=</span> <span class="p">{</span><span class="sa">f</span><span class="s2">&quot;decoder_</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">decoder_model_args</span><span class="p">}</span>

            <span class="c1"># allow assistant_encoder_outputs to be passed if we&#39;re doing assisted generating</span>
            <span class="k">if</span> <span class="s2">&quot;assistant_encoder_outputs&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
                <span class="n">model_args</span> <span class="o">|=</span> <span class="p">{</span><span class="s2">&quot;assistant_encoder_outputs&quot;</span><span class="p">}</span>

        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_args</span><span class="p">:</span>
                <span class="n">unused_model_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">unused_model_args</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The following `model_kwargs` are not used by the model: </span><span class="si">{</span><span class="n">unused_model_args</span><span class="si">}</span><span class="s2"> (note: typos in the&quot;</span>
                <span class="s2">&quot; generate arguments will also show up in this list)&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_generated_length</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">generation_config</span><span class="p">,</span> <span class="n">input_ids_length</span><span class="p">,</span> <span class="n">has_default_max_length</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Performs validation related to the resulting generated length&quot;&quot;&quot;</span>

        <span class="c1"># Can&#39;t throw warnings/exceptions during compilation</span>
        <span class="c1"># if is_torchdynamo_compiling():</span>
        <span class="c1">#     return</span>

        <span class="c1"># 1. Max length warnings related to poor parameterization</span>
        <span class="k">if</span> <span class="n">has_default_max_length</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_new_tokens</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span> <span class="o">==</span> <span class="mi">20</span><span class="p">:</span>
            <span class="c1"># 20 is the default max_length of the generation config</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Using the model-agnostic default `max_length` (=</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="si">}</span><span class="s2">) to control the &quot;</span>
                <span class="s2">&quot;generation length. We recommend setting `max_new_tokens` to control the maximum length of the &quot;</span>
                <span class="s2">&quot;generation.&quot;</span><span class="p">,</span>
                <span class="ne">UserWarning</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">input_ids_length</span> <span class="o">&gt;=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">:</span>
            <span class="n">input_ids_string</span> <span class="o">=</span> <span class="s2">&quot;decoder_input_ids&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="s2">&quot;input_ids&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Input length of </span><span class="si">{</span><span class="n">input_ids_string</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">input_ids_length</span><span class="si">}</span><span class="s2">, but `max_length` is set to&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="si">}</span><span class="s2">. This can lead to unexpected behavior. You should consider&quot;</span>
                <span class="s2">&quot; increasing `max_length` or, better yet, setting `max_new_tokens`.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># 2. Min length warnings due to unfeasible parameter combinations</span>
        <span class="n">min_length_error_suffix</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot; Generation will stop at the defined maximum length. You should decrease the minimum length and/or &quot;</span>
            <span class="s2">&quot;increase the maximum length.&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">has_default_max_length</span><span class="p">:</span>
            <span class="n">min_length_error_suffix</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot; Note that `max_length` is set to </span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="si">}</span><span class="s2">, its default value.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">min_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">min_length</span> <span class="o">&gt;</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unfeasible length constraints: `min_length` (</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">min_length</span><span class="si">}</span><span class="s2">) is larger than&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; the maximum possible length (</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="si">}</span><span class="s2">).&quot;</span> <span class="o">+</span> <span class="n">min_length_error_suffix</span><span class="p">,</span>
                <span class="ne">UserWarning</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">min_new_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">min_length</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">min_new_tokens</span> <span class="o">+</span> <span class="n">input_ids_length</span>
            <span class="k">if</span> <span class="n">min_length</span> <span class="o">&gt;</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unfeasible length constraints: `min_new_tokens` (</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">min_new_tokens</span><span class="si">}</span><span class="s2">), when &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;added to the prompt length (</span><span class="si">{</span><span class="n">input_ids_length</span><span class="si">}</span><span class="s2">), is larger than&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; the maximum possible length (</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="si">}</span><span class="s2">).&quot;</span> <span class="o">+</span> <span class="n">min_length_error_suffix</span><span class="p">,</span>
                    <span class="ne">UserWarning</span><span class="p">,</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_prepare_generated_length</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">,</span>
        <span class="n">has_default_max_length</span><span class="p">,</span>
        <span class="n">has_default_min_length</span><span class="p">,</span>
        <span class="n">model_input_name</span><span class="p">,</span>
        <span class="n">input_ids_length</span><span class="p">,</span>
        <span class="n">inputs_tensor</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Prepared max and min length in generaion configs to avoid clashes between similar attributes&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_new_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">has_default_max_length</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Both `max_new_tokens` (=</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_new_tokens</span><span class="si">}</span><span class="s2">) and `max_length`(=&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="si">}</span><span class="s2">) seem to have been set. `max_new_tokens` will take precedence. &quot;</span>
                    <span class="s2">&quot;Please refer to the documentation for more information. &quot;</span>
                    <span class="s2">&quot;(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)&quot;</span>
                <span class="p">)</span>
            <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_new_tokens</span> <span class="o">+</span> <span class="n">input_ids_length</span>

        <span class="c1"># if both `inputs_embeds` and `input_ids` are passed, we do not correct the length</span>
        <span class="c1"># otherwise we need total length [inputs-embeds-len + new-tokens-len] to not go beyond indicated `max_length``</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="n">model_input_name</span> <span class="o">==</span> <span class="s2">&quot;inputs_embeds&quot;</span>
            <span class="ow">and</span> <span class="n">input_ids_length</span> <span class="o">!=</span> <span class="n">inputs_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
        <span class="p">):</span>
            <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span> <span class="o">-=</span> <span class="n">inputs_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># same for min length</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">min_new_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">has_default_min_length</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Both `min_new_tokens` (=</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">min_new_tokens</span><span class="si">}</span><span class="s2">) and `min_length`(=&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">min_length</span><span class="si">}</span><span class="s2">) seem to have been set. `min_new_tokens` will take precedence. &quot;</span>
                    <span class="s2">&quot;Please refer to the documentation for more information. &quot;</span>
                    <span class="s2">&quot;(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)&quot;</span>
                <span class="p">)</span>
            <span class="n">generation_config</span><span class="o">.</span><span class="n">min_length</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">min_new_tokens</span> <span class="o">+</span> <span class="n">input_ids_length</span>

        <span class="k">elif</span> <span class="p">(</span>
            <span class="n">model_input_name</span> <span class="o">==</span> <span class="s2">&quot;inputs_embeds&quot;</span>
            <span class="ow">and</span> <span class="n">input_ids_length</span> <span class="o">!=</span> <span class="n">inputs_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
        <span class="p">):</span>
            <span class="n">generation_config</span><span class="o">.</span><span class="n">min_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">min_length</span> <span class="o">-</span> <span class="n">inputs_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">generation_config</span>

    <span class="k">def</span> <span class="nf">_prepare_generation_config</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerationConfig</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">GenerationConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares the base generation config, then applies any generation configuration options from kwargs. This</span>
<span class="sd">        function handles retrocompatibility with respect to configuration files.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO joao: when we can detect `fullgraph=True` in `torch.compile` (https://github.com/pytorch/pytorch/pull/120400)</span>
        <span class="c1"># replace `is_torchdynamo_compiling` by the corresponding check. As it is, we are being too restrictive with</span>
        <span class="c1"># the parameterization in `fullgraph=False` so as to enable `fullgraph=True`.</span>
        <span class="c1"># priority: `generation_config` argument &gt; `model.generation_config` (the default generation config)</span>
        <span class="n">using_model_generation_config</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">generation_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,</span>
            <span class="c1"># three conditions must be met</span>
            <span class="c1"># 1) the generation config must have been created from the model config (`_from_model_config` field);</span>
            <span class="c1"># 2) the generation config must have seen no modification since its creation (the hash is the same);</span>
            <span class="c1"># 3) the user must have set generation parameters in the model config.</span>
            <span class="c1"># NOTE: `torch.compile` can&#39;t compile `hash`, this legacy support is disabled with compilation.</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">_from_model_config</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">_original_object_hash</span> <span class="o">==</span> <span class="nb">hash</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">)</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_has_non_default_generation_parameters</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="n">new_generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_model_config</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">new_generation_config</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                        <span class="s2">&quot;You have modified the pretrained model configuration to control generation. This is a&quot;</span>
                        <span class="s2">&quot; deprecated strategy to control generation.&quot;</span>
                        <span class="s2">&quot; Please use and modify the model generation configuration (see&quot;</span>
                        <span class="s2">&quot; https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )&quot;</span>
                    <span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span> <span class="o">=</span> <span class="n">new_generation_config</span>
            <span class="n">using_model_generation_config</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">generation_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span>
            <span class="n">using_model_generation_config</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># `torch.compile` can&#39;t compile `copy.deepcopy`, arguments in `kwargs` that are part of `generation_config`</span>
        <span class="c1"># will mutate the object with `.update`. As such, passing these arguments through `kwargs` is disabled -- an</span>
        <span class="c1"># exception will be raised in `_validate_model_kwargs`</span>
        <span class="n">generation_config</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">generation_config</span><span class="p">)</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># If `generation_config` is provided, let&#39;s fallback ALL special tokens to the default values for the model</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">using_model_generation_config</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">bos_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">generation_config</span><span class="o">.</span><span class="n">bos_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">bos_token_id</span>
            <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span>
            <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span>
            <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">decoder_start_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">generation_config</span><span class="o">.</span><span class="n">decoder_start_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>

        <span class="k">return</span> <span class="n">generation_config</span><span class="p">,</span> <span class="n">model_kwargs</span>

    <span class="k">def</span> <span class="nf">_get_initial_cache_position</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculates `cache_position` for the pre-fill stage based on `input_ids` and optionally past length&quot;&quot;&quot;</span>
        <span class="c1"># `torch.compile`-friendly `ops.arange` from a shape -- the lines below are equivalent to `ops.arange`</span>
        <span class="k">if</span> <span class="s2">&quot;inputs_embeds&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
            <span class="n">cache_position</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;inputs_embeds&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cache_position</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="n">past_length</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cache</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span>
            <span class="n">past_length</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cache</span><span class="p">,</span> <span class="n">Cache</span><span class="p">):</span>
                <span class="n">past_length</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">cache</span><span class="p">,</span> <span class="s2">&quot;get_seq_length&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">cache</span><span class="o">.</span><span class="n">get_seq_length</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">past_length</span> <span class="o">=</span> <span class="n">cache</span><span class="o">.</span><span class="n">get_seq_length</span><span class="p">()</span>

            <span class="n">cache_position</span> <span class="o">=</span> <span class="n">cache_position</span><span class="p">[</span><span class="n">past_length</span><span class="p">:]</span>

        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;cache_position&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cache_position</span>
        <span class="k">return</span> <span class="n">model_kwargs</span>

    <span class="k">def</span> <span class="nf">_get_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cache_implementation</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_cache_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Cache</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets a cache for `generate`, that will persist across calls. A new cache will only be initialized a</span>
<span class="sd">        new `generate` call requires a larger cache or uses a different batch size.</span>

<span class="sd">        Returns the resulting cache object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cache_cls</span><span class="p">:</span> <span class="n">Cache</span> <span class="o">=</span> <span class="n">NEED_SETUP_CACHE_CLASSES_MAPPING</span><span class="p">[</span><span class="n">cache_implementation</span><span class="p">]</span>
        <span class="n">requires_cross_attention_cache</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">or</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_cache&quot;</span><span class="p">):</span>
            <span class="n">cache_to_check</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">self_attention_cache</span> <span class="k">if</span> <span class="n">requires_cross_attention_cache</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span>

        <span class="k">if</span> <span class="n">cache_implementation</span> <span class="o">==</span> <span class="s2">&quot;sliding_window&quot;</span><span class="p">:</span>
            <span class="n">max_cache_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sliding_window</span><span class="p">,</span> <span class="n">max_cache_len</span><span class="p">)</span>

        <span class="n">need_new_cache</span> <span class="o">=</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_cache&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cache_to_check</span><span class="p">,</span> <span class="n">cache_cls</span><span class="p">))</span>
            <span class="ow">or</span> <span class="n">cache_to_check</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="o">!=</span> <span class="n">max_batch_size</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">cache_implementation</span> <span class="o">!=</span> <span class="s2">&quot;mamba&quot;</span><span class="p">:</span>
            <span class="n">need_new_cache</span> <span class="o">=</span> <span class="n">need_new_cache</span> <span class="ow">or</span> <span class="n">cache_to_check</span><span class="o">.</span><span class="n">max_cache_len</span> <span class="o">&lt;</span> <span class="n">max_cache_len</span>

        <span class="k">if</span> <span class="n">requires_cross_attention_cache</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_cache&quot;</span><span class="p">):</span>
            <span class="n">need_new_cache</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">need_new_cache</span>
                <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">cross_attention_cache</span><span class="o">.</span><span class="n">max_cache_len</span> <span class="o">!=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">need_new_cache</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;_pre_quantization_dtype&quot;</span><span class="p">):</span>
                <span class="n">cache_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_pre_quantization_dtype</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">cache_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>

            <span class="n">cache_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
                <span class="s2">&quot;max_batch_size&quot;</span><span class="p">:</span> <span class="n">max_batch_size</span><span class="p">,</span>
                <span class="s2">&quot;max_cache_len&quot;</span><span class="p">:</span> <span class="n">max_cache_len</span><span class="p">,</span>
                <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">cache_dtype</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span> <span class="o">=</span> <span class="n">cache_cls</span><span class="p">(</span><span class="o">**</span><span class="n">cache_kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">requires_cross_attention_cache</span><span class="p">:</span>
                <span class="n">encoder_kwargs</span> <span class="o">=</span> <span class="n">cache_kwargs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                <span class="n">encoder_kwargs</span><span class="p">[</span><span class="s2">&quot;max_cache_len&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span> <span class="o">=</span> <span class="n">EncoderDecoderCache</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">,</span> <span class="n">cache_cls</span><span class="p">(</span><span class="o">**</span><span class="n">encoder_kwargs</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span>

    <span class="k">def</span> <span class="nf">_supports_default_dynamic_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return `True` if current model can use a `DynamicCache` instance when initializing the `past_key_values`.</span>
<span class="sd">        This is mostly the same as `_supports_cache_class` attribute, but add exception for `Jamba` model which</span>
<span class="sd">        uses its own `HybridMambaAttentionDynamicCache` and do not need to initialize the Cache in advance in</span>
<span class="sd">        order to save memory (because no back and forth `to_legacy_cache` and `from_legacy_cache` will be performed</span>
<span class="sd">        for `HybridMambaAttentionDynamicCache`).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supports_cache_class</span> <span class="ow">and</span> <span class="s2">&quot;jamba&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_prepare_special_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">,</span>
        <span class="n">kwargs_has_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares the special tokens for generation, overwriting the generation config with their processed versions</span>
<span class="sd">        converted to tensor.</span>

<span class="sd">        Note that `generation_config` is changed in place and stops being serializable after this method is called.</span>
<span class="sd">        That is no problem if called within `generate` (`generation_config` is a local copy that doesn&#39;t leave the</span>
<span class="sd">        function). However, if called outside `generate`, consider creating a copy of `generation_config` first.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Convert special tokens to tensors</span>
        <span class="k">def</span> <span class="nf">_tensor_or_none</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">token</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">token</span>
            <span class="k">return</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

        <span class="n">bos_token_tensor</span> <span class="o">=</span> <span class="n">_tensor_or_none</span><span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">bos_token_id</span><span class="p">)</span>
        <span class="n">eos_token_tensor</span> <span class="o">=</span> <span class="n">_tensor_or_none</span><span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span>
        <span class="n">pad_token_tensor</span> <span class="o">=</span> <span class="n">_tensor_or_none</span><span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
        <span class="n">decoder_start_token_tensor</span> <span class="o">=</span> <span class="n">_tensor_or_none</span><span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">decoder_start_token_id</span><span class="p">)</span>

        <span class="c1"># for BC we also try to get `decoder_start_token_id` or `bos_token_id` (#30892)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">decoder_start_token_tensor</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">decoder_start_token_tensor</span> <span class="k">if</span> <span class="n">decoder_start_token_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">bos_token_tensor</span>
            <span class="p">)</span>

        <span class="c1"># We can have more than one eos token. Always treat it as a 1D tensor (when it exists).</span>
        <span class="k">if</span> <span class="n">eos_token_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">eos_token_tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">eos_token_tensor</span> <span class="o">=</span> <span class="n">eos_token_tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Set pad token if unset (and there are conditions to do so)</span>
        <span class="k">if</span> <span class="n">pad_token_tensor</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">eos_token_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">kwargs_has_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">kwargs_has_attention_mask</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;The attention mask and the pad token id were not set. As a consequence, you may observe &quot;</span>
                    <span class="s2">&quot;unexpected behavior. Please pass your input&#39;s `attention_mask` to obtain reliable results.&quot;</span>
                <span class="p">)</span>
            <span class="n">pad_token_tensor</span> <span class="o">=</span> <span class="n">eos_token_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Setting `pad_token_id` to `eos_token_id`:</span><span class="si">{</span><span class="n">pad_token_tensor</span><span class="si">}</span><span class="s2"> for open-end generation.&quot;</span><span class="p">)</span>

        <span class="c1"># Sanity checks/warnings</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">and</span> <span class="n">decoder_start_token_tensor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">eos_token_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">ops</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">elements</span><span class="o">=</span><span class="n">eos_token_tensor</span><span class="p">,</span> <span class="n">test_elements</span><span class="o">=</span><span class="n">pad_token_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="n">kwargs_has_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">kwargs_has_attention_mask</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
                    <span class="s2">&quot;The attention mask is not set and cannot be inferred from input because pad token is same as &quot;</span>
                    <span class="s2">&quot;eos token. As a consequence, you may observe unexpected behavior. Please pass your input&#39;s &quot;</span>
                    <span class="s2">&quot;`attention_mask` to obtain reliable results.&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="n">eos_token_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">eos_token_tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">eos_token_tensor</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`eos_token_id` should consist of positive integers, but is </span><span class="si">{</span><span class="n">eos_token_tensor</span><span class="si">}</span><span class="s2">. Your generation &quot;</span>
                <span class="s2">&quot;will not stop until the maximum length is reached. Depending on other flags, it may even crash.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Update generation config with the updated special tokens tensors</span>
        <span class="c1"># NOTE: this must be written into a different attribute name than the one holding the original special tokens</span>
        <span class="c1"># (in their non-tensor form), in order to enable end-to-end compilation. See</span>
        <span class="n">generation_config</span><span class="o">.</span><span class="n">_bos_token_tensor</span> <span class="o">=</span> <span class="n">bos_token_tensor</span>
        <span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span> <span class="o">=</span> <span class="n">eos_token_tensor</span>
        <span class="n">generation_config</span><span class="o">.</span><span class="n">_pad_token_tensor</span> <span class="o">=</span> <span class="n">pad_token_tensor</span>
        <span class="n">generation_config</span><span class="o">.</span><span class="n">_decoder_start_token_tensor</span> <span class="o">=</span> <span class="n">decoder_start_token_tensor</span>

    <span class="nd">@no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerationConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">StoppingCriteriaList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prefix_allowed_tokens_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">assistant_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;PreTrainedModel&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">streamer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;BaseStreamer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">negative_prompt_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">negative_prompt_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">GenerateOutput</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Generates sequences of token ids for models with a language modeling head.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the</span>
<span class="sd">        model&#39;s default generation configuration. You can override any `generation_config` by passing the corresponding</span>
<span class="sd">        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.</span>

<span class="sd">        For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">        guide](../generation_strategies).</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            inputs (`mindspore.Tensor` of varying shape depending on the modality, *optional*):</span>
<span class="sd">                The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the</span>
<span class="sd">                method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`</span>
<span class="sd">                should be in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of</span>
<span class="sd">                `input_ids`, `input_values`, `input_features`, or `pixel_values`.</span>
<span class="sd">            generation_config ([`~generation.GenerationConfig`], *optional*):</span>
<span class="sd">                The generation configuration to be used as base parametrization for the generation call. `**kwargs`</span>
<span class="sd">                passed to generate matching the attributes of `generation_config` will override them. If</span>
<span class="sd">                `generation_config` is not provided, the default will be used, which has the following loading</span>
<span class="sd">                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model</span>
<span class="sd">                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]&#39;s</span>
<span class="sd">                default values, whose documentation should be checked to parameterize generation.</span>
<span class="sd">            logits_processor (`LogitsProcessorList`, *optional*):</span>
<span class="sd">                Custom logits processors that complement the default logits processors built from arguments and</span>
<span class="sd">                generation config. If a logit processor is passed that is already created with the arguments or a</span>
<span class="sd">                generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">            stopping_criteria (`StoppingCriteriaList`, *optional*):</span>
<span class="sd">                Custom stopping criteria that complements the default stopping criteria built from arguments and a</span>
<span class="sd">                generation config. If a stopping criteria is passed that is already created with the arguments or a</span>
<span class="sd">                generation config an error is thrown. If your stopping criteria depends on the `scores` input, make</span>
<span class="sd">                sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is</span>
<span class="sd">                intended for advanced users.</span>
<span class="sd">            prefix_allowed_tokens_fn (`Callable[[int, mindspore.Tensor], List[int]]`, *optional*):</span>
<span class="sd">                If provided, this function constraints the beam search to allowed tokens only at each step. If not</span>
<span class="sd">                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and</span>
<span class="sd">                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned</span>
<span class="sd">                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful</span>
<span class="sd">                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity</span>
<span class="sd">                Retrieval](https://arxiv.org/abs/2010.00904).</span>
<span class="sd">            synced_gpus (`bool`, *optional*):</span>
<span class="sd">                Whether to continue running the while loop until max_length. Unless overridden this flag will be set to</span>
<span class="sd">                `True` under DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished</span>
<span class="sd">                generating before other GPUs. Otherwise it&#39;ll be set to `False`.</span>
<span class="sd">            assistant_model (`PreTrainedModel`, *optional*):</span>
<span class="sd">                An assistant model that can be used to accelerate generation. The assistant model must have the exact</span>
<span class="sd">                same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model</span>
<span class="sd">                is much faster than running generation with the model you&#39;re calling generate from. As such, the</span>
<span class="sd">                assistant model should be much smaller.</span>
<span class="sd">            streamer (`BaseStreamer`, *optional*):</span>
<span class="sd">                Streamer object that will be used to stream the generated sequences. Generated tokens are passed</span>
<span class="sd">                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.</span>
<span class="sd">            negative_prompt_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">                The negative prompt needed for some processors such as CFG. The batch size must match the input batch</span>
<span class="sd">                size. This is an experimental feature, subject to breaking API changes in future versions.</span>
<span class="sd">            negative_prompt_attention_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">                Attention_mask for `negative_prompt_ids`.</span>
<span class="sd">            kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">                Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be</span>
<span class="sd">                forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder</span>
<span class="sd">                specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.</span>

<span class="sd">        Return:</span>
<span class="sd">            [`~utils.ModelOutput`] or `mindspore.Tensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`</span>
<span class="sd">            or when `config.return_dict_in_generate=True`) or a `mindspore.Tensor`.</span>

<span class="sd">                If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible</span>
<span class="sd">                [`~utils.ModelOutput`] types are:</span>

<span class="sd">                    - [`~generation.GenerateDecoderOnlyOutput`],</span>
<span class="sd">                    - [`~generation.GenerateBeamDecoderOnlyOutput`]</span>

<span class="sd">                If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible</span>
<span class="sd">                [`~utils.ModelOutput`] types are:</span>

<span class="sd">                    - [`~generation.GenerateEncoderDecoderOutput`],</span>
<span class="sd">                    - [`~generation.GenerateBeamEncoderDecoderOutput`]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_run_profiler</span> <span class="o">=</span> <span class="n">parse_flag_from_env</span><span class="p">(</span><span class="s1">&#39;MS_ENABLE_RUNTIME_PROFILER&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1"># 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_model_class</span><span class="p">()</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;tokenizer&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># Pull this out first, we only use it for stopping criteria</span>
        <span class="n">generation_config</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_generation_config</span><span class="p">(</span><span class="n">generation_config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_model_kwargs</span><span class="p">(</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_assistant</span><span class="p">(</span><span class="n">assistant_model</span><span class="p">)</span>

        <span class="c1"># 2. Set generation parameters if not already defined</span>
        <span class="k">if</span> <span class="n">synced_gpus</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># if is_deepspeed_zero3_enabled() and dist.get_world_size() &gt; 1:</span>
            <span class="c1">#     synced_gpus = True</span>
            <span class="c1"># else:</span>
            <span class="n">synced_gpus</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">logits_processor</span> <span class="o">=</span> <span class="n">logits_processor</span> <span class="k">if</span> <span class="n">logits_processor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>
        <span class="n">stopping_criteria</span> <span class="o">=</span> <span class="n">stopping_criteria</span> <span class="k">if</span> <span class="n">stopping_criteria</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">StoppingCriteriaList</span><span class="p">()</span>

        <span class="n">accepts_attention_mask</span> <span class="o">=</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">requires_attention_mask</span> <span class="o">=</span> <span class="s2">&quot;encoder_outputs&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_kwargs</span>
        <span class="n">kwargs_has_attention_mask</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="c1"># 3. Define model inputs</span>
        <span class="n">inputs_tensor</span><span class="p">,</span> <span class="n">model_input_name</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_model_inputs</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">bos_token_id</span><span class="p">,</span> <span class="n">model_kwargs</span>
        <span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">inputs_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_special_tokens</span><span class="p">(</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">kwargs_has_attention_mask</span><span class="p">)</span>

        <span class="c1"># decoder-only models must use left-padding for batched generation.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="c1"># If `input_ids` was given, check if the last id in any sequence is `pad_token_id`</span>
            <span class="c1"># Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">generation_config</span><span class="o">.</span><span class="n">_pad_token_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">1</span>
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
                <span class="ow">and</span> <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">inputs_tensor</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_pad_token_tensor</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;A decoder-only architecture is being used, but right-padding was detected! For correct &quot;</span>
                    <span class="s2">&quot;generation results, please set `padding_side=&#39;left&#39;` when initializing the tokenizer.&quot;</span>
                <span class="p">)</span>

        <span class="c1"># 4. Define other model kwargs</span>
        <span class="c1"># decoder-only models with inputs_embeds forwarding must use caching (otherwise we can&#39;t detect whether we are</span>
        <span class="c1"># generating the first new token or not, and we only want to use the embeddings for the first new token)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">and</span> <span class="n">model_input_name</span> <span class="o">==</span> <span class="s2">&quot;inputs_embeds&quot;</span><span class="p">:</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;use_cache&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;use_cache&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">use_cache</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">kwargs_has_attention_mask</span> <span class="ow">and</span> <span class="n">requires_attention_mask</span> <span class="ow">and</span> <span class="n">accepts_attention_mask</span><span class="p">:</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_attention_mask_for_generation</span><span class="p">(</span>
                <span class="n">inputs_tensor</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_pad_token_tensor</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">and</span> <span class="s2">&quot;encoder_outputs&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
            <span class="c1"># if model is encoder decoder encoder_outputs are created and added to `model_kwargs`</span>
            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_encoder_decoder_kwargs_for_generation</span><span class="p">(</span>
                <span class="n">inputs_tensor</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">,</span> <span class="n">model_input_name</span><span class="p">,</span> <span class="n">generation_config</span>
            <span class="p">)</span>
        <span class="c1"># 5. Prepare `input_ids` which will be used for auto-regressive generation</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_decoder_input_ids_for_generation</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">model_input_name</span><span class="o">=</span><span class="n">model_input_name</span><span class="p">,</span>
                <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
                <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">_decoder_start_token_tensor</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">inputs_tensor</span> <span class="k">if</span> <span class="n">model_input_name</span> <span class="o">==</span> <span class="s2">&quot;input_ids&quot;</span> <span class="k">else</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;input_ids&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">token_healing</span><span class="p">:</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">heal_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">streamer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">streamer</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="c1"># 6. Prepare `max_length` depending on other stopping criteria.</span>
        <span class="n">input_ids_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">has_default_max_length</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;max_length&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">has_default_min_length</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;min_length&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">min_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">generation_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_generated_length</span><span class="p">(</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">has_default_max_length</span><span class="o">=</span><span class="n">has_default_max_length</span><span class="p">,</span>
            <span class="n">has_default_min_length</span><span class="o">=</span><span class="n">has_default_min_length</span><span class="p">,</span>
            <span class="n">model_input_name</span><span class="o">=</span><span class="n">model_input_name</span><span class="p">,</span>
            <span class="n">inputs_tensor</span><span class="o">=</span><span class="n">inputs_tensor</span><span class="p">,</span>
            <span class="n">input_ids_length</span><span class="o">=</span><span class="n">input_ids_length</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">use_dynamic_cache_by_default</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="s2">&quot;mamba&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="n">cache_name</span> <span class="o">=</span> <span class="s2">&quot;cache_params&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cache_name</span> <span class="o">=</span> <span class="s2">&quot;past_key_values&quot;</span>

        <span class="c1"># TODO(joao): support static caches in assisted generation. assisted generation needs to roll back caches,</span>
        <span class="c1"># which is only supported in dynamic caches atm</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">assistant_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supports_default_dynamic_cache</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
                <span class="s2">&quot;An assistant model is provided, using a dynamic cache instead of a cache of type=&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span><span class="si">}</span><span class="s2">&#39;.&quot;</span>
            <span class="p">)</span>
            <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># if (model_kwargs.get(cache_name) is not None) and is_torchdynamo_compiling():</span>
        <span class="c1">#     raise ValueError(</span>
        <span class="c1">#         &quot;Passing `past_key_values` is not supported when compiling `model.generate` with torch.compile -- you &quot;</span>
        <span class="c1">#         &quot;may get incorrect outputs. Please compile `model.forward` only or use the `cache_implementation` &quot;</span>
        <span class="c1">#         &quot;input argument.&quot;</span>
        <span class="c1">#     )</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">cache_name</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Passing both `cache_implementation` (used to initialize certain caches) and `</span><span class="si">{</span><span class="n">cache_name</span><span class="si">}</span><span class="s2">` (a &quot;</span>
                <span class="s2">&quot;Cache object) is unsupported. Please use only one of the two.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="ow">in</span> <span class="n">NEED_SETUP_CACHE_CLASSES_MAPPING</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="o">==</span> <span class="s2">&quot;static&quot;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supports_static_cache</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;This model does not support `cache_implementation=&#39;static&#39;`. Please check the following &quot;</span>
                        <span class="s2">&quot;issue: https://github.com/huggingface/transformers/issues/28981&quot;</span>
                    <span class="p">)</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="n">cache_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_cache</span><span class="p">(</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span><span class="p">,</span>
                    <span class="nb">getattr</span><span class="p">(</span><span class="n">generation_config</span><span class="p">,</span> <span class="s2">&quot;num_beams&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span>
                    <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
                    <span class="n">model_kwargs</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="o">==</span> <span class="s2">&quot;quantized&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;MindNLP do not support quantized generation.&#39;</span><span class="p">)</span>
                <span class="c1"># if not self._supports_quantized_cache:</span>
                <span class="c1">#     raise ValueError(</span>
                <span class="c1">#         &quot;This model does not support the quantized cache. If you want your model to support quantized &quot;</span>
                <span class="c1">#         &quot;cache, please open an issue.&quot;</span>
                <span class="c1">#     )</span>

                <span class="c1"># cache_config = (</span>
                <span class="c1">#     generation_config.cache_config</span>
                <span class="c1">#     if generation_config.cache_config is not None</span>
                <span class="c1">#     else QuantizedCacheConfig()</span>
                <span class="c1"># )</span>
                <span class="c1"># cache_class = QUANT_BACKEND_CLASSES_MAPPING[cache_config.backend]</span>

                <span class="c1"># if cache_config.backend == &quot;quanto&quot; and not is_quanto_available():</span>
                <span class="c1">#     raise ImportError(</span>
                <span class="c1">#         &quot;You need to install `quanto` in order to use KV cache quantization with quanto backend. &quot;</span>
                <span class="c1">#         &quot;Please install it via  with `pip install quanto`&quot;</span>
                <span class="c1">#     )</span>
                <span class="c1"># elif cache_config.backend == &quot;HQQ&quot; and not is_hqq_available():</span>
                <span class="c1">#     raise ImportError(</span>
                <span class="c1">#         &quot;You need to install `HQQ` in order to use KV cache quantization with HQQ backend. &quot;</span>
                <span class="c1">#         &quot;Please install it via  with `pip install hqq`&quot;</span>
                <span class="c1">#     )</span>

                <span class="c1"># model_kwargs[cache_name] = cache_class(cache_config)</span>
        <span class="c1"># Use DynamicCache() instance by default. This will avoid back and forth from legacy format that</span>
        <span class="c1"># keeps copying the cache thus using much more memory</span>
        <span class="k">elif</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supports_default_dynamic_cache</span><span class="p">():</span>
            <span class="n">past</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">cache_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">requires_cross_attention_cache</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">or</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">past</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="n">cache_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">DynamicCache</span><span class="p">()</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">requires_cross_attention_cache</span>
                    <span class="k">else</span> <span class="n">EncoderDecoderCache</span><span class="p">(</span><span class="n">DynamicCache</span><span class="p">(),</span> <span class="n">DynamicCache</span><span class="p">())</span>
                <span class="p">)</span>
                <span class="n">use_dynamic_cache_by_default</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">past</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="n">cache_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">DynamicCache</span><span class="o">.</span><span class="n">from_legacy_cache</span><span class="p">(</span><span class="n">past</span><span class="p">)</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">requires_cross_attention_cache</span>
                    <span class="k">else</span> <span class="n">EncoderDecoderCache</span><span class="o">.</span><span class="n">from_legacy_cache</span><span class="p">(</span><span class="n">past</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">use_dynamic_cache_by_default</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_generated_length</span><span class="p">(</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">input_ids_length</span><span class="p">,</span> <span class="n">has_default_max_length</span><span class="p">)</span>

        <span class="c1"># 7. determine generation mode</span>
        <span class="n">generation_mode</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">get_generation_mode</span><span class="p">(</span><span class="n">assistant_model</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">streamer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># 8. prepare distribution pre_processing samplers</span>
        <span class="n">prepared_logits_processor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_logits_processor</span><span class="p">(</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">input_ids_seq_length</span><span class="o">=</span><span class="n">input_ids_length</span><span class="p">,</span>
            <span class="n">encoder_input_ids</span><span class="o">=</span><span class="n">inputs_tensor</span><span class="p">,</span>
            <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="n">prefix_allowed_tokens_fn</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span>
            <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="n">negative_prompt_ids</span><span class="o">=</span><span class="n">negative_prompt_ids</span><span class="p">,</span>
            <span class="n">negative_prompt_attention_mask</span><span class="o">=</span><span class="n">negative_prompt_attention_mask</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># 9. prepare stopping criteria</span>
        <span class="n">prepared_stopping_criteria</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_stopping_criteria</span><span class="p">(</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">stopping_criteria</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">_run_profiler</span><span class="p">:</span>
            <span class="n">_framework_profiler_step_start</span><span class="p">()</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;Enabling the profiler will generate larger files. Please set `max_length` or `max_new_tokens` to a smaller value (recommended less than 10)&#39;</span><span class="p">)</span>

        <span class="c1"># 10. go into different generation modes</span>
        <span class="k">if</span> <span class="n">generation_mode</span> <span class="o">==</span> <span class="n">GenerationMode</span><span class="o">.</span><span class="n">ASSISTED_GENERATION</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;num_return_sequences has to be 1 when doing assisted generate, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;but is </span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_return_sequences</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;assisted generate is only supported for batch_size = 1&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;use_cache&quot;</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;assisted generate requires `use_cache=True`&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="o">==</span> <span class="s2">&quot;static&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;assisted generate is not supported with `static_cache`&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_stateful</span><span class="p">:</span>
                <span class="c1"># In assisted generation we need the ability to confirm whether the model would pick certain tokens,</span>
                <span class="c1"># which is not possible with stateful models (they can&#39;t reset to a previous subset of generated text)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;assisted generation is not supported with stateful models, such as </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

            <span class="c1"># 11. Get the candidate generator, given the parameterization</span>
            <span class="n">candidate_generator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_candidate_generator</span><span class="p">(</span>
                <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">inputs_tensor</span><span class="o">=</span><span class="n">inputs_tensor</span><span class="p">,</span>
                <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span>
                <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span>
                <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># 12. prepare logits warper (if `do_sample` is `True`)</span>
            <span class="n">prepared_logits_warper</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_get_logits_warper</span><span class="p">(</span>
                    <span class="n">generation_config</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">do_sample</span>
                <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

            <span class="c1"># 13. run assisted generate</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_assisted_decoding</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">candidate_generator</span><span class="o">=</span><span class="n">candidate_generator</span><span class="p">,</span>
                <span class="n">logits_processor</span><span class="o">=</span><span class="n">prepared_logits_processor</span><span class="p">,</span>
                <span class="n">logits_warper</span><span class="o">=</span><span class="n">prepared_logits_warper</span><span class="p">,</span>
                <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">prepared_stopping_criteria</span><span class="p">,</span>
                <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
                <span class="n">synced_gpus</span><span class="o">=</span><span class="n">synced_gpus</span><span class="p">,</span>
                <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">generation_mode</span> <span class="o">==</span> <span class="n">GenerationMode</span><span class="o">.</span><span class="n">DOLA_GENERATION</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_stateful</span><span class="p">:</span>
                <span class="c1"># DoLa decoding was not designed for stateful models, and would require some changes</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;dola decoding is not supported with stateful models, such as </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="n">prepared_logits_warper</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_get_logits_warper</span><span class="p">(</span><span class="n">generation_config</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">do_sample</span>
                <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dola_decoding</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">dola_layers</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">dola_layers</span><span class="p">,</span>
                <span class="n">logits_processor</span><span class="o">=</span><span class="n">prepared_logits_processor</span><span class="p">,</span>
                <span class="n">logits_warper</span><span class="o">=</span><span class="n">prepared_logits_warper</span><span class="p">,</span>
                <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">prepared_stopping_criteria</span><span class="p">,</span>
                <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
                <span class="n">synced_gpus</span><span class="o">=</span><span class="n">synced_gpus</span><span class="p">,</span>
                <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">generation_mode</span> <span class="o">==</span> <span class="n">GenerationMode</span><span class="o">.</span><span class="n">CONTRASTIVE_SEARCH</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;use_cache&quot;</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Contrastive search requires `use_cache=True`&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_stateful</span><span class="p">:</span>
                <span class="c1"># Just like assisted generation, we need to be able to rollback to a previous state (see comment above)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;contrastive search is not supported with stateful models, such as </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_contrastive_search</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">logits_processor</span><span class="o">=</span><span class="n">prepared_logits_processor</span><span class="p">,</span>
                <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">prepared_stopping_criteria</span><span class="p">,</span>
                <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
                <span class="n">synced_gpus</span><span class="o">=</span><span class="n">synced_gpus</span><span class="p">,</span>
                <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">generation_mode</span> <span class="ow">in</span> <span class="p">(</span><span class="n">GenerationMode</span><span class="o">.</span><span class="n">SAMPLE</span><span class="p">,</span> <span class="n">GenerationMode</span><span class="o">.</span><span class="n">GREEDY_SEARCH</span><span class="p">):</span>
            <span class="c1"># 11. prepare logits warper</span>
            <span class="n">prepared_logits_warper</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_get_logits_warper</span><span class="p">(</span><span class="n">generation_config</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">do_sample</span>
                <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

            <span class="c1"># 12. expand input_ids with `num_return_sequences` additional sequences per batch</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_inputs_for_generation</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">expand_size</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_return_sequences</span><span class="p">,</span>
                <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">logits_processor</span><span class="o">=</span><span class="n">prepared_logits_processor</span><span class="p">,</span>
                <span class="n">logits_warper</span><span class="o">=</span><span class="n">prepared_logits_warper</span><span class="p">,</span>
                <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">prepared_stopping_criteria</span><span class="p">,</span>
                <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
                <span class="n">synced_gpus</span><span class="o">=</span><span class="n">synced_gpus</span><span class="p">,</span>
                <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">generation_mode</span> <span class="ow">in</span> <span class="p">(</span><span class="n">GenerationMode</span><span class="o">.</span><span class="n">BEAM_SAMPLE</span><span class="p">,</span> <span class="n">GenerationMode</span><span class="o">.</span><span class="n">BEAM_SEARCH</span><span class="p">):</span>
            <span class="c1"># 11. prepare logits warper</span>
            <span class="n">prepared_logits_warper</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_get_logits_warper</span><span class="p">(</span><span class="n">generation_config</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">do_sample</span>
                <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

            <span class="c1"># 12. prepare beam search scorer</span>
            <span class="n">beam_scorer</span> <span class="o">=</span> <span class="n">BeamSearchScorer</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">num_beams</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
                <span class="n">length_penalty</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">length_penalty</span><span class="p">,</span>
                <span class="n">do_early_stopping</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">,</span>
                <span class="n">num_beam_hyps_to_keep</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_return_sequences</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># 13. interleave input_ids with `num_beams` additional sequences per batch</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_inputs_for_generation</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">expand_size</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
                <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># 14. run beam sample</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beam_search</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">beam_scorer</span><span class="p">,</span>
                <span class="n">logits_processor</span><span class="o">=</span><span class="n">prepared_logits_processor</span><span class="p">,</span>
                <span class="n">logits_warper</span><span class="o">=</span><span class="n">prepared_logits_warper</span><span class="p">,</span>
                <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">prepared_stopping_criteria</span><span class="p">,</span>
                <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
                <span class="n">synced_gpus</span><span class="o">=</span><span class="n">synced_gpus</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">generation_mode</span> <span class="o">==</span> <span class="n">GenerationMode</span><span class="o">.</span><span class="n">GROUP_BEAM_SEARCH</span><span class="p">:</span>
            <span class="c1"># 11. prepare beam search scorer</span>
            <span class="n">beam_scorer</span> <span class="o">=</span> <span class="n">BeamSearchScorer</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">num_beams</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
                <span class="n">length_penalty</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">length_penalty</span><span class="p">,</span>
                <span class="n">do_early_stopping</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">,</span>
                <span class="n">num_beam_hyps_to_keep</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_return_sequences</span><span class="p">,</span>
                <span class="n">num_beam_groups</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beam_groups</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># 12. interleave input_ids with `num_beams` additional sequences per batch</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_inputs_for_generation</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">expand_size</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
                <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># 13. run beam search</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_group_beam_search</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">beam_scorer</span><span class="p">,</span>
                <span class="n">logits_processor</span><span class="o">=</span><span class="n">prepared_logits_processor</span><span class="p">,</span>
                <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">prepared_stopping_criteria</span><span class="p">,</span>
                <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
                <span class="n">synced_gpus</span><span class="o">=</span><span class="n">synced_gpus</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">generation_mode</span> <span class="o">==</span> <span class="n">GenerationMode</span><span class="o">.</span><span class="n">CONSTRAINED_BEAM_SEARCH</span><span class="p">:</span>
            <span class="n">final_constraints</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">constraints</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">final_constraints</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">constraints</span>

            <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">force_words_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

                <span class="k">def</span> <span class="nf">typeerror</span><span class="p">():</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;`force_words_ids` has to either be a `List[List[List[int]]]` or `List[List[int]]` &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;of positive integers, but is </span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">force_words_ids</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>

                <span class="k">if</span> <span class="p">(</span>
                    <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">force_words_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
                    <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">force_words_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="p">):</span>
                    <span class="n">typeerror</span><span class="p">()</span>

                <span class="k">for</span> <span class="n">word_ids</span> <span class="ow">in</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">force_words_ids</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">word_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">word_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                            <span class="n">typeerror</span><span class="p">()</span>
                        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">for</span> <span class="n">token_ids</span> <span class="ow">in</span> <span class="n">word_ids</span><span class="p">):</span>
                            <span class="n">typeerror</span><span class="p">()</span>
                        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
                            <span class="nb">any</span><span class="p">((</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">token_id</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">token_ids</span><span class="p">)</span>
                            <span class="k">for</span> <span class="n">token_ids</span> <span class="ow">in</span> <span class="n">word_ids</span>
                        <span class="p">):</span>
                            <span class="n">typeerror</span><span class="p">()</span>

                        <span class="n">constraint</span> <span class="o">=</span> <span class="n">DisjunctiveConstraint</span><span class="p">(</span><span class="n">word_ids</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">word_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                            <span class="n">typeerror</span><span class="p">()</span>
                        <span class="k">if</span> <span class="nb">any</span><span class="p">((</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">token_id</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">word_ids</span><span class="p">):</span>
                            <span class="n">typeerror</span><span class="p">()</span>

                        <span class="n">constraint</span> <span class="o">=</span> <span class="n">PhrasalConstraint</span><span class="p">(</span><span class="n">word_ids</span><span class="p">)</span>
                    <span class="n">final_constraints</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">constraint</span><span class="p">)</span>

            <span class="c1"># 11. prepare beam search scorer</span>
            <span class="n">constrained_beam_scorer</span> <span class="o">=</span> <span class="n">ConstrainedBeamSearchScorer</span><span class="p">(</span>
                <span class="n">constraints</span><span class="o">=</span><span class="n">final_constraints</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">num_beams</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
                <span class="n">length_penalty</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">length_penalty</span><span class="p">,</span>
                <span class="n">do_early_stopping</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">,</span>
                <span class="n">num_beam_hyps_to_keep</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_return_sequences</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># 12. interleave input_ids with `num_beams` additional sequences per batch</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_inputs_for_generation</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">expand_size</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
                <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># 13. run beam search</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_constrained_beam_search</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">constrained_beam_scorer</span><span class="o">=</span><span class="n">constrained_beam_scorer</span><span class="p">,</span>
                <span class="n">logits_processor</span><span class="o">=</span><span class="n">prepared_logits_processor</span><span class="p">,</span>
                <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">prepared_stopping_criteria</span><span class="p">,</span>
                <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
                <span class="n">synced_gpus</span><span class="o">=</span><span class="n">synced_gpus</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">_run_profiler</span><span class="p">:</span>
            <span class="n">_framework_profiler_step_end</span><span class="p">()</span>

        <span class="c1"># Convert to legacy cache if needed</span>
        <span class="k">if</span> <span class="n">use_dynamic_cache_by_default</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">return_legacy_cache</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">ModelOutput</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="s2">&quot;past_key_values&quot;</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span> <span class="p">(</span><span class="n">DynamicCache</span><span class="p">,</span> <span class="n">EncoderDecoderCache</span><span class="p">)):</span>
                    <span class="n">result</span><span class="o">.</span><span class="n">past_key_values</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">past_key_values</span><span class="o">.</span><span class="n">to_legacy_cache</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">_has_unfinished_sequences</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">this_peer_finished</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">cur_len</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns whether there are still unfinished sequences in the device. The existence of unfinished sequences is</span>
<span class="sd">        fed through `this_peer_finished`. ZeRO stage 3-friendly.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># torch.compile does not support data-dependent control flow. This is a workaround to allow torch.compile,</span>
        <span class="c1"># although we lose the ability to stop when all sequences return an EOS token (and other stopping criteria)</span>
        <span class="c1"># TODO (joao): remove this when torch&#39;s support for control flow is not experimental (https://pytorch.org/docs/stable/generated/torch.cond.html)</span>
        <span class="c1"># if is_torchdynamo_compiling():</span>
        <span class="c1">#     return cur_len &lt; max_length</span>
        <span class="c1"># else:</span>
        <span class="c1"># if synced_gpus:</span>
        <span class="c1">#     # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.</span>
        <span class="c1">#     # The following logic allows an early break if all peers finished generating their sequence</span>
        <span class="c1">#     this_peer_finished_flag = mindspore.tensor(0.0 if this_peer_finished else 1.0)</span>
        <span class="c1">#     # send 0.0 if we finished, 1.0 otherwise</span>
        <span class="c1">#     dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)</span>
        <span class="c1">#     # did all peers finish? the reduced sum will be 0.0 then</span>
        <span class="c1">#     if this_peer_finished_flag.item() == 0.0:</span>
        <span class="c1">#         return False</span>
        <span class="k">if</span> <span class="n">this_peer_finished</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">heal_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;PreTrainedTokenizerBase&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences of token ids for models with a language modeling head.</span>
<span class="sd">        Parameters:</span>
<span class="sd">            input_ids (`mindspore.Tensor`): The sequence used as a prompt for the generation.</span>
<span class="sd">            tokenizer (`PreTrainedTokenizerBase`, *optional*): The tokenizer used to decode the input ids.</span>
<span class="sd">        Return:</span>
<span class="sd">            `mindspore.Tensor` where each sequence has its tail token replaced with its appropriate extension.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">tokenizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot; When generating with token healing, you must pass the model&#39;s tokenizer to the `tokenizer` &quot;</span>
                <span class="s2">&quot;argument of `generate`.&quot;</span>
            <span class="p">)</span>

        <span class="n">bos_token_id</span><span class="p">,</span> <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="n">vocab_trie</span> <span class="o">=</span> <span class="n">ExtensionsTrie</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">get_vocab</span><span class="p">())</span>
        <span class="n">generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">)</span>

        <span class="c1"># assumption: leading/trailing whitespace is not meaningful, so the prompts are</span>
        <span class="c1"># stripped before re-tokenizing to desensitize generation to whitespace artefacts</span>
        <span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
            <span class="n">prompts</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>

        <span class="c1"># replace bos with pad to not condition healing on it</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">input_ids</span> <span class="o">==</span> <span class="n">bos_token_id</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">)</span>

        <span class="n">tail_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">space_tok</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># tail tokens are used for a prefix search, thus, whitespaces are replaced with</span>
        <span class="c1"># their tokenization (e.g. &#39;Ġ&#39;) to enable search for tokens prefixed with a whitespace</span>
        <span class="n">tail_toks</span> <span class="o">=</span> <span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">space_tok</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tail_ids</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">tail_id</span><span class="p">,</span> <span class="n">tail_tok</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">tail_ids</span><span class="p">,</span> <span class="n">tail_toks</span><span class="p">)):</span>
            <span class="n">batch_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">batch_ids</span> <span class="o">==</span> <span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">():</span>
                <span class="k">continue</span>  <span class="c1"># skip empty sequences (all pad ids)</span>

            <span class="c1"># apply bias for alternatives (extensions) to the tail token</span>
            <span class="n">seq_bias</span> <span class="o">=</span> <span class="p">{(</span><span class="n">alt_tok</span><span class="p">,):</span> <span class="mf">10.0</span> <span class="k">for</span> <span class="n">alt_tok</span> <span class="ow">in</span> <span class="n">vocab_trie</span><span class="o">.</span><span class="n">values</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="n">tail_tok</span><span class="p">)}</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq_bias</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">continue</span>  <span class="c1"># skip if there are no token alternatives to heal with</span>

            <span class="c1"># slightly favor original token to limit aggressive healing e.g. &#39;http&#39; -&gt; &#39;https&#39;</span>
            <span class="n">seq_bias</span><span class="p">[(</span><span class="n">tail_id</span><span class="p">,)]</span> <span class="o">+=</span> <span class="mf">1.0</span>
            <span class="n">generation_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">sequence_bias</span><span class="o">=</span><span class="n">seq_bias</span><span class="p">)</span>

            <span class="n">trimmed_ids</span> <span class="o">=</span> <span class="n">batch_ids</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="c1"># if the prompt is a single (non-pad) token, regenerate from bos</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_ids</span><span class="p">[</span><span class="n">batch_ids</span> <span class="o">!=</span> <span class="n">pad_token_id</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">trimmed_ids</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">bos_token_id</span>

            <span class="n">input_ids</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">trimmed_ids</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">input_ids</span>

    <span class="k">def</span> <span class="nf">contrastive_search</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
            <span class="s2">&quot;Calling `contrastive_search` directly is deprecated.41. Use `generate` or a &quot;</span>
            <span class="s2">&quot;custom generation loop instead.&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_contrastive_search</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_dola_decoding</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">dola_layers</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">LogitsProcessorList</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="p">:</span> <span class="n">StoppingCriteriaList</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">streamer</span><span class="p">:</span> <span class="s2">&quot;BaseStreamer&quot;</span><span class="p">,</span>
        <span class="n">logits_warper</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">],</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">GenerateNonBeamOutput</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences of token ids for models with a language modeling head using **dola decoding** and can be</span>
<span class="sd">        used for decoder-only text models.</span>
<span class="sd">        The method is based on the paper &quot;DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language</span>
<span class="sd">        Models&quot; (https://arxiv.org/abs/2309.03883) in ICLR 2024.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                The sequence used as a prompt for the generation.</span>
<span class="sd">            dola_layers (`Union[str, List[int]]`):</span>
<span class="sd">                The candidate layers used in contrasting layers of DoLa. It can be either 1) &#39;low&#39; or &#39;high&#39;, which</span>
<span class="sd">                means the lower part or higher part of the model layers, respectively, or 2) a list of layer indices</span>
<span class="sd">                to be used for candidate layers. The 0-th layer is the word embedding layer of the model.</span>
<span class="sd">            logits_processor (`LogitsProcessorList`):</span>
<span class="sd">                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]</span>
<span class="sd">                used to modify the prediction scores of the language modeling head applied at each generation step.</span>
<span class="sd">            stopping_criteria (`StoppingCriteriaList`, *optional*):</span>
<span class="sd">                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]</span>
<span class="sd">                used to tell if the generation loop should stop.</span>
<span class="sd">            generation_config ([`~generation.GenerationConfig`]):</span>
<span class="sd">                The generation configuration to be used as parametrization of the decoding method.</span>
<span class="sd">            synced_gpus (`bool`):</span>
<span class="sd">                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</span>
<span class="sd">            streamer (`BaseStreamer`, *optional*):</span>
<span class="sd">                Streamer object that will be used to stream the generated sequences. Generated tokens are passed</span>
<span class="sd">                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.</span>
<span class="sd">            logits_warper (`LogitsProcessorList`, *optional*):</span>
<span class="sd">                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used</span>
<span class="sd">                to warp the prediction score distribution of the language modeling head applied before multinomial</span>
<span class="sd">                sampling at each generation step.</span>
<span class="sd">            model_kwargs:</span>
<span class="sd">                Additional model specific keyword arguments will be forwarded to the `forward` function of the model.</span>
<span class="sd">                If model is an encoder-decoder model the kwargs should include `encoder_outputs`.</span>

<span class="sd">        Return:</span>
<span class="sd">            [`~generation.GenerateDecoderOnlyOutput`], [`~generation.GenerateEncoderDecoderOutput`]</span>
<span class="sd">            or `mindspore.Tensor`: A `mindspore.Tensor` containing the generated tokens (default behaviour) or a</span>
<span class="sd">            [`~generation.GenerateDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and</span>
<span class="sd">            `return_dict_in_generate=True` or a [`~generation.GenerateEncoderDecoderOutput`] if</span>
<span class="sd">            `model.config.is_encoder_decoder=True`.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;DoLa decoding is only available for decoder-only models.&quot;</span><span class="p">)</span>
        <span class="c1"># init values</span>

        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_pad_token_tensor</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="n">output_scores</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_scores</span>
        <span class="n">output_logits</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_logits</span>
        <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">return_dict_in_generate</span>
        <span class="n">has_eos_stopping_criteria</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">criteria</span><span class="p">,</span> <span class="s2">&quot;eos_token_id&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">criteria</span> <span class="ow">in</span> <span class="n">stopping_criteria</span><span class="p">)</span>
        <span class="n">do_sample</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">do_sample</span>
        <span class="k">if</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">logits_warper</span><span class="p">,</span> <span class="n">LogitsProcessorList</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`do_sample` is set to `True`, `logits_warper` must be a `LogitsProcessorList` instance (it is &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">logits_warper</span><span class="si">}</span><span class="s2">).&quot;</span>
            <span class="p">)</span>

        <span class="c1"># init attention / hidden states / scores tuples</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">raw_logits</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_logits</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">cross_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_hidden_states</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># keep track of which sequences are already finished</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">unfinished_sequences</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_initial_cache_position</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>

        <span class="n">this_peer_finished</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># prepare layers for DoLa decoding</span>
        <span class="n">final_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span>
        <span class="c1"># if the model has tied word embeddings, we skip the word embeddings (0-th) layer and start from the 2nd layer,</span>
        <span class="c1"># as the early exit from word embeddings will become identity function</span>
        <span class="c1"># if the model is really shallow (&lt;=2 layers), we use the 1st layer if it&#39;s not the final layer and the 0-th</span>
        <span class="c1"># layer otherwise. Notice that DoLa does not help shallow models much.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">tie_word_embeddings</span><span class="p">:</span>
            <span class="n">start_layer</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="n">final_layer</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">start_layer</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="k">elif</span> <span class="n">final_layer</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">start_layer</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">start_layer</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># For `N`-layer models with `N &lt;= 40` layers, the layers of `range(0, N // 2, 2)` and `range(N // 2, N, 2)`</span>
        <span class="c1"># are used for `&#39;low&#39;` and `&#39;high&#39;` layers, respectively.</span>
        <span class="c1"># For models with `N &gt; 40` layers, the layers of `range(0, 20, 2)` and `range(N - 20, N, 2)` are used for</span>
        <span class="c1"># `&#39;low&#39;` and `&#39;high&#39;` layers, respectively.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dola_layers</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">dola_layers</span> <span class="o">==</span> <span class="s2">&quot;low&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">start_layer</span> <span class="o">==</span> <span class="n">final_layer</span> <span class="o">//</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">candidate_premature_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">start_layer</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">candidate_premature_layers</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">start_layer</span><span class="p">,</span> <span class="n">final_layer</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
                    <span class="k">if</span> <span class="n">final_layer</span> <span class="o">&lt;=</span> <span class="mi">40</span>
                    <span class="k">else</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">start_layer</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
                <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dola_layers</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">dola_layers</span> <span class="o">==</span> <span class="s2">&quot;high&quot;</span><span class="p">:</span>
            <span class="n">candidate_premature_layers</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">final_layer</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">final_layer</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">final_layer</span> <span class="o">&lt;=</span> <span class="mi">40</span>
                <span class="k">else</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">final_layer</span> <span class="o">-</span> <span class="mi">20</span><span class="p">,</span> <span class="n">final_layer</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
            <span class="p">)</span>
        <span class="c1"># Set the `dola_layers` to a list of integers for layer indices to contrast manually specified layers.</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dola_layers</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">candidate_premature_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dola_layers</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">final_layer</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dola_layers must be either &#39;low&#39;, &#39;high&#39; or a list of integers.&quot;</span><span class="p">)</span>

        <span class="n">lm_head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">lm_head</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;DoLa is not supported for models that don&#39;t have output embeddings.&quot;</span><span class="p">)</span>

        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_unfinished_sequences</span><span class="p">(</span><span class="n">this_peer_finished</span><span class="p">,</span> <span class="n">synced_gpus</span><span class="p">):</span>
            <span class="c1"># prepare model inputs</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

            <span class="c1"># forward pass to get next token</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
                <span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">final_layer_next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">final_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">candidate_premature_logits</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">candidate_premature_layer</span> <span class="ow">in</span> <span class="n">candidate_premature_layers</span><span class="p">:</span>
                <span class="n">candidate_premature_logits</span><span class="p">[</span><span class="n">candidate_premature_layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">lm_head</span><span class="p">(</span>
                    <span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">[</span><span class="n">candidate_premature_layer</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">synced_gpus</span> <span class="ow">and</span> <span class="n">this_peer_finished</span><span class="p">:</span>
                <span class="k">continue</span>  <span class="c1"># don&#39;t waste resources running the code we don&#39;t need</span>

            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">_dola_select_contrast</span><span class="p">(</span>
                <span class="n">candidate_premature_layers</span><span class="p">,</span> <span class="n">candidate_premature_logits</span><span class="p">,</span> <span class="n">final_logits</span>
            <span class="p">)</span>
            <span class="c1"># pre-process distribution</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">logits_processor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_logits</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>  <span class="c1"># sample</span>
                <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">logits_warper</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_scores</span><span class="p">)</span>
            <span class="c1"># Store scores, attentions and hidden_states when required</span>
            <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="n">scores</span> <span class="o">+=</span> <span class="p">(</span><span class="n">next_token_scores</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_logits</span><span class="p">:</span>
                    <span class="n">raw_logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">final_layer_next_token_logits</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                    <span class="n">decoder_attentions</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,)</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                        <span class="n">cross_attentions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,)</span>

                <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                    <span class="n">decoder_hidden_states</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,)</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
                        <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,)</span>
                    <span class="p">)</span>

            <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>  <span class="c1"># sample</span>
                <span class="n">probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># argmax</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># finished sentences should have their next token be a padding token</span>
            <span class="k">if</span> <span class="n">has_eos_stopping_criteria</span><span class="p">:</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">next_tokens</span> <span class="o">*</span> <span class="n">unfinished_sequences</span> <span class="o">+</span> <span class="n">pad_token_id</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">unfinished_sequences</span><span class="p">)</span>

            <span class="c1"># update generated ids, model inputs, and length for next step</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">streamer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">streamer</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">)</span>
            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span>
                <span class="n">outputs</span><span class="p">,</span>
                <span class="n">model_kwargs</span><span class="p">,</span>
                <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># stop when each sentence is finished</span>
            <span class="n">unfinished_sequences</span> <span class="o">=</span> <span class="n">unfinished_sequences</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">stopping_criteria</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
            <span class="n">this_peer_finished</span> <span class="o">=</span> <span class="n">unfinished_sequences</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">streamer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">streamer</span><span class="o">.</span><span class="n">end</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">GenerateDecoderOnlyOutput</span><span class="p">(</span>
                <span class="n">sequences</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                <span class="n">logits</span><span class="o">=</span><span class="n">raw_logits</span><span class="p">,</span>
                <span class="n">attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">input_ids</span>

    <span class="nd">@no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_contrastive_search</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">LogitsProcessorList</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="p">:</span> <span class="n">StoppingCriteriaList</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">streamer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;BaseStreamer&quot;</span><span class="p">],</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">GenerateNonBeamOutput</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences of token ids for models with a language modeling head using **contrastive search** and can</span>
<span class="sd">        be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                The sequence used as a prompt for the generation.</span>
<span class="sd">            logits_processor (`LogitsProcessorList`):</span>
<span class="sd">                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]</span>
<span class="sd">                used to modify the prediction scores of the language modeling head applied at each generation step.</span>
<span class="sd">            stopping_criteria (`StoppingCriteriaList`):</span>
<span class="sd">                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]</span>
<span class="sd">                used to tell if the generation loop should stop.</span>
<span class="sd">            generation_config ([`~generation.GenerationConfig`]):</span>
<span class="sd">                The generation configuration to be used as parametrization of the decoding method.</span>
<span class="sd">            synced_gpus (`bool`):</span>
<span class="sd">                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</span>
<span class="sd">            streamer (`BaseStreamer`, *optional*):</span>
<span class="sd">                Streamer object that will be used to stream the generated sequences. Generated tokens are passed</span>
<span class="sd">                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.</span>
<span class="sd">            model_kwargs:</span>
<span class="sd">                Additional model specific keyword arguments will be forwarded to the `forward` function of the model.</span>
<span class="sd">                If model is an encoder-decoder model the kwargs should include `encoder_outputs`.</span>

<span class="sd">        Return:</span>
<span class="sd">            [`~generation.GenerateDecoderOnlyOutput`], [`~generation.GenerateEncoderDecoderOutput`]</span>
<span class="sd">            or `mindspore.Tensor`: A `mindspore.Tensor` containing the generated tokens (default behaviour) or a</span>
<span class="sd">            [`~generation.GenerateDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and</span>
<span class="sd">            `return_dict_in_generate=True` or a [`~generation.GenerateEncoderDecoderOutput`] if</span>
<span class="sd">            `model.config.is_encoder_decoder=True`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># init values</span>
        <span class="n">has_eos_stopping_criteria</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">criteria</span><span class="p">,</span> <span class="s2">&quot;eos_token_id&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">criteria</span> <span class="ow">in</span> <span class="n">stopping_criteria</span><span class="p">)</span>
        <span class="n">top_k</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">top_k</span>
        <span class="n">penalty_alpha</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">penalty_alpha</span>
        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_pad_token_tensor</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="n">output_scores</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_scores</span>
        <span class="n">output_logits</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_logits</span>
        <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">return_dict_in_generate</span>
        <span class="n">sequential</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">low_memory</span>

        <span class="c1"># init attention / hidden states / scores tuples</span>
        <span class="n">raw_logits</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_logits</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">cross_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_hidden_states</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># if model is an encoder-decoder, retrieve encoder attention weights and hidden states</span>
        <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">encoder_attentions</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attentions&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

        <span class="c1"># keep track of which sequences are already finished</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">unfinished_sequences</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_initial_cache_position</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>

        <span class="n">this_peer_finished</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_unfinished_sequences</span><span class="p">(</span><span class="n">this_peer_finished</span><span class="p">,</span> <span class="n">synced_gpus</span><span class="p">):</span>
            <span class="c1"># if the first step in the loop, encode all the prefix and obtain: (1) past_key_values;</span>
            <span class="c1"># (2) last_hidden_states; (3) logit_for_next_step; (4) update model kwargs for the next step</span>
            <span class="k">if</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">],</span> <span class="p">(</span><span class="n">Cache</span><span class="p">,</span> <span class="n">EncoderDecoderCache</span><span class="p">))</span>
                <span class="ow">and</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get_seq_length</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="p">):</span>
                <span class="c1"># prepare inputs</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;use_cache&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

                <span class="c1"># encode the given prefix and prepare model inputs; encoder-decoder model process the prefix and save</span>
                <span class="c1"># the `encoder_outputs`</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
                    <span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span>
                <span class="p">)</span>

                <span class="c1"># last decoder hidden states will be used to compute the degeneration penalty (cosine similarity with</span>
                <span class="c1"># previous tokens)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                    <span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

                <span class="c1"># next logit for contrastive search to select top-k candidate tokens</span>
                <span class="c1"># Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for this first iteration</span>
                <span class="c1"># (the clone itself is always small)</span>
                <span class="n">logit_for_next_step</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

                <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span>
                    <span class="n">outputs</span><span class="p">,</span>
                    <span class="n">model_kwargs</span><span class="p">,</span>
                    <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="n">sequential</span><span class="p">:</span>
                    <span class="c1"># Expands model inputs top_k times, for batched forward passes (akin to beam search).</span>
                    <span class="n">_</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_inputs_for_generation</span><span class="p">(</span>
                        <span class="n">expand_size</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span> <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span>
                    <span class="p">)</span>

                <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> does not support caching and therefore **can&#39;t** be used &quot;</span>
                        <span class="s2">&quot;for contrastive search.&quot;</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="p">(</span>
                    <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">))</span>
                    <span class="ow">or</span> <span class="n">past_key_values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">batch_size</span>
                <span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> does not have a standard cache format and therefore **can&#39;t** be &quot;</span>
                        <span class="s2">&quot;used for contrastive search without further modifications.&quot;</span>
                    <span class="p">)</span>

            <span class="c1"># contrastive_search main logic start:</span>
            <span class="c1"># contrastive search decoding consists of two steps: (1) candidate tokens recall; (2) candidate re-rank by</span>
            <span class="c1"># degeneration penalty</span>
            <span class="n">processed_logit_for_next_step</span> <span class="o">=</span> <span class="n">logits_processor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">logit_for_next_step</span><span class="p">)</span>
            <span class="n">next_probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">processed_logit_for_next_step</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">top_k_probs</span><span class="p">,</span> <span class="n">top_k_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">next_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span>

            <span class="c1"># Store scores, attentions and hidden_states when required</span>
            <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">output_logits</span><span class="p">:</span>
                    <span class="n">raw_logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">logit_for_next_step</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="n">scores</span> <span class="o">+=</span> <span class="p">(</span><span class="n">processed_logit_for_next_step</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                    <span class="n">decoder_attentions</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,)</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                        <span class="n">cross_attentions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,)</span>

                <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                    <span class="n">decoder_hidden_states</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,)</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
                        <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,)</span>
                    <span class="p">)</span>

            <span class="c1"># This is needed to properly delete outputs.logits which may be very large for this first iteration</span>
            <span class="c1"># Otherwise a reference to outputs.logits is kept all along until after the next call to self.forward()</span>
            <span class="k">del</span> <span class="n">outputs</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">sequential</span><span class="p">:</span>
                <span class="c1"># Replicates the new past_key_values to match the `top_k` candidates</span>
                <span class="n">past</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span>
                <span class="c1"># If it is a static cache, modify it in-place layer after layer to save memory</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">past</span><span class="p">,</span> <span class="n">DynamicCache</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="n">past</span><span class="p">,</span> <span class="n">EncoderDecoderCache</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">past</span><span class="o">.</span><span class="n">self_attention_cache</span><span class="p">,</span> <span class="n">DynamicCache</span><span class="p">)</span>
                <span class="p">):</span>
                    <span class="n">past</span><span class="o">.</span><span class="n">batch_repeat_interleave</span><span class="p">(</span><span class="n">top_k</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_key_values</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">past</span><span class="p">:</span>
                        <span class="n">items</span> <span class="o">=</span> <span class="p">[]</span>
                        <span class="c1"># item is either the key or the value matrix</span>
                        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">layer</span><span class="p">:</span>
                            <span class="n">items</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">top_k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
                        <span class="n">new_key_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">items</span><span class="p">))</span>

                    <span class="n">past</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_key_values</span><span class="p">)</span>

                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">past</span>

            <span class="k">if</span> <span class="n">sequential</span><span class="p">:</span>
                <span class="n">all_outputs</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">top_k</span><span class="p">):</span>
                    <span class="c1"># compute the candidate tokens by the language model and collect their hidden_states</span>
                    <span class="n">next_model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">top_k_ids</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

                    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
                        <span class="o">**</span><span class="n">next_model_inputs</span><span class="p">,</span>
                        <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">],</span> <span class="n">DynamicCache</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
                        <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">],</span> <span class="n">EncoderDecoderCache</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">self_attention_cache</span><span class="p">,</span> <span class="n">DynamicCache</span><span class="p">)</span>
                    <span class="p">):</span>
                        <span class="c1"># Remove past K-V from output since we don&#39;t need to stack later</span>
                        <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
                        <span class="c1"># Remove last token from past K-V since we don&#39;t want to append it at this point</span>
                        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">crop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

                    <span class="n">all_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">stack_model_outputs</span><span class="p">(</span><span class="n">all_outputs</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># compute the candidate tokens by the language model and collect their hidden_states</span>
                <span class="c1"># assembles top_k_ids into batch of size k</span>
                <span class="n">next_model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">top_k_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

                <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
                    <span class="o">**</span><span class="n">next_model_inputs</span><span class="p">,</span>
                    <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="c1"># This is essential to avoid having a last reference to the big past K-V and double the necesary memory</span>
            <span class="c1"># in the next loop</span>
            <span class="k">del</span> <span class="n">next_model_inputs</span>

            <span class="c1"># name is different for encoder-decoder and decoder-only models</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                <span class="n">next_hidden</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">full_hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">next_hidden</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">full_hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span>

            <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">context_hidden</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">last_hidden_states</span><span class="p">,</span> <span class="n">top_k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

            <span class="c1"># compute the degeneration penalty and re-rank the candidates based on the degeneration penalty and the</span>
            <span class="c1"># model confidence. Keeping `selected_idx` on CPU enables multi-device contrastive search and doesn&#39;t</span>
            <span class="c1"># introduce (noticeable) slowdowns on single-device runs.</span>
            <span class="n">selected_idx</span> <span class="o">=</span> <span class="n">_ranking_fast</span><span class="p">(</span><span class="n">context_hidden</span><span class="p">,</span> <span class="n">next_hidden</span><span class="p">,</span> <span class="n">top_k_probs</span><span class="p">,</span> <span class="n">penalty_alpha</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)</span>

            <span class="c1"># This will be used instead of the previous inneficient ops.stack(ops.split())</span>
            <span class="n">augmented_idx</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([(</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">top_k</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">selected_idx</span><span class="p">)])</span>

            <span class="c1"># prepare for the next step: (1) next token_id; (2) past_key_values; (3) last_hidden_states for computing</span>
            <span class="c1"># the degeneration penalty; (4) logits for selecting next top-k candidates; (5) selected tokens scores</span>
            <span class="c1"># (model confidence minus degeneration penalty); (6) decoder hidden_states</span>
            <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">getitem</span><span class="p">(</span><span class="n">top_k_ids</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">top_k_ids</span><span class="p">))),</span> <span class="n">selected_idx</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">top_k_ids</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">top_k_ids</span><span class="p">))),</span> <span class="n">selected_idx</span><span class="p">]</span>
            <span class="n">next_hidden</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">next_hidden</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">top_k</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
                <span class="n">next_hidden</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">getitem</span><span class="p">(</span><span class="n">next_hidden</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)),</span> <span class="n">selected_idx</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">next_hidden</span> <span class="o">=</span> <span class="n">next_hidden</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)),</span> <span class="n">selected_idx</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">last_hidden_states</span><span class="p">,</span> <span class="n">next_hidden</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">next_decoder_hidden_states</span> <span class="o">=</span> <span class="p">()</span>
            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">full_hidden_states</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
                    <span class="n">layer</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">getitem</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)),</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)),</span> <span class="n">selected_idx</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">layer</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">top_k</span><span class="p">))[</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)),</span> <span class="n">selected_idx</span><span class="p">,</span> <span class="p">:]</span>
                <span class="n">next_decoder_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer</span><span class="p">,)</span>

            <span class="c1"># generate past_key_values cache of only the selected token</span>
            <span class="k">if</span> <span class="n">sequential</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
                    <span class="n">next_model_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span>
                        <span class="n">ops</span><span class="o">.</span><span class="n">getitem</span><span class="p">(</span><span class="n">top_k_ids</span><span class="p">,</span> <span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">selected_idx</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="o">**</span><span class="n">model_kwargs</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">next_model_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span>
                        <span class="n">top_k_ids</span><span class="p">[:,</span> <span class="n">selected_idx</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="o">**</span><span class="n">model_kwargs</span>
                    <span class="p">)</span>
                <span class="n">selected_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
                    <span class="o">**</span><span class="n">next_model_input</span><span class="p">,</span>
                    <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">next_past_key_values</span> <span class="o">=</span> <span class="n">selected_outputs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">next_past_key_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_past_from_model_output</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
                <span class="c1"># Do it in-place layer per layer to save memory</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">next_past_key_values</span><span class="p">,</span> <span class="n">DynamicCache</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="n">next_past_key_values</span><span class="p">,</span> <span class="n">EncoderDecoderCache</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">next_past_key_values</span><span class="o">.</span><span class="n">self_attention_cache</span><span class="p">,</span> <span class="n">DynamicCache</span><span class="p">)</span>
                <span class="p">):</span>
                    <span class="n">next_past_key_values</span><span class="o">.</span><span class="n">batch_select_indices</span><span class="p">(</span><span class="n">augmented_idx</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_key_values</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">next_past_key_values</span><span class="p">:</span>
                        <span class="n">items</span> <span class="o">=</span> <span class="p">[]</span>
                        <span class="c1"># item is either the key or the value matrix</span>
                        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">layer</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
                                <span class="n">items</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">getitem</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="p">(</span><span class="n">augmented_idx</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))))</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="n">items</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="n">augmented_idx</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span>
                        <span class="n">new_key_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">items</span><span class="p">))</span>

                    <span class="n">next_past_key_values</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_key_values</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
                <span class="n">logit_for_next_step</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">getitem</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)),</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)),</span> <span class="n">selected_idx</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logit_for_next_step</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_k</span><span class="p">))[</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)),</span> <span class="n">selected_idx</span><span class="p">,</span> <span class="p">:]</span>

            <span class="c1"># Rebuilds the relevant parts of the model output for the selected token, for use in the next iteration</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                <span class="n">next_step_cross_attentions</span> <span class="o">=</span> <span class="p">()</span>
                <span class="n">next_step_decoder_attentions</span> <span class="o">=</span> <span class="p">()</span>
                <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
                            <span class="n">layer</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">getitem</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">top_k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)),</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)),</span> <span class="n">selected_idx</span><span class="p">,</span> <span class="bp">Ellipsis</span><span class="p">))</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">layer</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">top_k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))[</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)),</span> <span class="n">selected_idx</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
                        <span class="n">next_step_cross_attentions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer</span><span class="p">,)</span>
                    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
                            <span class="n">layer</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">getitem</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">top_k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)),</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)),</span> <span class="n">selected_idx</span><span class="p">,</span> <span class="bp">Ellipsis</span><span class="p">))</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">layer</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">top_k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))[</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)),</span> <span class="n">selected_idx</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
                        <span class="n">next_step_decoder_attentions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer</span><span class="p">,)</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">Seq2SeqLMOutput</span><span class="p">(</span>
                    <span class="n">past_key_values</span><span class="o">=</span><span class="n">next_past_key_values</span><span class="p">,</span>
                    <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">next_decoder_hidden_states</span><span class="p">,</span>
                    <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">next_step_decoder_attentions</span> <span class="ow">or</span> <span class="kc">None</span><span class="p">,</span>
                    <span class="n">cross_attentions</span><span class="o">=</span><span class="n">next_step_cross_attentions</span> <span class="ow">or</span> <span class="kc">None</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">next_step_attentions</span> <span class="o">=</span> <span class="p">()</span>
                <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
                            <span class="n">layer</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">getitem</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">top_k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)),</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)),</span> <span class="n">selected_idx</span><span class="p">,</span> <span class="bp">Ellipsis</span><span class="p">))</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">layer</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">top_k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))[</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)),</span> <span class="n">selected_idx</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
                        <span class="n">next_step_attentions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer</span><span class="p">,)</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">CausalLMOutputWithPast</span><span class="p">(</span>
                    <span class="n">past_key_values</span><span class="o">=</span><span class="n">next_past_key_values</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">next_decoder_hidden_states</span><span class="p">,</span>
                    <span class="n">attentions</span><span class="o">=</span><span class="n">next_step_attentions</span> <span class="ow">or</span> <span class="kc">None</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="c1"># contrastive_search main logic end</span>

            <span class="k">if</span> <span class="n">synced_gpus</span> <span class="ow">and</span> <span class="n">this_peer_finished</span><span class="p">:</span>
                <span class="k">continue</span>  <span class="c1"># don&#39;t waste resources running the code we don&#39;t need</span>

            <span class="c1"># finished sentences should have their next token be a padding token</span>
            <span class="k">if</span> <span class="n">has_eos_stopping_criteria</span><span class="p">:</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">next_tokens</span> <span class="o">*</span> <span class="n">unfinished_sequences</span> <span class="o">+</span> <span class="n">pad_token_id</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">unfinished_sequences</span><span class="p">)</span>

            <span class="c1"># update generated ids, model inputs, and length for next step</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">long</span><span class="p">()],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">streamer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">streamer</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">)</span>
            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span>
                <span class="n">outputs</span><span class="p">,</span>
                <span class="n">model_kwargs</span><span class="p">,</span>
                <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># stop when each sentence is finished</span>
            <span class="n">unfinished_sequences</span> <span class="o">=</span> <span class="n">unfinished_sequences</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">stopping_criteria</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
            <span class="n">this_peer_finished</span> <span class="o">=</span> <span class="n">unfinished_sequences</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">streamer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">streamer</span><span class="o">.</span><span class="n">end</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
            <span class="c1"># Contrastive search works by forward looking at the next token, so we need to exclude it from</span>
            <span class="c1"># `past_key_values` to be consistent with the other decoding methods</span>
            <span class="k">if</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">],</span> <span class="n">DynamicCache</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">],</span> <span class="n">EncoderDecoderCache</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">self_attention_cache</span><span class="p">,</span> <span class="n">DynamicCache</span><span class="p">)</span>
                <span class="p">):</span>
                    <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">crop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">past_key_values</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]:</span>
                        <span class="n">layer_past_key_values</span> <span class="o">=</span> <span class="p">[]</span>
                        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">layer</span><span class="p">:</span>
                            <span class="n">layer_past_key_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>
                        <span class="n">past_key_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">layer_past_key_values</span><span class="p">))</span>
                    <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">GenerateEncoderDecoderOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">logits</span><span class="o">=</span><span class="n">raw_logits</span><span class="p">,</span>
                    <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_attentions</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">cross_attentions</span><span class="o">=</span><span class="n">cross_attentions</span><span class="p">,</span>
                    <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                    <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">GenerateDecoderOnlyOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">logits</span><span class="o">=</span><span class="n">raw_logits</span><span class="p">,</span>
                    <span class="n">attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                    <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">),</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">input_ids</span>

    <span class="k">def</span> <span class="nf">_sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">LogitsProcessorList</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="p">:</span> <span class="n">StoppingCriteriaList</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">streamer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;BaseStreamer&quot;</span><span class="p">],</span>
        <span class="n">logits_warper</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">GenerateNonBeamOutput</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences of token ids for models with a language modeling head using **multinomial sampling** and</span>
<span class="sd">        can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                The sequence used as a prompt for the generation.</span>
<span class="sd">            logits_processor (`LogitsProcessorList`):</span>
<span class="sd">                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]</span>
<span class="sd">                used to modify the prediction scores of the language modeling head applied at each generation step.</span>
<span class="sd">            stopping_criteria (`StoppingCriteriaList`):</span>
<span class="sd">                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]</span>
<span class="sd">                used to tell if the generation loop should stop.</span>
<span class="sd">            generation_config ([`~generation.GenerationConfig`]):</span>
<span class="sd">                The generation configuration to be used as parametrization of the decoding method.</span>
<span class="sd">            synced_gpus (`bool`):</span>
<span class="sd">                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</span>
<span class="sd">            streamer (`BaseStreamer`, *optional*):</span>
<span class="sd">                Streamer object that will be used to stream the generated sequences. Generated tokens are passed</span>
<span class="sd">                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.</span>
<span class="sd">            logits_warper (`LogitsProcessorList`, *optional*):</span>
<span class="sd">                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used</span>
<span class="sd">                to warp the prediction score distribution of the language modeling head applied before multinomial</span>
<span class="sd">                sampling at each generation step. Only required with sampling strategies (i.e. `do_sample` is set in</span>
<span class="sd">                `generation_config`)</span>
<span class="sd">            model_kwargs:</span>
<span class="sd">                Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is</span>
<span class="sd">                an encoder-decoder model the kwargs should include `encoder_outputs`.</span>

<span class="sd">        Return:</span>
<span class="sd">            [`~generation.GenerateDecoderOnlyOutput`], [`~generation.GenerateEncoderDecoderOutput`] or `mindspore.Tensor`:</span>
<span class="sd">            A `mindspore.Tensor` containing the generated tokens (default behaviour) or a</span>
<span class="sd">            [`~generation.GenerateDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and</span>
<span class="sd">            `return_dict_in_generate=True` or a [`~generation.GenerateEncoderDecoderOutput`] if</span>
<span class="sd">            `model.config.is_encoder_decoder=True`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># init values</span>
        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_pad_token_tensor</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="n">output_scores</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_scores</span>
        <span class="n">output_logits</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_logits</span>
        <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">return_dict_in_generate</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span>
        <span class="n">has_eos_stopping_criteria</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">criteria</span><span class="p">,</span> <span class="s2">&quot;eos_token_id&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">criteria</span> <span class="ow">in</span> <span class="n">stopping_criteria</span><span class="p">)</span>
        <span class="n">do_sample</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">do_sample</span>
        <span class="k">if</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">logits_warper</span><span class="p">,</span> <span class="n">LogitsProcessorList</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`do_sample` is set to `True`, `logits_warper` must be a `LogitsProcessorList` instance (it is &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">logits_warper</span><span class="si">}</span><span class="s2">).&quot;</span>
            <span class="p">)</span>

        <span class="c1"># init attention / hidden states / scores tuples</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">raw_logits</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_logits</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">cross_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_hidden_states</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># if model is an encoder-decoder, retrieve encoder attention weights and hidden states</span>
        <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">encoder_attentions</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attentions&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

        <span class="c1"># keep track of which sequences are already finished</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">this_peer_finished</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">unfinished_sequences</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_initial_cache_position</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>

        <span class="n">time_record</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">_record_time</span> <span class="o">=</span> <span class="n">parse_flag_from_env</span><span class="p">(</span><span class="s1">&#39;INFERENCE_TIME_RECORD&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_unfinished_sequences</span><span class="p">(</span>
            <span class="n">this_peer_finished</span><span class="p">,</span> <span class="n">synced_gpus</span><span class="p">,</span> <span class="n">cur_len</span><span class="o">=</span><span class="n">cur_len</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="n">_record_time</span><span class="p">:</span>
                <span class="n">infer_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="c1"># prepare model inputs</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

            <span class="c1"># prepare variable output controls (note: some models won&#39;t accept all output controls)</span>
            <span class="n">model_inputs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;output_attentions&quot;</span><span class="p">:</span> <span class="n">output_attentions</span><span class="p">}</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="p">{})</span>
            <span class="n">model_inputs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">:</span> <span class="n">output_hidden_states</span><span class="p">}</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="p">{})</span>

            <span class="c1"># forward pass to get next token</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">synced_gpus</span> <span class="ow">and</span> <span class="n">this_peer_finished</span><span class="p">:</span>
                <span class="k">continue</span>  <span class="c1"># don&#39;t waste resources running the code we don&#39;t need</span>

            <span class="c1"># Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration</span>
            <span class="c1"># (the clone itself is always small)</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

            <span class="c1"># pre-process distribution</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">logits_processor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_logits</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>
                <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">logits_warper</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_scores</span><span class="p">)</span>

            <span class="c1"># Store scores, attentions and hidden_states when required</span>
            <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="n">scores</span> <span class="o">+=</span> <span class="p">(</span><span class="n">next_token_scores</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_logits</span><span class="p">:</span>
                    <span class="n">raw_logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">next_token_logits</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                    <span class="n">decoder_attentions</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,)</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                        <span class="n">cross_attentions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,)</span>

                <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                    <span class="n">decoder_hidden_states</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,)</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
                        <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,)</span>
                    <span class="p">)</span>

            <span class="c1"># token selection</span>
            <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>
                <span class="n">probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                <span class="c1"># TODO (joao): this OP throws &quot;skipping cudagraphs due to [&#39;incompatible ops&#39;]&quot;, find solution</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># finished sentences should have their next token be a padding token</span>
            <span class="k">if</span> <span class="n">has_eos_stopping_criteria</span><span class="p">:</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">next_tokens</span> <span class="o">*</span> <span class="n">unfinished_sequences</span> <span class="o">+</span> <span class="n">pad_token_id</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">unfinished_sequences</span><span class="p">)</span>
            <span class="c1"># update generated ids, model inputs, and length for next step</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">streamer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">streamer</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">)</span>
            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span>
                <span class="n">outputs</span><span class="p">,</span>
                <span class="n">model_kwargs</span><span class="p">,</span>
                <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">unfinished_sequences</span> <span class="o">=</span> <span class="n">unfinished_sequences</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">stopping_criteria</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
            <span class="n">this_peer_finished</span> <span class="o">=</span> <span class="n">unfinished_sequences</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="n">cur_len</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">_record_time</span><span class="p">:</span>
                <span class="n">infer_stop</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
                <span class="n">time_record</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">infer_stop</span> <span class="o">-</span> <span class="n">infer_start</span><span class="p">)</span>
            <span class="c1"># This is needed to properly delete outputs.logits which may be very large for first iteration</span>
            <span class="c1"># Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration</span>
            <span class="k">del</span> <span class="n">outputs</span>

        <span class="n">average_infer_time</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">time_record</span><span class="p">:</span>
            <span class="n">time_record</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">average_infer_time</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">time_record</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">time_record</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;average inference time is: </span><span class="si">{</span><span class="n">average_infer_time</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;inference time record: </span><span class="si">{</span><span class="n">time_record</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">streamer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">streamer</span><span class="o">.</span><span class="n">end</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">GenerateEncoderDecoderOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">logits</span><span class="o">=</span><span class="n">raw_logits</span><span class="p">,</span>
                    <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_attentions</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">cross_attentions</span><span class="o">=</span><span class="n">cross_attentions</span><span class="p">,</span>
                    <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                    <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">),</span>
                    <span class="n">average_infer_time</span><span class="o">=</span><span class="n">average_infer_time</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">GenerateDecoderOnlyOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">logits</span><span class="o">=</span><span class="n">raw_logits</span><span class="p">,</span>
                    <span class="n">attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                    <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">),</span>
                    <span class="n">average_infer_time</span><span class="o">=</span><span class="n">average_infer_time</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">input_ids</span>

    <span class="k">def</span> <span class="nf">_temporary_reorder_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">past_key_values</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Temporary function to handle the different types of cache reordering processes while we roll out `Cache`.</span>

<span class="sd">        TODO: standardize cache formats and make all models compatible with `Cache`. It would remove the need</span>
<span class="sd">        for this function, with `Cache.reorder_cache` being the sole remaining code path</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_class</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="c1"># Exception 1: code path for models using the legacy cache format</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="n">past_key_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reorder_cache</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span>
        <span class="c1"># Exception 2: models with different cache formats. These are limited to `DynamicCache` until their</span>
        <span class="c1"># cache format is standardized, to avoid adding complexity to the codebase.</span>
        <span class="k">elif</span> <span class="s2">&quot;gptbigcode&quot;</span> <span class="ow">in</span> <span class="n">model_class</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">,</span> <span class="p">(</span><span class="n">DynamicCache</span><span class="p">,</span> <span class="n">EncoderDecoderCache</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Using an unsupported cache format with </span><span class="si">{</span><span class="n">model_class</span><span class="si">}</span><span class="s2">. Currently, it only supports the &quot;</span>
                    <span class="s2">&quot;legacy tuple format or `DynamicCache`&quot;</span>
                <span class="p">)</span>
            <span class="n">past_key_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reorder_cache</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span>
            <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">DynamicCache</span><span class="o">.</span><span class="n">from_legacy_cache</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">)</span>
        <span class="c1"># Standard code path: use the `Cache.reorder_cache`</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">past_key_values</span><span class="o">.</span><span class="n">reorder_cache</span><span class="p">(</span><span class="n">beam_idx</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">past_key_values</span>

    <span class="k">def</span> <span class="nf">_beam_search</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">beam_scorer</span><span class="p">:</span> <span class="n">BeamScorer</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">LogitsProcessorList</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="p">:</span> <span class="n">StoppingCriteriaList</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">logits_warper</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">],</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">GenerateBeamOutput</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences of token ids for models with a language modeling head using **beam search decoding** and</span>
<span class="sd">        can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                The sequence used as a prompt for the generation.</span>
<span class="sd">            beam_scorer (`BeamScorer`):</span>
<span class="sd">                An derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and</span>
<span class="sd">                sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.</span>
<span class="sd">            logits_processor (`LogitsProcessorList`):</span>
<span class="sd">                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]</span>
<span class="sd">                used to modify the prediction scores of the language modeling head applied at each generation step.</span>
<span class="sd">            stopping_criteria (`StoppingCriteriaList`:</span>
<span class="sd">                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]</span>
<span class="sd">                used to tell if the generation loop should stop.</span>
<span class="sd">            generation_config ([`~generation.GenerationConfig`]):</span>
<span class="sd">                The generation configuration to be used as parametrization of the decoding method.</span>
<span class="sd">            synced_gpus (`bool`):</span>
<span class="sd">                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</span>
<span class="sd">            logits_warper (`LogitsProcessorList`, *optional*):</span>
<span class="sd">                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used</span>
<span class="sd">                to warp the prediction score distribution of the language modeling head applied before multinomial</span>
<span class="sd">                sampling at each generation step. Only required with sampling strategies (i.e. `do_sample` is set in</span>
<span class="sd">                `generation_config`)</span>
<span class="sd">            model_kwargs:</span>
<span class="sd">                Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is</span>
<span class="sd">                an encoder-decoder model the kwargs should include `encoder_outputs`.</span>

<span class="sd">        Return:</span>
<span class="sd">            [`generation.GenerateBeamDecoderOnlyOutput`], [`~generation.GenerateBeamEncoderDecoderOutput`] or</span>
<span class="sd">            `mindspore.Tensor`: A `mindspore.Tensor` containing the generated tokens (default behaviour) or a</span>
<span class="sd">            [`~generation.GenerateBeamDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and</span>
<span class="sd">            `return_dict_in_generate=True` or a [`~generation.GenerateBeamEncoderDecoderOutput`] if</span>
<span class="sd">            `model.config.is_encoder_decoder=True`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># init values</span>
        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_pad_token_tensor</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="n">output_scores</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_scores</span>
        <span class="n">output_logits</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_logits</span>
        <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">return_dict_in_generate</span>
        <span class="n">sequential</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">low_memory</span>
        <span class="n">do_sample</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">do_sample</span>
        <span class="k">if</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">logits_warper</span><span class="p">,</span> <span class="n">LogitsProcessorList</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`do_sample` is set to `True`, `logits_warper` must be a `LogitsProcessorList` instance (it is &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">logits_warper</span><span class="si">}</span><span class="s2">).&quot;</span>
            <span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">_beam_hyps</span><span class="p">)</span>
        <span class="n">num_beams</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span>

        <span class="n">batch_beam_size</span><span class="p">,</span> <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_initial_cache_position</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">!=</span> <span class="n">batch_beam_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Batch dimension of `input_ids` should be </span><span class="si">{</span><span class="n">num_beams</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">batch_size</span><span class="si">}</span><span class="s2">, but is </span><span class="si">{</span><span class="n">batch_beam_size</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># init attention / hidden states / scores tuples</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">raw_logits</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_logits</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">beam_indices</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">tuple</span><span class="p">(()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_beam_size</span><span class="p">))</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="n">decoder_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">cross_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_hidden_states</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># if model is an encoder-decoder, retrieve encoder attention weights and hidden states</span>
        <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">encoder_attentions</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attentions&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

        <span class="c1"># initialise score of first beam with 0 and the rest with -1e9. This makes sure that only tokens</span>
        <span class="c1"># of the first beam are considered to avoid sampling the exact same tokens across all beams.</span>
        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">beam_scores</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e9</span>
        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_scores</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,))</span>

        <span class="n">this_peer_finished</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">decoder_prompt_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># record the prompt length of decoder</span>


        <span class="n">time_record</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">_record_time</span> <span class="o">=</span> <span class="n">parse_flag_from_env</span><span class="p">(</span><span class="s1">&#39;INFERENCE_TIME_RECORD&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_unfinished_sequences</span><span class="p">(</span><span class="n">this_peer_finished</span><span class="p">,</span> <span class="n">synced_gpus</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">_record_time</span><span class="p">:</span>
                <span class="n">infer_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

            <span class="c1"># prepare variable output controls (note: some models won&#39;t accept all output controls)</span>
            <span class="n">model_inputs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;output_attentions&quot;</span><span class="p">:</span> <span class="n">output_attentions</span><span class="p">}</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="p">{})</span>
            <span class="n">model_inputs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">:</span> <span class="n">output_hidden_states</span><span class="p">}</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="p">{})</span>

            <span class="c1"># if sequential is True, split the input to batches of batch_size and run sequentially</span>
            <span class="k">if</span> <span class="n">sequential</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
                    <span class="n">model_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
                    <span class="k">for</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="p">[</span>
                        <span class="s2">&quot;fsmt&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;reformer&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;ctrl&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;gpt_bigcode&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;transo_xl&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;xlnet&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;cpm&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;jamba&quot;</span><span class="p">,</span>
                    <span class="p">]</span>
                <span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Currently generation for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> is not supported &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;for `low_memory beam_search`. Please open an issue on GitHub if you need this feature.&quot;</span>
                    <span class="p">)</span>

                <span class="n">inputs_per_sub_batches</span> <span class="o">=</span> <span class="n">_split_model_inputs</span><span class="p">(</span>
                    <span class="n">model_inputs</span><span class="p">,</span> <span class="n">split_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">full_batch_size</span><span class="o">=</span><span class="n">batch_beam_size</span>
                <span class="p">)</span>
                <span class="n">outputs_per_sub_batch</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="bp">self</span><span class="p">(</span><span class="o">**</span><span class="n">inputs_per_sub_batch</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">inputs_per_sub_batch</span> <span class="ow">in</span> <span class="n">inputs_per_sub_batches</span>
                <span class="p">]</span>

                <span class="n">outputs</span> <span class="o">=</span> <span class="n">stack_model_outputs</span><span class="p">(</span><span class="n">outputs_per_sub_batch</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>  <span class="c1"># Unchanged original behavior</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">synced_gpus</span> <span class="ow">and</span> <span class="n">this_peer_finished</span><span class="p">:</span>
                <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="k">continue</span>  <span class="c1"># don&#39;t waste resources running the code we don&#39;t need</span>

            <span class="c1"># Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration</span>
            <span class="c1"># (the clone itself is always small)</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span>
                <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
            <span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>

            <span class="n">next_token_scores_processed</span> <span class="o">=</span> <span class="n">logits_processor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_scores</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>
                <span class="n">next_token_scores_processed</span> <span class="o">=</span> <span class="n">logits_warper</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_scores_processed</span><span class="p">)</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores_processed</span> <span class="o">+</span> <span class="n">beam_scores</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span>
                <span class="n">next_token_scores_processed</span>
            <span class="p">)</span>

            <span class="c1"># Store scores, attentions and hidden_states when required</span>
            <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="n">scores</span> <span class="o">+=</span> <span class="p">(</span><span class="n">next_token_scores_processed</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_logits</span><span class="p">:</span>
                    <span class="n">raw_logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">next_token_logits</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                    <span class="n">decoder_attentions</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,)</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                        <span class="n">cross_attentions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                    <span class="n">decoder_hidden_states</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,)</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
                        <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,)</span>
                    <span class="p">)</span>

            <span class="c1"># reshape for beam search</span>
            <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">next_token_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">vocab_size</span><span class="p">)</span>

            <span class="c1"># Beam token selection: pick 1 + eos_token_id.shape[0] next tokens for each beam so we have at least 1</span>
            <span class="c1"># non eos token per beam.</span>
            <span class="n">n_eos_tokens</span> <span class="o">=</span> <span class="n">eos_token_id</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="n">n_tokens_to_keep</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">n_eos_tokens</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_beams</span>
            <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>
                <span class="n">probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">n_tokens_to_keep</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
                    <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">tf_gather</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="n">next_token_scores</span><span class="p">,</span> <span class="n">_indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="n">next_token_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">tf_gather</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">,</span> <span class="n">_indices</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">)</span>
                    <span class="n">next_token_scores</span><span class="p">,</span> <span class="n">_indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">_indices</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">next_token_scores</span><span class="p">,</span> <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span>
                    <span class="n">next_token_scores</span><span class="p">,</span> <span class="n">n_tokens_to_keep</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">largest</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>

            <span class="k">def</span> <span class="nf">replace_negative_indices</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">):</span>
                <span class="n">next_tokens_np</span> <span class="o">=</span> <span class="n">next_tokens</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>

                <span class="n">used_indices</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">next_tokens_np</span><span class="p">[</span><span class="n">next_tokens_np</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
                <span class="n">min_unused</span> <span class="o">=</span> <span class="mi">0</span>

                <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">token_row</span> <span class="ow">in</span> <span class="n">next_tokens_np</span><span class="p">:</span>
                    <span class="n">new_row</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">token_row</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                            <span class="k">while</span> <span class="n">min_unused</span> <span class="ow">in</span> <span class="n">used_indices</span><span class="p">:</span>
                                <span class="n">min_unused</span> <span class="o">+=</span> <span class="mi">1</span>
                            <span class="n">new_row</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">min_unused</span><span class="p">)</span>
                            <span class="n">used_indices</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">min_unused</span><span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">new_row</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                    <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_row</span><span class="p">)</span>

                <span class="k">return</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">next_tokens_np</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

            <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">replace_negative_indices</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">)</span>

            <span class="n">next_indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="s2">&quot;floor&quot;</span><span class="p">)</span>
            <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">next_tokens</span> <span class="o">%</span> <span class="n">vocab_size</span>

            <span class="c1"># stateless</span>
            <span class="n">beam_outputs</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">process</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">next_token_scores</span><span class="p">,</span>
                <span class="n">next_tokens</span><span class="p">,</span>
                <span class="n">next_indices</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
                <span class="n">beam_indices</span><span class="o">=</span><span class="n">beam_indices</span><span class="p">,</span>
                <span class="n">decoder_prompt_len</span><span class="o">=</span><span class="n">decoder_prompt_len</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_scores&quot;</span><span class="p">]</span>
            <span class="n">beam_next_tokens</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_tokens&quot;</span><span class="p">]</span>
            <span class="n">beam_idx</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_indices&quot;</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
                <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">ops</span><span class="o">.</span><span class="n">getitem</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="p">(</span><span class="n">beam_idx</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))),</span> <span class="n">beam_next_tokens</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">[</span><span class="n">beam_idx</span><span class="p">,</span> <span class="p">:],</span> <span class="n">beam_next_tokens</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span>
                <span class="n">outputs</span><span class="p">,</span>
                <span class="n">model_kwargs</span><span class="p">,</span>
                <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># This is needed to properly delete outputs.logits which may be very large for first iteration</span>
            <span class="c1"># Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration</span>
            <span class="c1"># IMPORTANT: Note that this should appear BEFORE the call to _reorder_cache() to save the maximum memory</span>
            <span class="c1"># (that way the memory peak does not include outputs.logits)</span>
            <span class="k">del</span> <span class="n">outputs</span>

            <span class="k">if</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_temporary_reorder_cache</span><span class="p">(</span>
                    <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">],</span> <span class="n">beam_idx</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">:</span>
                <span class="n">beam_indices</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">((</span><span class="n">beam_indices</span><span class="p">[</span><span class="n">beam_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+</span> <span class="p">(</span><span class="n">beam_idx</span><span class="p">[</span><span class="n">i</span><span class="p">],)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">beam_indices</span><span class="p">))))</span>

            <span class="c1"># increase cur_len</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">is_done</span> <span class="ow">or</span> <span class="nb">all</span><span class="p">(</span><span class="n">stopping_criteria</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">)):</span>
                <span class="n">this_peer_finished</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="k">if</span> <span class="n">_record_time</span><span class="p">:</span>
                <span class="n">infer_stop</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
                <span class="n">time_record</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">infer_stop</span> <span class="o">-</span> <span class="n">infer_start</span><span class="p">)</span>

        <span class="n">average_infer_time</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">time_record</span><span class="p">:</span>
            <span class="n">time_record</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">average_infer_time</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">time_record</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">time_record</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;average inference time is: </span><span class="si">{</span><span class="n">average_infer_time</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;inference time record: </span><span class="si">{</span><span class="n">time_record</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="n">sequence_outputs</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">finalize</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">beam_scores</span><span class="p">,</span>
            <span class="n">next_tokens</span><span class="p">,</span>
            <span class="n">next_indices</span><span class="p">,</span>
            <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
            <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">stopping_criteria</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">beam_indices</span><span class="o">=</span><span class="n">beam_indices</span><span class="p">,</span>
            <span class="n">decoder_prompt_len</span><span class="o">=</span><span class="n">decoder_prompt_len</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">output_scores</span><span class="p">:</span>
                <span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">GenerateBeamEncoderDecoderOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">],</span>
                    <span class="n">sequences_scores</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">],</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">logits</span><span class="o">=</span><span class="n">raw_logits</span><span class="p">,</span>
                    <span class="n">beam_indices</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;beam_indices&quot;</span><span class="p">],</span>
                    <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_attentions</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">cross_attentions</span><span class="o">=</span><span class="n">cross_attentions</span><span class="p">,</span>
                    <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                    <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">GenerateBeamDecoderOnlyOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">],</span>
                    <span class="n">sequences_scores</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">],</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">logits</span><span class="o">=</span><span class="n">raw_logits</span><span class="p">,</span>
                    <span class="n">beam_indices</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;beam_indices&quot;</span><span class="p">],</span>
                    <span class="n">attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                    <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">),</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_group_beam_search</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">beam_scorer</span><span class="p">:</span> <span class="n">BeamScorer</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">LogitsProcessorList</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="p">:</span> <span class="n">StoppingCriteriaList</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences of token ids for models with a language modeling head using **diverse beam search</span>
<span class="sd">        decoding** and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                The sequence used as a prompt for the generation.</span>
<span class="sd">            beam_scorer (`BeamScorer`):</span>
<span class="sd">                An derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and</span>
<span class="sd">                sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.</span>
<span class="sd">            logits_processor (`LogitsProcessorList`):</span>
<span class="sd">                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]</span>
<span class="sd">                used to modify the prediction scores of the language modeling head applied at each generation step.</span>
<span class="sd">            stopping_criteria (`StoppingCriteriaList`):</span>
<span class="sd">                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]</span>
<span class="sd">                used to tell if the generation loop should stop.</span>
<span class="sd">            generation_config ([`~generation.GenerationConfig`]):</span>
<span class="sd">                The generation configuration to be used as parametrization of the decoding method.</span>
<span class="sd">            synced_gpus (`bool`):</span>
<span class="sd">                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</span>
<span class="sd">            model_kwargs:</span>
<span class="sd">                Additional model specific kwargs that will be forwarded to the `forward` function of the model. If</span>
<span class="sd">                model is an encoder-decoder model the kwargs should include `encoder_outputs`.</span>

<span class="sd">        Return:</span>
<span class="sd">            [`~generation.GenerateBeamDecoderOnlyOutput`], [`~generation.GenerateBeamEncoderDecoderOutput`] or</span>
<span class="sd">            `mindspore.Tensor`: A `mindspore.Tensor` containing the generated tokens (default behaviour) or a</span>
<span class="sd">            [`~generation.GenerateBeamDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and</span>
<span class="sd">            `return_dict_in_generate=True` or a [`~generation.GenerateBeamEncoderDecoderOutput`] if</span>
<span class="sd">            `model.config.is_encoder_decoder=True`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># init values</span>
        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_pad_token_tensor</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="n">output_scores</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_scores</span>
        <span class="n">output_logits</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_logits</span>
        <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">return_dict_in_generate</span>

        <span class="n">num_beams</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span>
        <span class="n">num_beam_groups</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beam_groups</span>
        <span class="n">num_sub_beams</span> <span class="o">=</span> <span class="n">num_beams</span> <span class="o">//</span> <span class="n">num_beam_groups</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">_beam_hyps</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_beam_groups</span>

        <span class="n">batch_beam_size</span><span class="p">,</span> <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_initial_cache_position</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">:</span>
            <span class="n">beam_indices</span> <span class="o">=</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">(()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_sub_beams</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_beam_groups</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">beam_indices</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">!=</span> <span class="n">batch_beam_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Batch dimension of `input_ids` should be </span><span class="si">{</span><span class="n">num_beams</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">batch_size</span><span class="si">}</span><span class="s2">, but is </span><span class="si">{</span><span class="n">batch_beam_size</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># init attention / hidden states / scores tuples</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">raw_logits</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_logits</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">cross_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_hidden_states</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># if model is an encoder-decoder, retrieve encoder attention weights and hidden states</span>
        <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">encoder_attentions</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attentions&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

        <span class="c1"># initialise score of first beam of each group with 0 and the rest with -1e9. This ensures that the beams in</span>
        <span class="c1"># the same group don&#39;t produce same tokens everytime.</span>
        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">),</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">beam_scores</span><span class="p">[:,</span> <span class="p">::</span><span class="n">num_sub_beams</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_scores</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,))</span>

        <span class="n">this_peer_finished</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">decoder_prompt_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># record the prompt length of decoder</span>
        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_unfinished_sequences</span><span class="p">(</span><span class="n">this_peer_finished</span><span class="p">,</span> <span class="n">synced_gpus</span><span class="p">):</span>
            <span class="c1"># predicted tokens in cur_len step</span>
            <span class="n">current_tokens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

            <span class="c1"># indices which will form the beams in the next time step</span>
            <span class="n">reordering_indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

            <span class="c1"># do one decoder step on all beams of all sentences in batch</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

            <span class="c1"># prepare variable output controls (note: some models won&#39;t accept all output controls)</span>
            <span class="n">model_inputs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;output_attentions&quot;</span><span class="p">:</span> <span class="n">output_attentions</span><span class="p">}</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="p">{})</span>
            <span class="n">model_inputs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">:</span> <span class="n">output_hidden_states</span><span class="p">}</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="p">{})</span>

            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">synced_gpus</span> <span class="ow">and</span> <span class="n">this_peer_finished</span><span class="p">:</span>
                <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="k">continue</span>  <span class="c1"># don&#39;t waste resources running the code we don&#39;t need</span>

            <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                <span class="n">processed_score</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>
            <span class="k">if</span> <span class="n">output_logits</span><span class="p">:</span>
                <span class="c1"># Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration</span>
                <span class="c1"># (the clone itself is always small)</span>
                <span class="n">raw_logit_score</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

            <span class="k">for</span> <span class="n">beam_group_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_beam_groups</span><span class="p">):</span>
                <span class="n">group_start_idx</span> <span class="o">=</span> <span class="n">beam_group_idx</span> <span class="o">*</span> <span class="n">num_sub_beams</span>
                <span class="n">group_end_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">group_start_idx</span> <span class="o">+</span> <span class="n">num_sub_beams</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">)</span>
                <span class="n">group_size</span> <span class="o">=</span> <span class="n">group_end_idx</span> <span class="o">-</span> <span class="n">group_start_idx</span>

                <span class="c1"># indices of beams of current group among all sentences in batch</span>
                <span class="n">batch_group_indices</span> <span class="o">=</span> <span class="p">[]</span>

                <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
                    <span class="n">batch_group_indices</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
                        <span class="p">[</span><span class="n">batch_idx</span> <span class="o">*</span> <span class="n">num_beams</span> <span class="o">+</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group_start_idx</span><span class="p">,</span> <span class="n">group_end_idx</span><span class="p">)]</span>
                    <span class="p">)</span>
                <span class="n">group_input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">batch_group_indices</span><span class="p">]</span>

                <span class="c1"># select outputs of beams of current group only</span>
                <span class="c1"># No need to clone() the logits here as they will not retain outputs.logits at the end of the loop</span>
                <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
                    <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">getitem</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_group_indices</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[</span><span class="n">batch_group_indices</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
                <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span>
                    <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
                <span class="p">)</span>  <span class="c1"># (batch_size * group_size, vocab_size)</span>
                <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">next_token_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

                <span class="n">next_token_scores_processed</span> <span class="o">=</span> <span class="n">logits_processor</span><span class="p">(</span>
                    <span class="n">group_input_ids</span><span class="p">,</span> <span class="n">next_token_scores</span><span class="p">,</span> <span class="n">current_tokens</span><span class="o">=</span><span class="n">current_tokens</span><span class="p">,</span> <span class="n">beam_group_idx</span><span class="o">=</span><span class="n">beam_group_idx</span>
                <span class="p">)</span>
                <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores_processed</span> <span class="o">+</span> <span class="n">beam_scores</span><span class="p">[</span><span class="n">batch_group_indices</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">next_token_scores_processed</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
                        <span class="n">ops</span><span class="o">.</span><span class="n">setitem</span><span class="p">(</span><span class="n">processed_score</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_group_indices</span><span class="p">,),</span> <span class="n">next_token_scores_processed</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">processed_score</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">processed_score</span><span class="p">[</span><span class="n">batch_group_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_token_scores_processed</span>

                <span class="c1"># reshape for beam search</span>
                <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">group_size</span> <span class="o">*</span> <span class="n">vocab_size</span><span class="p">)</span>

                <span class="c1"># Sample 1 + len(eos_token_id) next tokens for each beam so we have at least 1 non eos token per beam.</span>
                <span class="n">n_eos_tokens</span> <span class="o">=</span> <span class="n">eos_token_id</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
                <span class="n">next_token_scores</span><span class="p">,</span> <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span>
                    <span class="n">next_token_scores</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">n_eos_tokens</span><span class="p">)</span> <span class="o">*</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">largest</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>

                <span class="n">next_indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="s2">&quot;floor&quot;</span><span class="p">)</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">next_tokens</span> <span class="o">%</span> <span class="n">vocab_size</span>

                <span class="c1"># stateless</span>
                <span class="n">process_beam_indices</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">beam_indices</span><span class="p">,</span> <span class="p">())</span> <span class="k">if</span> <span class="n">beam_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
                <span class="n">beam_outputs</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">process</span><span class="p">(</span>
                    <span class="n">group_input_ids</span><span class="p">,</span>
                    <span class="n">next_token_scores</span><span class="p">,</span>
                    <span class="n">next_tokens</span><span class="p">,</span>
                    <span class="n">next_indices</span><span class="p">,</span>
                    <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                    <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
                    <span class="n">beam_indices</span><span class="o">=</span><span class="n">process_beam_indices</span><span class="p">,</span>
                    <span class="n">group_index</span><span class="o">=</span><span class="n">beam_group_idx</span><span class="p">,</span>
                    <span class="n">decoder_prompt_len</span><span class="o">=</span><span class="n">decoder_prompt_len</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
                    <span class="n">ops</span><span class="o">.</span><span class="n">setitem</span><span class="p">(</span><span class="n">beam_scores</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_group_indices</span><span class="p">,),</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_scores&quot;</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">beam_scores</span><span class="p">[</span><span class="n">batch_group_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_scores&quot;</span><span class="p">]</span>
                <span class="n">beam_next_tokens</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_tokens&quot;</span><span class="p">]</span>
                <span class="n">beam_idx</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_indices&quot;</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="n">beam_indices</span><span class="p">[</span><span class="n">beam_group_idx</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
                        <span class="n">beam_indices</span><span class="p">[</span><span class="n">beam_group_idx</span><span class="p">][</span><span class="n">beam_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+</span> <span class="p">(</span><span class="n">beam_idx</span><span class="p">[</span><span class="n">i</span><span class="p">],)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">beam_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
                    <span class="p">)</span>

                <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
                    <span class="n">ops</span><span class="o">.</span><span class="n">setitem</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_group_indices</span><span class="p">,),</span> <span class="n">group_input_ids</span><span class="p">[</span><span class="n">beam_idx</span><span class="p">])</span>

                    <span class="n">group_input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">ops</span><span class="o">.</span><span class="n">getitem</span><span class="p">(</span><span class="n">group_input_ids</span><span class="p">,</span> <span class="p">(</span><span class="n">beam_idx</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))),</span> <span class="n">beam_next_tokens</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">ops</span><span class="o">.</span><span class="n">setitem</span><span class="p">(</span><span class="n">current_tokens</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_group_indices</span><span class="p">,),</span> <span class="n">group_input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

                    <span class="c1"># (beam_idx // group_size) -&gt; batch_idx</span>
                    <span class="c1"># (beam_idx % group_size) -&gt; offset of idx inside the group</span>
                    <span class="n">ops</span><span class="o">.</span><span class="n">setitem</span><span class="p">(</span>
                        <span class="n">reordering_indices</span><span class="p">,</span>
                        <span class="p">(</span><span class="n">batch_group_indices</span><span class="p">,),</span>
                        <span class="n">num_beams</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">beam_idx</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="s2">&quot;floor&quot;</span><span class="p">)</span>
                        <span class="o">+</span> <span class="n">group_start_idx</span>
                        <span class="o">+</span> <span class="p">(</span><span class="n">beam_idx</span> <span class="o">%</span> <span class="n">group_size</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">input_ids</span><span class="p">[</span><span class="n">batch_group_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">group_input_ids</span><span class="p">[</span><span class="n">beam_idx</span><span class="p">]</span>

                    <span class="n">group_input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">group_input_ids</span><span class="p">[</span><span class="n">beam_idx</span><span class="p">,</span> <span class="p">:],</span> <span class="n">beam_next_tokens</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">current_tokens</span><span class="p">[</span><span class="n">batch_group_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">group_input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

                    <span class="c1"># (beam_idx // group_size) -&gt; batch_idx</span>
                    <span class="c1"># (beam_idx % group_size) -&gt; offset of idx inside the group</span>
                    <span class="n">reordering_indices</span><span class="p">[</span><span class="n">batch_group_indices</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">num_beams</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">beam_idx</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="s2">&quot;floor&quot;</span><span class="p">)</span>
                        <span class="o">+</span> <span class="n">group_start_idx</span>
                        <span class="o">+</span> <span class="p">(</span><span class="n">beam_idx</span> <span class="o">%</span> <span class="n">group_size</span><span class="p">)</span>
                    <span class="p">)</span>

            <span class="c1"># Store scores, attentions and hidden_states when required</span>
            <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="n">scores</span> <span class="o">+=</span> <span class="p">(</span><span class="n">processed_score</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_logits</span><span class="p">:</span>
                    <span class="n">raw_logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">raw_logit_score</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                    <span class="n">decoder_attentions</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,)</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                        <span class="n">cross_attentions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,)</span>

                <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                    <span class="n">decoder_hidden_states</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,)</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
                        <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,)</span>
                    <span class="p">)</span>

            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">current_tokens</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span>
                <span class="n">outputs</span><span class="p">,</span>
                <span class="n">model_kwargs</span><span class="p">,</span>
                <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># This is needed to properly delete outputs.logits which may be very large for first iteration</span>
            <span class="c1"># Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration</span>
            <span class="c1"># IMPORTANT: Note that this should appear BEFORE the call to _reorder_cache() to save the maximum memory</span>
            <span class="c1"># (that way the memory peak does not include outputs.logits)</span>
            <span class="k">del</span> <span class="n">outputs</span>

            <span class="k">if</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_temporary_reorder_cache</span><span class="p">(</span>
                    <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">],</span> <span class="n">reordering_indices</span>
                <span class="p">)</span>

            <span class="c1"># increase cur_len</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">is_done</span> <span class="ow">or</span> <span class="nb">all</span><span class="p">(</span><span class="n">stopping_criteria</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">)):</span>
                <span class="n">this_peer_finished</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">final_beam_indices</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">beam_indices</span><span class="p">,</span> <span class="p">())</span> <span class="k">if</span> <span class="n">beam_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">sequence_outputs</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">finalize</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">beam_scores</span><span class="p">,</span>
            <span class="n">next_tokens</span><span class="p">,</span>
            <span class="n">next_indices</span><span class="p">,</span>
            <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
            <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">stopping_criteria</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">beam_indices</span><span class="o">=</span><span class="n">final_beam_indices</span><span class="p">,</span>
            <span class="n">decoder_prompt_len</span><span class="o">=</span><span class="n">decoder_prompt_len</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">output_scores</span><span class="p">:</span>
                <span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">GenerateBeamEncoderDecoderOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">],</span>
                    <span class="n">sequences_scores</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">],</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">logits</span><span class="o">=</span><span class="n">raw_logits</span><span class="p">,</span>
                    <span class="n">beam_indices</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;beam_indices&quot;</span><span class="p">],</span>
                    <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_attentions</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">cross_attentions</span><span class="o">=</span><span class="n">cross_attentions</span><span class="p">,</span>
                    <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                    <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">GenerateBeamDecoderOnlyOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">],</span>
                    <span class="n">sequences_scores</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">],</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">logits</span><span class="o">=</span><span class="n">raw_logits</span><span class="p">,</span>
                    <span class="n">beam_indices</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;beam_indices&quot;</span><span class="p">],</span>
                    <span class="n">attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                    <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">),</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_constrained_beam_search</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">constrained_beam_scorer</span><span class="p">:</span> <span class="n">ConstrainedBeamSearchScorer</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">LogitsProcessorList</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="p">:</span> <span class="n">StoppingCriteriaList</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">GenerateBeamOutput</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences of token ids for models with a language modeling head using **constrained beam search</span>
<span class="sd">        decoding** and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                The sequence used as a prompt for the generation.</span>
<span class="sd">            constrained_beam_scorer (`ConstrainedBeamSearchScorer`):</span>
<span class="sd">                A derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and</span>
<span class="sd">                sorted during generation, while satisfying a list of positive constraints. For more information, the</span>
<span class="sd">                documentation of [`ConstrainedBeamSearchScorer`] should be read.</span>
<span class="sd">            logits_processor (`LogitsProcessorList`):</span>
<span class="sd">                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]</span>
<span class="sd">                used to modify the prediction scores of the language modeling head applied at each generation step.</span>
<span class="sd">            stopping_criteria (`StoppingCriteriaList`):</span>
<span class="sd">                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]</span>
<span class="sd">                used to tell if the generation loop should stop.</span>
<span class="sd">            logits_warper (`LogitsProcessorList`):</span>
<span class="sd">                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used</span>
<span class="sd">                to warp the prediction score distribution of the language modeling head applied before multinomial</span>
<span class="sd">                sampling at each generation step.</span>
<span class="sd">            generation_config ([`~generation.GenerationConfig`]):</span>
<span class="sd">                The generation configuration to be used as parametrization of the decoding method.</span>
<span class="sd">            synced_gpus (`bool`):</span>
<span class="sd">                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</span>
<span class="sd">            model_kwargs:</span>
<span class="sd">                Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is</span>
<span class="sd">                an encoder-decoder model the kwargs should include `encoder_outputs`.</span>

<span class="sd">        Return:</span>
<span class="sd">            [`~generation.GenerateBeamDecoderOnlyOutput`], [`~generation.GenerateBeamEncoderDecoderOutput`] or</span>
<span class="sd">            `mindspore.Tensor`: A `mindspore.Tensor` containing the generated tokens (default behaviour) or a</span>
<span class="sd">            [`~generation.GenerateBeamDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and</span>
<span class="sd">            `return_dict_in_generate=True` or a [`~generation.GenerateBeamEncoderDecoderOutput`] if</span>
<span class="sd">            `model.config.is_encoder_decoder=True`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># init values</span>
        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_pad_token_tensor</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="n">output_scores</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_scores</span>
        <span class="n">output_logits</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_logits</span>
        <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">return_dict_in_generate</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">constrained_beam_scorer</span><span class="o">.</span><span class="n">_beam_hyps</span><span class="p">)</span>
        <span class="n">num_beams</span> <span class="o">=</span> <span class="n">constrained_beam_scorer</span><span class="o">.</span><span class="n">num_beams</span>

        <span class="n">batch_beam_size</span><span class="p">,</span> <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_initial_cache_position</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">!=</span> <span class="n">batch_beam_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Batch dimension of `input_ids` should be </span><span class="si">{</span><span class="n">num_beams</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">batch_size</span><span class="si">}</span><span class="s2">, but is </span><span class="si">{</span><span class="n">batch_beam_size</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># init attention / hidden states / scores tuples</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">raw_logits</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_logits</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">beam_indices</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">tuple</span><span class="p">(()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_beam_size</span><span class="p">))</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="n">decoder_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">cross_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_hidden_states</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># if model is an encoder-decoder, retrieve encoder attention weights and hidden states</span>
        <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">encoder_attentions</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attentions&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

        <span class="c1"># initialise score of first beam with 0 and the rest with -1e9. This makes sure that only tokens</span>
        <span class="c1"># of the first beam are considered to avoid sampling the exact same tokens across all beams.</span>
        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">beam_scores</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e9</span>
        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_scores</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,))</span>

        <span class="n">this_peer_finished</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">decoder_prompt_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># record the prompt length of decoder</span>
        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_unfinished_sequences</span><span class="p">(</span><span class="n">this_peer_finished</span><span class="p">,</span> <span class="n">synced_gpus</span><span class="p">):</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

            <span class="c1"># prepare variable output controls (note: some models won&#39;t accept all output controls)</span>
            <span class="n">model_inputs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;output_attentions&quot;</span><span class="p">:</span> <span class="n">output_attentions</span><span class="p">}</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="p">{})</span>
            <span class="n">model_inputs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">:</span> <span class="n">output_hidden_states</span><span class="p">}</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="p">{})</span>

            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">synced_gpus</span> <span class="ow">and</span> <span class="n">this_peer_finished</span><span class="p">:</span>
                <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="k">continue</span>  <span class="c1"># don&#39;t waste resources running the code we don&#39;t need</span>

            <span class="c1"># Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration</span>
            <span class="c1"># (the clone itself is always small)</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span>
                <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
            <span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>

            <span class="n">next_token_scores_processed</span> <span class="o">=</span> <span class="n">logits_processor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_scores</span><span class="p">)</span>

            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores_processed</span> <span class="o">+</span> <span class="n">beam_scores</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span>
                <span class="n">next_token_scores_processed</span>
            <span class="p">)</span>

            <span class="n">scores_for_all_vocab</span> <span class="o">=</span> <span class="n">next_token_scores</span>

            <span class="c1"># Store scores, attentions and hidden_states when required</span>
            <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="n">scores</span> <span class="o">+=</span> <span class="p">(</span><span class="n">next_token_scores</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_logits</span><span class="p">:</span>
                    <span class="n">raw_logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">next_token_logits</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                    <span class="n">decoder_attentions</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,)</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                        <span class="n">cross_attentions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,)</span>

                <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                    <span class="n">decoder_hidden_states</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,)</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
                        <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,)</span>
                    <span class="p">)</span>

            <span class="c1"># reshape for beam search</span>
            <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">next_token_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">vocab_size</span><span class="p">)</span>

            <span class="c1"># Sample 1 + len(eos_token_id) next tokens for each beam so we have at least 1 non eos token per beam.</span>
            <span class="n">n_eos_tokens</span> <span class="o">=</span> <span class="n">eos_token_id</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="n">next_token_scores</span><span class="p">,</span> <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span>
                <span class="n">next_token_scores</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">n_eos_tokens</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">largest</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>

            <span class="n">next_indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_tokens</span> <span class="o">/</span> <span class="n">vocab_size</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
            <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">next_tokens</span> <span class="o">%</span> <span class="n">vocab_size</span>

            <span class="c1"># stateless</span>
            <span class="n">beam_outputs</span> <span class="o">=</span> <span class="n">constrained_beam_scorer</span><span class="o">.</span><span class="n">process</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">next_token_scores</span><span class="p">,</span>
                <span class="n">next_tokens</span><span class="p">,</span>
                <span class="n">next_indices</span><span class="p">,</span>
                <span class="n">scores_for_all_vocab</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
                <span class="n">beam_indices</span><span class="o">=</span><span class="n">beam_indices</span><span class="p">,</span>
                <span class="n">decoder_prompt_len</span><span class="o">=</span><span class="n">decoder_prompt_len</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_scores&quot;</span><span class="p">]</span>
            <span class="n">beam_next_tokens</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_tokens&quot;</span><span class="p">]</span>
            <span class="n">beam_idx</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_indices&quot;</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
                <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">ops</span><span class="o">.</span><span class="n">getitem</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="p">(</span><span class="n">beam_idx</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))),</span> <span class="n">beam_next_tokens</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">[</span><span class="n">beam_idx</span><span class="p">,</span> <span class="p">:],</span> <span class="n">beam_next_tokens</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span>
                <span class="n">outputs</span><span class="p">,</span>
                <span class="n">model_kwargs</span><span class="p">,</span>
                <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># This is needed to properly delete outputs.logits which may be very large for first iteration</span>
            <span class="c1"># Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration</span>
            <span class="c1"># IMPORTANT: Note that this should appear BEFORE the call to _reorder_cache() to save the maximum memory</span>
            <span class="c1"># (that way the memory peak does not include outputs.logits)</span>
            <span class="k">del</span> <span class="n">outputs</span>

            <span class="k">if</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_temporary_reorder_cache</span><span class="p">(</span>
                    <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">],</span> <span class="n">beam_idx</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">:</span>
                <span class="n">beam_indices</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">((</span><span class="n">beam_indices</span><span class="p">[</span><span class="n">beam_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+</span> <span class="p">(</span><span class="n">beam_idx</span><span class="p">[</span><span class="n">i</span><span class="p">],)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">beam_indices</span><span class="p">))))</span>

            <span class="c1"># increase cur_len</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">constrained_beam_scorer</span><span class="o">.</span><span class="n">is_done</span> <span class="ow">or</span> <span class="nb">all</span><span class="p">(</span><span class="n">stopping_criteria</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">)):</span>
                <span class="n">this_peer_finished</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">sequence_outputs</span> <span class="o">=</span> <span class="n">constrained_beam_scorer</span><span class="o">.</span><span class="n">finalize</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">beam_scores</span><span class="p">,</span>
            <span class="n">next_tokens</span><span class="p">,</span>
            <span class="n">next_indices</span><span class="p">,</span>
            <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
            <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">stopping_criteria</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">beam_indices</span><span class="o">=</span><span class="n">beam_indices</span><span class="p">,</span>
            <span class="n">decoder_prompt_len</span><span class="o">=</span><span class="n">decoder_prompt_len</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">output_scores</span><span class="p">:</span>
                <span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">GenerateBeamEncoderDecoderOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">],</span>
                    <span class="n">sequences_scores</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">],</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">logits</span><span class="o">=</span><span class="n">raw_logits</span><span class="p">,</span>
                    <span class="n">beam_indices</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;beam_indices&quot;</span><span class="p">],</span>
                    <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_attentions</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">cross_attentions</span><span class="o">=</span><span class="n">cross_attentions</span><span class="p">,</span>
                    <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                    <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">GenerateBeamDecoderOnlyOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">],</span>
                    <span class="n">sequences_scores</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">],</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">logits</span><span class="o">=</span><span class="n">raw_logits</span><span class="p">,</span>
                    <span class="n">beam_indices</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;beam_indices&quot;</span><span class="p">],</span>
                    <span class="n">attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                    <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">),</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_assisted_decoding</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">candidate_generator</span><span class="p">:</span> <span class="n">CandidateGenerator</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">LogitsProcessorList</span><span class="p">,</span>
        <span class="n">logits_warper</span><span class="p">:</span> <span class="n">LogitsProcessorList</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="p">:</span> <span class="n">StoppingCriteriaList</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">streamer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;BaseStreamer&quot;</span><span class="p">],</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">GenerateNonBeamOutput</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences of token ids for models with a language modeling head using **greedy decoding** or</span>
<span class="sd">        **sample** (depending on `do_sample`), assisted by candidate sequences. Assisted generation is an example of a</span>
<span class="sd">        candidate decoding strategy. Can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text</span>
<span class="sd">        models.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`):</span>
<span class="sd">                The sequence used as a prompt for the generation.</span>
<span class="sd">            candidate_generator (`CandidateGenerator`):</span>
<span class="sd">                A derived instance of [`CandidateGenerator`] that defines how candidate sequences are generated. For</span>
<span class="sd">                more information, the documentation of [`CandidateGenerator`] should be read.</span>
<span class="sd">            logits_processor (`LogitsProcessorList`):</span>
<span class="sd">                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]</span>
<span class="sd">                used to modify the prediction scores of the language modeling head applied at each generation step.</span>
<span class="sd">            logits_warper (`LogitsProcessorList`):</span>
<span class="sd">                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used</span>
<span class="sd">                to warp the prediction score distribution of the language modeling head applied before multinomial</span>
<span class="sd">                sampling at each generation step. Only used if sampling is active.</span>
<span class="sd">            stopping_criteria (`StoppingCriteriaList`):</span>
<span class="sd">                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]</span>
<span class="sd">                used to tell if the generation loop should stop.</span>
<span class="sd">            generation_config ([`~generation.GenerationConfig`]):</span>
<span class="sd">                The generation configuration to be used as parametrization of the decoding method.</span>
<span class="sd">            synced_gpus (`bool`):</span>
<span class="sd">                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</span>
<span class="sd">            streamer (`BaseStreamer`, *optional*):</span>
<span class="sd">                Streamer object that will be used to stream the generated sequences. Generated tokens are passed</span>
<span class="sd">                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.</span>
<span class="sd">            model_kwargs:</span>
<span class="sd">                Additional model specific keyword arguments will be forwarded to the `forward` function of the model.</span>
<span class="sd">                If model is an encoder-decoder model the kwargs should include `encoder_outputs`.</span>

<span class="sd">        Return:</span>
<span class="sd">            [`~generation.GenerateDecoderOnlyOutput`], [`~generation.GenerateEncoderDecoderOutput`] or</span>
<span class="sd">            `mindspore.Tensor`: A `mindspore.Tensor` containing the generated tokens (default behaviour) or a</span>
<span class="sd">            [`~generation.GenerateDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and</span>
<span class="sd">            `return_dict_in_generate=True` or a [`~generation.GenerateEncoderDecoderOutput`] if</span>
<span class="sd">            `model.config.is_encoder_decoder=True`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># init values</span>
        <span class="n">do_sample</span> <span class="o">=</span> <span class="n">logits_warper</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="n">output_scores</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_scores</span>
        <span class="n">output_logits</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_logits</span>
        <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">return_dict_in_generate</span>

        <span class="c1"># init attention / hidden states / scores tuples</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">raw_logits</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_logits</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">cross_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_hidden_states</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># if model is an encoder-decoder, retrieve encoder attention weights and hidden states</span>
        <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">encoder_attentions</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attentions&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

        <span class="c1"># keep track of which sequences are already finished</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">unfinished_sequences</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_initial_cache_position</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>

        <span class="c1"># This is needed if return_dict_in_generate is True</span>
        <span class="n">start_from_empty_dynamic_cache</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">DynamicCache</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">EncoderDecoderCache</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">past_key_values</span><span class="o">.</span><span class="n">self_attention_cache</span><span class="p">,</span> <span class="n">DynamicCache</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">start_from_empty_dynamic_cache</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">this_peer_finished</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_unfinished_sequences</span><span class="p">(</span><span class="n">this_peer_finished</span><span class="p">,</span> <span class="n">synced_gpus</span><span class="p">):</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="c1">#  1. Fetch candidate sequences from a `CandidateGenerator`</span>
            <span class="n">candidate_input_ids</span><span class="p">,</span> <span class="n">candidate_logits</span> <span class="o">=</span> <span class="n">candidate_generator</span><span class="o">.</span><span class="n">get_candidates</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

            <span class="n">candidate_length</span> <span class="o">=</span> <span class="n">candidate_input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">is_done_candidate</span> <span class="o">=</span> <span class="n">stopping_criteria</span><span class="p">(</span><span class="n">candidate_input_ids</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

            <span class="c1"># 2. Use the original model to obtain the next token logits given the candidate sequence. We obtain</span>
            <span class="c1"># `candidate_length + 1` relevant logits from this process: in the event that all candidates are correct,</span>
            <span class="c1"># we use this forward pass to also pick the subsequent logits in the original model.</span>

            <span class="c1"># 2.1. Prepare the model inputs</span>
            <span class="n">candidate_kwargs</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">)</span>
            <span class="n">candidate_kwargs</span> <span class="o">=</span> <span class="n">_prepare_attention_mask</span><span class="p">(</span>
                <span class="n">candidate_kwargs</span><span class="p">,</span> <span class="n">candidate_input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
            <span class="p">)</span>
            <span class="n">candidate_kwargs</span> <span class="o">=</span> <span class="n">_prepare_token_type_ids</span><span class="p">(</span><span class="n">candidate_kwargs</span><span class="p">,</span> <span class="n">candidate_input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">if</span> <span class="s2">&quot;cache_position&quot;</span> <span class="ow">in</span> <span class="n">candidate_kwargs</span><span class="p">:</span>
                <span class="n">candidate_kwargs</span><span class="p">[</span><span class="s2">&quot;cache_position&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                    <span class="p">(</span>
                        <span class="n">candidate_kwargs</span><span class="p">[</span><span class="s2">&quot;cache_position&quot;</span><span class="p">],</span>
                        <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">cur_len</span><span class="p">,</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="n">candidate_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">candidate_kwargs</span><span class="p">[</span><span class="s2">&quot;cache_position&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
                    <span class="p">),</span>
                    <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">candidate_input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">candidate_kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;num_logits_to_keep&quot;</span> <span class="ow">in</span> <span class="n">model_inputs</span><span class="p">:</span>
                <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;num_logits_to_keep&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">candidate_length</span> <span class="o">+</span> <span class="mi">1</span>

            <span class="c1"># 2.2. Run a forward pass on the candidate sequence</span>
            <span class="c1"># prepare variable output controls (note: some models won&#39;t accept all output controls)</span>
            <span class="n">model_inputs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;output_attentions&quot;</span><span class="p">:</span> <span class="n">output_attentions</span><span class="p">}</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="p">{})</span>
            <span class="n">model_inputs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">:</span> <span class="n">output_hidden_states</span><span class="p">}</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="p">{})</span>

            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>

            <span class="c1"># 2.3. Process the new logits</span>
            <span class="n">new_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="n">candidate_length</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">:]</span>  <span class="c1"># excludes the input prompt if present</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">new_logits</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits_processor</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">candidate_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="n">new_logits</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">logits_processor</span><span class="p">(</span><span class="n">candidate_input_ids</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="n">i</span><span class="p">],</span> <span class="n">new_logits</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
            <span class="k">if</span> <span class="n">do_sample</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits_warper</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">candidate_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="n">new_logits</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">logits_warper</span><span class="p">(</span><span class="n">candidate_input_ids</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="n">i</span><span class="p">],</span> <span class="n">new_logits</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:])</span>

            <span class="c1"># 3. Select the accepted tokens. There are two possible cases:</span>
            <span class="c1"># Case 1: `do_sample=True` and we have logits for the candidates (originally from speculative decoding)</span>
            <span class="c1"># 👉 Apply algorithm 1 from the speculative decoding paper (https://arxiv.org/pdf/2211.17192.pdf).</span>
            <span class="k">if</span> <span class="n">do_sample</span> <span class="ow">and</span> <span class="n">candidate_logits</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">valid_tokens</span><span class="p">,</span> <span class="n">n_matches</span> <span class="o">=</span> <span class="n">_speculative_sampling</span><span class="p">(</span>
                    <span class="n">candidate_input_ids</span><span class="p">,</span>
                    <span class="n">candidate_logits</span><span class="p">,</span>
                    <span class="n">candidate_length</span><span class="p">,</span>
                    <span class="n">new_logits</span><span class="p">,</span>
                    <span class="n">is_done_candidate</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="c1"># Case 2: all other cases (originally from assisted generation) 👉 Compare the tokens selected from the</span>
            <span class="c1"># original model logits with the candidate tokens. We can keep the candidate tokens until the first</span>
            <span class="c1"># mismatch, or until the max length is reached.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>
                    <span class="n">probs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">new_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">selected_tokens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">selected_tokens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">new_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

                <span class="n">candidate_new_tokens</span> <span class="o">=</span> <span class="n">candidate_input_ids</span><span class="p">[:,</span> <span class="n">cur_len</span><span class="p">:]</span>
                <span class="k">if</span> <span class="mi">0</span> <span class="ow">in</span> <span class="n">candidate_new_tokens</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                    <span class="n">n_matches</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">n_matches</span> <span class="o">=</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">cumsum</span><span class="p">((</span><span class="o">~</span><span class="p">(</span><span class="n">candidate_new_tokens</span> <span class="o">==</span> <span class="n">selected_tokens</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="c1"># Ensure we don&#39;t generate beyond max_len or an EOS token</span>
                <span class="k">if</span> <span class="n">is_done_candidate</span> <span class="ow">and</span> <span class="n">n_matches</span> <span class="o">==</span> <span class="n">candidate_length</span><span class="p">:</span>
                    <span class="n">n_matches</span> <span class="o">-=</span> <span class="mi">1</span>
                <span class="n">valid_tokens</span> <span class="o">=</span> <span class="n">selected_tokens</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">n_matches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

            <span class="c1"># 4. Update variables according to the number of matching assistant tokens. Remember: the token generated</span>
            <span class="c1"># by the model after the last candidate match is also valid, as it is generated from a correct sequence.</span>
            <span class="c1"># Because of this last token, assisted generation search reduces to a normal greedy search/sample if there</span>
            <span class="c1"># is no match.</span>

            <span class="c1"># 4.1. Get the valid continuation, after the matching tokens</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">valid_tokens</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">streamer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">streamer</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">valid_tokens</span><span class="p">)</span>
            <span class="n">new_cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="c1"># 4.2. Discard past key values relative to unused assistant tokens</span>
            <span class="n">new_cache_size</span> <span class="o">=</span> <span class="n">new_cur_len</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="n">outputs</span><span class="o">.</span><span class="n">past_key_values</span> <span class="o">=</span> <span class="n">_crop_past_key_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">new_cache_size</span><span class="p">)</span>

            <span class="c1"># 5. Update the candidate generation strategy if needed</span>
            <span class="n">candidate_generator</span><span class="o">.</span><span class="n">update_candidate_strategy</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">new_logits</span><span class="p">,</span> <span class="n">n_matches</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">synced_gpus</span> <span class="ow">and</span> <span class="n">this_peer_finished</span><span class="p">:</span>
                <span class="k">continue</span>  <span class="c1"># don&#39;t waste resources running the code we don&#39;t need</span>

            <span class="c1"># Store scores, attentions and hidden_states when required</span>
            <span class="c1"># Assistant: modified to append one tuple element per token, as in the other generation methods.</span>
            <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="n">scores</span> <span class="o">+=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_logits</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_matches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">output_logits</span><span class="p">:</span>
                    <span class="n">raw_logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">next_token_logits</span><span class="p">,)</span>

                <span class="k">if</span> <span class="s2">&quot;past_key_values&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_kwargs</span> <span class="ow">or</span> <span class="n">start_from_empty_dynamic_cache</span><span class="p">:</span>
                    <span class="n">added_len</span> <span class="o">=</span> <span class="n">new_cur_len</span>
                    <span class="c1"># set it to false for other iterations</span>
                    <span class="n">start_from_empty_dynamic_cache</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">added_len</span> <span class="o">=</span> <span class="n">n_matches</span> <span class="o">+</span> <span class="mi">1</span>

                <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                        <span class="n">cross_attentions</span> <span class="o">=</span> <span class="n">_split_model_outputs</span><span class="p">(</span>
                            <span class="n">cross_attentions</span><span class="p">,</span> <span class="n">outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">,</span> <span class="n">added_len</span>
                        <span class="p">)</span>
                        <span class="n">decoder_attentions</span> <span class="o">=</span> <span class="n">_split_model_outputs</span><span class="p">(</span>
                            <span class="n">decoder_attentions</span><span class="p">,</span>
                            <span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,</span>
                            <span class="n">cur_len</span><span class="p">,</span>
                            <span class="n">added_len</span><span class="p">,</span>
                            <span class="n">is_decoder_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">decoder_attentions</span> <span class="o">=</span> <span class="n">_split_model_outputs</span><span class="p">(</span>
                            <span class="n">decoder_attentions</span><span class="p">,</span>
                            <span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
                            <span class="n">cur_len</span><span class="p">,</span>
                            <span class="n">added_len</span><span class="p">,</span>
                            <span class="n">is_decoder_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="p">)</span>
                <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                        <span class="n">decoder_hidden_states</span> <span class="o">=</span> <span class="n">_split_model_outputs</span><span class="p">(</span>
                            <span class="n">decoder_hidden_states</span><span class="p">,</span> <span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">,</span> <span class="n">added_len</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">decoder_hidden_states</span> <span class="o">=</span> <span class="n">_split_model_outputs</span><span class="p">(</span>
                            <span class="n">decoder_hidden_states</span><span class="p">,</span> <span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">,</span> <span class="n">added_len</span>
                        <span class="p">)</span>

            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span>
                <span class="n">outputs</span><span class="p">,</span>
                <span class="n">model_kwargs</span><span class="p">,</span>
                <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
                <span class="n">num_new_tokens</span><span class="o">=</span><span class="n">n_matches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">unfinished_sequences</span> <span class="o">=</span> <span class="n">unfinished_sequences</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">stopping_criteria</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
            <span class="n">this_peer_finished</span> <span class="o">=</span> <span class="n">unfinished_sequences</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">streamer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">streamer</span><span class="o">.</span><span class="n">end</span><span class="p">()</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="n">candidate_generator</span><span class="p">,</span> <span class="s2">&quot;assistant_model&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">candidate_generator</span><span class="o">.</span><span class="n">assistant_model</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_assistant_tokens_schedule</span> <span class="o">==</span> <span class="s2">&quot;heuristic&quot;</span>
        <span class="p">):</span>
            <span class="n">candidate_generator</span><span class="o">.</span><span class="n">assistant_model</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_assistant_tokens</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">candidate_generator</span><span class="o">.</span><span class="n">num_assistant_tokens</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">GenerateEncoderDecoderOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">logits</span><span class="o">=</span><span class="n">raw_logits</span><span class="p">,</span>
                    <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_attentions</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">cross_attentions</span><span class="o">=</span><span class="n">cross_attentions</span><span class="p">,</span>
                    <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                    <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">GenerateDecoderOnlyOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">logits</span><span class="o">=</span><span class="n">raw_logits</span><span class="p">,</span>
                    <span class="n">attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                    <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">),</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">input_ids</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.generation.utils.GenerationMixin.compute_transition_scores" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">generation</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">GenerationMixin</span><span class="o">.</span><span class="n">compute_transition_scores</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">beam_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">normalize_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.generation.utils.GenerationMixin.compute_transition_scores" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Computes the transition scores of sequences given the generation scores (and beam indices, if beam search was
used). This is a convenient method to quicky obtain the scores of the selected tokens at generation time.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sequences</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generated sequences. The second dimension (sequence_length) is either equal to <code>max_length</code> or
shorter if all batches finished early due to the <code>eos_token_id</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scores</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Transition scores for each vocabulary token at each generation step. Beam transition scores consisting
of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.
Tuple of <code>mindspore.Tensor</code> with up to <code>max_new_tokens</code> elements (one element for each generated token),
with each tensor of shape <code>(batch_size*num_beams, config.vocab_size)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(mindspore.Tensor)`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>beam_indices</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Beam indices of generated token id at each generation step. <code>mindspore.Tensor</code> of shape
<code>(batch_size*num_return_sequences, sequence_length)</code>. Only required if a <code>num_beams&gt;1</code> at
generate-time.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>normalize_logits</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to normalize the logits (which, for legacy reasons, may be unnormalized).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="return" open>
  <summary>Return</summary>
  <p><code>mindspore.Tensor</code>: A <code>mindspore.Tensor</code> of shape <code>(batch_size*num_return_sequences, sequence_length)</code> containing
    the transition scores (logits)</p>
</details>        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Today is&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Example 1: Print the scores for each token generated with Greedy Search</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">transition_scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compute_transition_scores</span><span class="p">(</span>
<span class="o">...</span>     <span class="n">outputs</span><span class="o">.</span><span class="n">sequences</span><span class="p">,</span> <span class="n">outputs</span><span class="o">.</span><span class="n">scores</span><span class="p">,</span> <span class="n">normalize_logits</span><span class="o">=</span><span class="kc">True</span>
<span class="o">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># encoder-decoder models, like BART or T5.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">input_length</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">generated_tokens</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">sequences</span><span class="p">[:,</span> <span class="n">input_length</span><span class="p">:]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">tok</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">generated_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">transition_scores</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
<span class="o">...</span>     <span class="c1"># | token | token string | log probability | probability</span>
<span class="o">...</span>     <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;| </span><span class="si">{</span><span class="n">tok</span><span class="si">:</span><span class="s2">5d</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tok</span><span class="p">)</span><span class="si">:</span><span class="s2">8s</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">score</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">score</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="o">|</span>   <span class="mi">262</span> <span class="o">|</span>  <span class="n">the</span>     <span class="o">|</span> <span class="o">-</span><span class="mf">1.414</span> <span class="o">|</span> <span class="mf">24.33</span><span class="o">%</span>
<span class="o">|</span>  <span class="mi">1110</span> <span class="o">|</span>  <span class="n">day</span>     <span class="o">|</span> <span class="o">-</span><span class="mf">2.609</span> <span class="o">|</span> <span class="mf">7.36</span><span class="o">%</span>
<span class="o">|</span>   <span class="mi">618</span> <span class="o">|</span>  <span class="n">when</span>    <span class="o">|</span> <span class="o">-</span><span class="mf">2.010</span> <span class="o">|</span> <span class="mf">13.40</span><span class="o">%</span>
<span class="o">|</span>   <span class="mi">356</span> <span class="o">|</span>  <span class="n">we</span>      <span class="o">|</span> <span class="o">-</span><span class="mf">1.859</span> <span class="o">|</span> <span class="mf">15.58</span><span class="o">%</span>
<span class="o">|</span>   <span class="mi">460</span> <span class="o">|</span>  <span class="n">can</span>     <span class="o">|</span> <span class="o">-</span><span class="mf">2.508</span> <span class="o">|</span> <span class="mf">8.14</span><span class="o">%</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Example 2: Reconstruct the sequence scores from Beam Search</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="o">...</span>     <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">num_beams</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">output_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="o">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">transition_scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compute_transition_scores</span><span class="p">(</span>
<span class="o">...</span>     <span class="n">outputs</span><span class="o">.</span><span class="n">sequences</span><span class="p">,</span> <span class="n">outputs</span><span class="o">.</span><span class="n">scores</span><span class="p">,</span> <span class="n">outputs</span><span class="o">.</span><span class="n">beam_indices</span><span class="p">,</span> <span class="n">normalize_logits</span><span class="o">=</span><span class="kc">False</span>
<span class="o">...</span> <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># If you sum the generated tokens&#39; scores and apply the length penalty, you&#39;ll get the sequence scores.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Tip 1: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># use case, you might want to recompute it with `normalize_logits=True`.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Tip 2: the output length does NOT include the input length</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">output_length</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">transition_scores</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">length_penalty</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">length_penalty</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">reconstructed_scores</span> <span class="o">=</span> <span class="n">transition_scores</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">output_length</span><span class="o">**</span><span class="n">length_penalty</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">sequences_scores</span><span class="p">,</span> <span class="n">reconstructed_scores</span><span class="p">))</span>
<span class="kc">True</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\generation\utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">compute_transition_scores</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sequences</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">scores</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">beam_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">normalize_logits</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the transition scores of sequences given the generation scores (and beam indices, if beam search was</span>
<span class="sd">    used). This is a convenient method to quicky obtain the scores of the selected tokens at generation time.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        sequences (`mindspore.Tensor`):</span>
<span class="sd">            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or</span>
<span class="sd">            shorter if all batches finished early due to the `eos_token_id`.</span>
<span class="sd">        scores (`tuple(mindspore.Tensor)`):</span>
<span class="sd">            Transition scores for each vocabulary token at each generation step. Beam transition scores consisting</span>
<span class="sd">            of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.</span>
<span class="sd">            Tuple of `mindspore.Tensor` with up to `max_new_tokens` elements (one element for each generated token),</span>
<span class="sd">            with each tensor of shape `(batch_size*num_beams, config.vocab_size)`.</span>
<span class="sd">        beam_indices (`mindspore.Tensor`, *optional*):</span>
<span class="sd">            Beam indices of generated token id at each generation step. `mindspore.Tensor` of shape</span>
<span class="sd">            `(batch_size*num_return_sequences, sequence_length)`. Only required if a `num_beams&gt;1` at</span>
<span class="sd">            generate-time.</span>
<span class="sd">        normalize_logits (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to normalize the logits (which, for legacy reasons, may be unnormalized).</span>

<span class="sd">    Return:</span>
<span class="sd">        `mindspore.Tensor`: A `mindspore.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)` containing</span>
<span class="sd">            the transition scores (logits)</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import GPT2Tokenizer, AutoModelForCausalLM</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>

<span class="sd">    &gt;&gt;&gt; tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;openai-community/gpt2&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer.pad_token_id = tokenizer.eos_token_id</span>
<span class="sd">    &gt;&gt;&gt; inputs = tokenizer([&quot;Today is&quot;], return_tensors=&quot;ms&quot;)</span>

<span class="sd">    &gt;&gt;&gt; # Example 1: Print the scores for each token generated with Greedy Search</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)</span>
<span class="sd">    &gt;&gt;&gt; transition_scores = model.compute_transition_scores(</span>
<span class="sd">    ...     outputs.sequences, outputs.scores, normalize_logits=True</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; # input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for</span>
<span class="sd">    &gt;&gt;&gt; # encoder-decoder models, like BART or T5.</span>
<span class="sd">    &gt;&gt;&gt; input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]</span>
<span class="sd">    &gt;&gt;&gt; generated_tokens = outputs.sequences[:, input_length:]</span>
<span class="sd">    &gt;&gt;&gt; for tok, score in zip(generated_tokens[0], transition_scores[0]):</span>
<span class="sd">    ...     # | token | token string | log probability | probability</span>
<span class="sd">    ...     print(f&quot;| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}&quot;)</span>
<span class="sd">    |   262 |  the     | -1.414 | 24.33%</span>
<span class="sd">    |  1110 |  day     | -2.609 | 7.36%</span>
<span class="sd">    |   618 |  when    | -2.010 | 13.40%</span>
<span class="sd">    |   356 |  we      | -1.859 | 15.58%</span>
<span class="sd">    |   460 |  can     | -2.508 | 8.14%</span>

<span class="sd">    &gt;&gt;&gt; # Example 2: Reconstruct the sequence scores from Beam Search</span>
<span class="sd">    &gt;&gt;&gt; outputs = model.generate(</span>
<span class="sd">    ...     **inputs,</span>
<span class="sd">    ...     max_new_tokens=5,</span>
<span class="sd">    ...     num_beams=4,</span>
<span class="sd">    ...     num_return_sequences=4,</span>
<span class="sd">    ...     return_dict_in_generate=True,</span>
<span class="sd">    ...     output_scores=True,</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; transition_scores = model.compute_transition_scores(</span>
<span class="sd">    ...     outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; # If you sum the generated tokens&#39; scores and apply the length penalty, you&#39;ll get the sequence scores.</span>
<span class="sd">    &gt;&gt;&gt; # Tip 1: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the</span>
<span class="sd">    &gt;&gt;&gt; # use case, you might want to recompute it with `normalize_logits=True`.</span>
<span class="sd">    &gt;&gt;&gt; # Tip 2: the output length does NOT include the input length</span>
<span class="sd">    &gt;&gt;&gt; output_length = np.sum(transition_scores.numpy() &lt; 0, axis=1)</span>
<span class="sd">    &gt;&gt;&gt; length_penalty = model.generation_config.length_penalty</span>
<span class="sd">    &gt;&gt;&gt; reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)</span>
<span class="sd">    &gt;&gt;&gt; print(np.allclose(outputs.sequences_scores, reconstructed_scores))</span>
<span class="sd">    True</span>
<span class="sd">    ```&quot;&quot;&quot;</span>
    <span class="c1"># 1. In absence of `beam_indices`, we can assume that we come from e.g. greedy search, which is equivalent</span>
    <span class="c1"># to a beam search approach were the first (and only) beam is always selected</span>
    <span class="k">if</span> <span class="n">beam_indices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">beam_indices</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">beam_indices</span> <span class="o">=</span> <span class="n">beam_indices</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>

    <span class="c1"># 2. reshape scores as [batch_size*vocab_size, # generation steps] with # generation steps being</span>
    <span class="c1"># seq_len - input_length</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 3. Optionally normalize the logits (across the vocab dimension)</span>
    <span class="k">if</span> <span class="n">normalize_logits</span><span class="p">:</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># 4. cut beam_indices to longest beam length</span>
    <span class="n">beam_indices_mask</span> <span class="o">=</span> <span class="n">beam_indices</span> <span class="o">&lt;</span> <span class="mi">0</span>
    <span class="n">max_beam_length</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beam_indices_mask</span><span class="o">.</span><span class="n">long</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">beam_indices</span> <span class="o">=</span> <span class="n">beam_indices</span><span class="p">[:,</span> <span class="p">:</span><span class="n">max_beam_length</span><span class="p">]</span>
    <span class="n">beam_indices_mask</span> <span class="o">=</span> <span class="n">beam_indices_mask</span><span class="p">[:,</span> <span class="p">:</span><span class="n">max_beam_length</span><span class="p">]</span>

    <span class="c1"># 5. Set indices of beams that finished early to 0; such indices will be masked correctly afterwards</span>
    <span class="n">beam_indices</span><span class="p">[</span><span class="n">beam_indices_mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># 6. multiply beam_indices with vocab size to gather correctly from scores</span>
    <span class="n">beam_sequence_indices</span> <span class="o">=</span> <span class="n">beam_indices</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>

    <span class="c1"># 7. Define which indices contributed to scores</span>
    <span class="n">cut_idx</span> <span class="o">=</span> <span class="n">sequences</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">max_beam_length</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:,</span> <span class="n">cut_idx</span><span class="p">:]</span> <span class="o">+</span> <span class="n">beam_sequence_indices</span>

    <span class="c1"># 8. Compute scores</span>
    <span class="n">transition_scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>

    <span class="c1"># 9. Mask out transition_scores of beams that stopped early</span>
    <span class="n">transition_scores</span><span class="p">[</span><span class="n">beam_indices_mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">return</span> <span class="n">transition_scores</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.generation.utils.GenerationMixin.generate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">generation</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">GenerationMixin</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stopping_criteria</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">synced_gpus</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">negative_prompt_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">negative_prompt_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.generation.utils.GenerationMixin.generate" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Generates sequences of token ids for models with a language modeling head.</p>
<p><Tip warning={true}></p>
<p>Most generation-controlling parameters are set in <code>generation_config</code> which, if not passed, will be set to the
model's default generation configuration. You can override any <code>generation_config</code> by passing the corresponding
parameters to generate(), e.g. <code>.generate(inputs, num_beams=4, do_sample=True)</code>.</p>
<p>For an overview of generation strategies and code examples, check out the <a href="../generation_strategies">following
guide</a>.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>inputs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sequence used as a prompt for the generation or as model inputs to the encoder. If <code>None</code> the
method initializes it with <code>bos_token_id</code> and a batch size of 1. For decoder-only models <code>inputs</code>
should be in the format of <code>input_ids</code>. For encoder-decoder models <em>inputs</em> can represent any of
<code>input_ids</code>, <code>input_values</code>, <code>input_features</code>, or <code>pixel_values</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of varying shape depending on the modality, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>generation_config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generation configuration to be used as base parametrization for the generation call. <code>**kwargs</code>
passed to generate matching the attributes of <code>generation_config</code> will override them. If
<code>generation_config</code> is not provided, the default will be used, which has the following loading
priority: 1) from the <code>generation_config.json</code> model file, if it exists; 2) from the model
configuration. Please note that unspecified parameters will inherit [<code>~generation.GenerationConfig</code>]'s
default values, whose documentation should be checked to parameterize generation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>[`~generation.GenerationConfig`], *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logits_processor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Custom logits processors that complement the default logits processors built from arguments and
generation config. If a logit processor is passed that is already created with the arguments or a
generation config an error is thrown. This feature is intended for advanced users.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`LogitsProcessorList`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stopping_criteria</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Custom stopping criteria that complements the default stopping criteria built from arguments and a
generation config. If a stopping criteria is passed that is already created with the arguments or a
generation config an error is thrown. If your stopping criteria depends on the <code>scores</code> input, make
sure you pass <code>return_dict_in_generate=True, output_scores=True</code> to <code>generate</code>. This feature is
intended for advanced users.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`StoppingCriteriaList`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prefix_allowed_tokens_fn</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If provided, this function constraints the beam search to allowed tokens only at each step. If not
provided no constraint is applied. This function takes 2 arguments: the batch ID <code>batch_id</code> and
<code>input_ids</code>. It has to return a list with the allowed tokens for the next generation step conditioned
on the batch ID <code>batch_id</code> and the previously generated tokens <code>inputs_ids</code>. This argument is useful
for constrained generation conditioned on the prefix, as described in <a href="https://arxiv.org/abs/2010.00904">Autoregressive Entity
Retrieval</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable[[int, mindspore.Tensor], List[int]]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>synced_gpus</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to continue running the while loop until max_length. Unless overridden this flag will be set to
<code>True</code> under DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished
generating before other GPUs. Otherwise it'll be set to <code>False</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>assistant_model</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An assistant model that can be used to accelerate generation. The assistant model must have the exact
same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model
is much faster than running generation with the model you're calling generate from. As such, the
assistant model should be much smaller.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`PreTrainedModel`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>streamer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Streamer object that will be used to stream the generated sequences. Generated tokens are passed
through <code>streamer.put(token_ids)</code> and the streamer is responsible for any further processing.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`BaseStreamer`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>negative_prompt_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The negative prompt needed for some processors such as CFG. The batch size must match the input batch
size. This is an experimental feature, subject to breaking API changes in future versions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>negative_prompt_attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Attention_mask for <code>negative_prompt_ids</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Ad hoc parametrization of <code>generation_config</code> and/or additional model-specific kwargs that will be
forwarded to the <code>forward</code> function of the model. If the model is an encoder-decoder model, encoder
specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with <em>decoder_</em>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, Any]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="return" open>
  <summary>Return</summary>
  <p>[<code>~utils.ModelOutput</code>] or <code>mindspore.Tensor</code>: A [<code>~utils.ModelOutput</code>] (if <code>return_dict_in_generate=True</code>
or when <code>config.return_dict_in_generate=True</code>) or a <code>mindspore.Tensor</code>.</p>
<div class="highlight"><pre><span></span><code>If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible
[`~utils.ModelOutput`] types are:

    - [`~generation.GenerateDecoderOnlyOutput`],
    - [`~generation.GenerateBeamDecoderOnlyOutput`]

If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible
[`~utils.ModelOutput`] types are:

    - [`~generation.GenerateEncoderDecoderOutput`],
    - [`~generation.GenerateBeamEncoderDecoderOutput`]
</code></pre></div>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\generation\utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span>
<span class="normal">2105</span>
<span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span>
<span class="normal">2109</span>
<span class="normal">2110</span>
<span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerationConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stopping_criteria</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">StoppingCriteriaList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prefix_allowed_tokens_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">synced_gpus</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">assistant_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;PreTrainedModel&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">streamer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;BaseStreamer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">negative_prompt_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">negative_prompt_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">GenerateOutput</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Generates sequences of token ids for models with a language modeling head.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the</span>
<span class="sd">    model&#39;s default generation configuration. You can override any `generation_config` by passing the corresponding</span>
<span class="sd">    parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.</span>

<span class="sd">    For an overview of generation strategies and code examples, check out the [following</span>
<span class="sd">    guide](../generation_strategies).</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        inputs (`mindspore.Tensor` of varying shape depending on the modality, *optional*):</span>
<span class="sd">            The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the</span>
<span class="sd">            method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`</span>
<span class="sd">            should be in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of</span>
<span class="sd">            `input_ids`, `input_values`, `input_features`, or `pixel_values`.</span>
<span class="sd">        generation_config ([`~generation.GenerationConfig`], *optional*):</span>
<span class="sd">            The generation configuration to be used as base parametrization for the generation call. `**kwargs`</span>
<span class="sd">            passed to generate matching the attributes of `generation_config` will override them. If</span>
<span class="sd">            `generation_config` is not provided, the default will be used, which has the following loading</span>
<span class="sd">            priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model</span>
<span class="sd">            configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]&#39;s</span>
<span class="sd">            default values, whose documentation should be checked to parameterize generation.</span>
<span class="sd">        logits_processor (`LogitsProcessorList`, *optional*):</span>
<span class="sd">            Custom logits processors that complement the default logits processors built from arguments and</span>
<span class="sd">            generation config. If a logit processor is passed that is already created with the arguments or a</span>
<span class="sd">            generation config an error is thrown. This feature is intended for advanced users.</span>
<span class="sd">        stopping_criteria (`StoppingCriteriaList`, *optional*):</span>
<span class="sd">            Custom stopping criteria that complements the default stopping criteria built from arguments and a</span>
<span class="sd">            generation config. If a stopping criteria is passed that is already created with the arguments or a</span>
<span class="sd">            generation config an error is thrown. If your stopping criteria depends on the `scores` input, make</span>
<span class="sd">            sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is</span>
<span class="sd">            intended for advanced users.</span>
<span class="sd">        prefix_allowed_tokens_fn (`Callable[[int, mindspore.Tensor], List[int]]`, *optional*):</span>
<span class="sd">            If provided, this function constraints the beam search to allowed tokens only at each step. If not</span>
<span class="sd">            provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and</span>
<span class="sd">            `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned</span>
<span class="sd">            on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful</span>
<span class="sd">            for constrained generation conditioned on the prefix, as described in [Autoregressive Entity</span>
<span class="sd">            Retrieval](https://arxiv.org/abs/2010.00904).</span>
<span class="sd">        synced_gpus (`bool`, *optional*):</span>
<span class="sd">            Whether to continue running the while loop until max_length. Unless overridden this flag will be set to</span>
<span class="sd">            `True` under DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished</span>
<span class="sd">            generating before other GPUs. Otherwise it&#39;ll be set to `False`.</span>
<span class="sd">        assistant_model (`PreTrainedModel`, *optional*):</span>
<span class="sd">            An assistant model that can be used to accelerate generation. The assistant model must have the exact</span>
<span class="sd">            same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model</span>
<span class="sd">            is much faster than running generation with the model you&#39;re calling generate from. As such, the</span>
<span class="sd">            assistant model should be much smaller.</span>
<span class="sd">        streamer (`BaseStreamer`, *optional*):</span>
<span class="sd">            Streamer object that will be used to stream the generated sequences. Generated tokens are passed</span>
<span class="sd">            through `streamer.put(token_ids)` and the streamer is responsible for any further processing.</span>
<span class="sd">        negative_prompt_ids (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            The negative prompt needed for some processors such as CFG. The batch size must match the input batch</span>
<span class="sd">            size. This is an experimental feature, subject to breaking API changes in future versions.</span>
<span class="sd">        negative_prompt_attention_mask (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Attention_mask for `negative_prompt_ids`.</span>
<span class="sd">        kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">            Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be</span>
<span class="sd">            forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder</span>
<span class="sd">            specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.</span>

<span class="sd">    Return:</span>
<span class="sd">        [`~utils.ModelOutput`] or `mindspore.Tensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`</span>
<span class="sd">        or when `config.return_dict_in_generate=True`) or a `mindspore.Tensor`.</span>

<span class="sd">            If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible</span>
<span class="sd">            [`~utils.ModelOutput`] types are:</span>

<span class="sd">                - [`~generation.GenerateDecoderOnlyOutput`],</span>
<span class="sd">                - [`~generation.GenerateBeamDecoderOnlyOutput`]</span>

<span class="sd">            If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible</span>
<span class="sd">            [`~utils.ModelOutput`] types are:</span>

<span class="sd">                - [`~generation.GenerateEncoderDecoderOutput`],</span>
<span class="sd">                - [`~generation.GenerateBeamEncoderDecoderOutput`]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_run_profiler</span> <span class="o">=</span> <span class="n">parse_flag_from_env</span><span class="p">(</span><span class="s1">&#39;MS_ENABLE_RUNTIME_PROFILER&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="c1"># 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_validate_model_class</span><span class="p">()</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;tokenizer&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># Pull this out first, we only use it for stopping criteria</span>
    <span class="n">generation_config</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_generation_config</span><span class="p">(</span><span class="n">generation_config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_validate_model_kwargs</span><span class="p">(</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_validate_assistant</span><span class="p">(</span><span class="n">assistant_model</span><span class="p">)</span>

    <span class="c1"># 2. Set generation parameters if not already defined</span>
    <span class="k">if</span> <span class="n">synced_gpus</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># if is_deepspeed_zero3_enabled() and dist.get_world_size() &gt; 1:</span>
        <span class="c1">#     synced_gpus = True</span>
        <span class="c1"># else:</span>
        <span class="n">synced_gpus</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="n">logits_processor</span> <span class="o">=</span> <span class="n">logits_processor</span> <span class="k">if</span> <span class="n">logits_processor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>
    <span class="n">stopping_criteria</span> <span class="o">=</span> <span class="n">stopping_criteria</span> <span class="k">if</span> <span class="n">stopping_criteria</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">StoppingCriteriaList</span><span class="p">()</span>

    <span class="n">accepts_attention_mask</span> <span class="o">=</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">requires_attention_mask</span> <span class="o">=</span> <span class="s2">&quot;encoder_outputs&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_kwargs</span>
    <span class="n">kwargs_has_attention_mask</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="c1"># 3. Define model inputs</span>
    <span class="n">inputs_tensor</span><span class="p">,</span> <span class="n">model_input_name</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_model_inputs</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">bos_token_id</span><span class="p">,</span> <span class="n">model_kwargs</span>
    <span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">inputs_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_special_tokens</span><span class="p">(</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">kwargs_has_attention_mask</span><span class="p">)</span>

    <span class="c1"># decoder-only models must use left-padding for batched generation.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
        <span class="c1"># If `input_ids` was given, check if the last id in any sequence is `pad_token_id`</span>
        <span class="c1"># Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">generation_config</span><span class="o">.</span><span class="n">_pad_token_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">1</span>
            <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
            <span class="ow">and</span> <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">inputs_tensor</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_pad_token_tensor</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;A decoder-only architecture is being used, but right-padding was detected! For correct &quot;</span>
                <span class="s2">&quot;generation results, please set `padding_side=&#39;left&#39;` when initializing the tokenizer.&quot;</span>
            <span class="p">)</span>

    <span class="c1"># 4. Define other model kwargs</span>
    <span class="c1"># decoder-only models with inputs_embeds forwarding must use caching (otherwise we can&#39;t detect whether we are</span>
    <span class="c1"># generating the first new token or not, and we only want to use the embeddings for the first new token)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">and</span> <span class="n">model_input_name</span> <span class="o">==</span> <span class="s2">&quot;inputs_embeds&quot;</span><span class="p">:</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;use_cache&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;use_cache&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">use_cache</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">kwargs_has_attention_mask</span> <span class="ow">and</span> <span class="n">requires_attention_mask</span> <span class="ow">and</span> <span class="n">accepts_attention_mask</span><span class="p">:</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_attention_mask_for_generation</span><span class="p">(</span>
            <span class="n">inputs_tensor</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_pad_token_tensor</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">_eos_token_tensor</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">and</span> <span class="s2">&quot;encoder_outputs&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
        <span class="c1"># if model is encoder decoder encoder_outputs are created and added to `model_kwargs`</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_encoder_decoder_kwargs_for_generation</span><span class="p">(</span>
            <span class="n">inputs_tensor</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">,</span> <span class="n">model_input_name</span><span class="p">,</span> <span class="n">generation_config</span>
        <span class="p">)</span>
    <span class="c1"># 5. Prepare `input_ids` which will be used for auto-regressive generation</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
        <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_decoder_input_ids_for_generation</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">model_input_name</span><span class="o">=</span><span class="n">model_input_name</span><span class="p">,</span>
            <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">_decoder_start_token_tensor</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">inputs_tensor</span> <span class="k">if</span> <span class="n">model_input_name</span> <span class="o">==</span> <span class="s2">&quot;input_ids&quot;</span> <span class="k">else</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;input_ids&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">token_healing</span><span class="p">:</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">heal_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">streamer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">streamer</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
    <span class="c1"># 6. Prepare `max_length` depending on other stopping criteria.</span>
    <span class="n">input_ids_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">has_default_max_length</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;max_length&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">has_default_min_length</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;min_length&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">min_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">generation_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_generated_length</span><span class="p">(</span>
        <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
        <span class="n">has_default_max_length</span><span class="o">=</span><span class="n">has_default_max_length</span><span class="p">,</span>
        <span class="n">has_default_min_length</span><span class="o">=</span><span class="n">has_default_min_length</span><span class="p">,</span>
        <span class="n">model_input_name</span><span class="o">=</span><span class="n">model_input_name</span><span class="p">,</span>
        <span class="n">inputs_tensor</span><span class="o">=</span><span class="n">inputs_tensor</span><span class="p">,</span>
        <span class="n">input_ids_length</span><span class="o">=</span><span class="n">input_ids_length</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">use_dynamic_cache_by_default</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="s2">&quot;mamba&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
        <span class="n">cache_name</span> <span class="o">=</span> <span class="s2">&quot;cache_params&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cache_name</span> <span class="o">=</span> <span class="s2">&quot;past_key_values&quot;</span>

    <span class="c1"># TODO(joao): support static caches in assisted generation. assisted generation needs to roll back caches,</span>
    <span class="c1"># which is only supported in dynamic caches atm</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="n">assistant_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supports_default_dynamic_cache</span><span class="p">()</span>
    <span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
            <span class="s2">&quot;An assistant model is provided, using a dynamic cache instead of a cache of type=&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span><span class="si">}</span><span class="s2">&#39;.&quot;</span>
        <span class="p">)</span>
        <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># if (model_kwargs.get(cache_name) is not None) and is_torchdynamo_compiling():</span>
    <span class="c1">#     raise ValueError(</span>
    <span class="c1">#         &quot;Passing `past_key_values` is not supported when compiling `model.generate` with torch.compile -- you &quot;</span>
    <span class="c1">#         &quot;may get incorrect outputs. Please compile `model.forward` only or use the `cache_implementation` &quot;</span>
    <span class="c1">#         &quot;input argument.&quot;</span>
    <span class="c1">#     )</span>
    <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">cache_name</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Passing both `cache_implementation` (used to initialize certain caches) and `</span><span class="si">{</span><span class="n">cache_name</span><span class="si">}</span><span class="s2">` (a &quot;</span>
            <span class="s2">&quot;Cache object) is unsupported. Please use only one of the two.&quot;</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="ow">in</span> <span class="n">NEED_SETUP_CACHE_CLASSES_MAPPING</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="o">==</span> <span class="s2">&quot;static&quot;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supports_static_cache</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;This model does not support `cache_implementation=&#39;static&#39;`. Please check the following &quot;</span>
                    <span class="s2">&quot;issue: https://github.com/huggingface/transformers/issues/28981&quot;</span>
                <span class="p">)</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="n">cache_name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_cache</span><span class="p">(</span>
                <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span><span class="p">,</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="n">generation_config</span><span class="p">,</span> <span class="s2">&quot;num_beams&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span>
                <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="o">==</span> <span class="s2">&quot;quantized&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;MindNLP do not support quantized generation.&#39;</span><span class="p">)</span>
            <span class="c1"># if not self._supports_quantized_cache:</span>
            <span class="c1">#     raise ValueError(</span>
            <span class="c1">#         &quot;This model does not support the quantized cache. If you want your model to support quantized &quot;</span>
            <span class="c1">#         &quot;cache, please open an issue.&quot;</span>
            <span class="c1">#     )</span>

            <span class="c1"># cache_config = (</span>
            <span class="c1">#     generation_config.cache_config</span>
            <span class="c1">#     if generation_config.cache_config is not None</span>
            <span class="c1">#     else QuantizedCacheConfig()</span>
            <span class="c1"># )</span>
            <span class="c1"># cache_class = QUANT_BACKEND_CLASSES_MAPPING[cache_config.backend]</span>

            <span class="c1"># if cache_config.backend == &quot;quanto&quot; and not is_quanto_available():</span>
            <span class="c1">#     raise ImportError(</span>
            <span class="c1">#         &quot;You need to install `quanto` in order to use KV cache quantization with quanto backend. &quot;</span>
            <span class="c1">#         &quot;Please install it via  with `pip install quanto`&quot;</span>
            <span class="c1">#     )</span>
            <span class="c1"># elif cache_config.backend == &quot;HQQ&quot; and not is_hqq_available():</span>
            <span class="c1">#     raise ImportError(</span>
            <span class="c1">#         &quot;You need to install `HQQ` in order to use KV cache quantization with HQQ backend. &quot;</span>
            <span class="c1">#         &quot;Please install it via  with `pip install hqq`&quot;</span>
            <span class="c1">#     )</span>

            <span class="c1"># model_kwargs[cache_name] = cache_class(cache_config)</span>
    <span class="c1"># Use DynamicCache() instance by default. This will avoid back and forth from legacy format that</span>
    <span class="c1"># keeps copying the cache thus using much more memory</span>
    <span class="k">elif</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supports_default_dynamic_cache</span><span class="p">():</span>
        <span class="n">past</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">cache_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">requires_cross_attention_cache</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">or</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">past</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="n">cache_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">DynamicCache</span><span class="p">()</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">requires_cross_attention_cache</span>
                <span class="k">else</span> <span class="n">EncoderDecoderCache</span><span class="p">(</span><span class="n">DynamicCache</span><span class="p">(),</span> <span class="n">DynamicCache</span><span class="p">())</span>
            <span class="p">)</span>
            <span class="n">use_dynamic_cache_by_default</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">past</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="n">cache_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">DynamicCache</span><span class="o">.</span><span class="n">from_legacy_cache</span><span class="p">(</span><span class="n">past</span><span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">requires_cross_attention_cache</span>
                <span class="k">else</span> <span class="n">EncoderDecoderCache</span><span class="o">.</span><span class="n">from_legacy_cache</span><span class="p">(</span><span class="n">past</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">use_dynamic_cache_by_default</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_validate_generated_length</span><span class="p">(</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">input_ids_length</span><span class="p">,</span> <span class="n">has_default_max_length</span><span class="p">)</span>

    <span class="c1"># 7. determine generation mode</span>
    <span class="n">generation_mode</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">get_generation_mode</span><span class="p">(</span><span class="n">assistant_model</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">streamer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.&quot;</span>
        <span class="p">)</span>

    <span class="c1"># 8. prepare distribution pre_processing samplers</span>
    <span class="n">prepared_logits_processor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_logits_processor</span><span class="p">(</span>
        <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
        <span class="n">input_ids_seq_length</span><span class="o">=</span><span class="n">input_ids_length</span><span class="p">,</span>
        <span class="n">encoder_input_ids</span><span class="o">=</span><span class="n">inputs_tensor</span><span class="p">,</span>
        <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="n">prefix_allowed_tokens_fn</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span>
        <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="n">negative_prompt_ids</span><span class="o">=</span><span class="n">negative_prompt_ids</span><span class="p">,</span>
        <span class="n">negative_prompt_attention_mask</span><span class="o">=</span><span class="n">negative_prompt_attention_mask</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># 9. prepare stopping criteria</span>
    <span class="n">prepared_stopping_criteria</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_stopping_criteria</span><span class="p">(</span>
        <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">stopping_criteria</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">_run_profiler</span><span class="p">:</span>
        <span class="n">_framework_profiler_step_start</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;Enabling the profiler will generate larger files. Please set `max_length` or `max_new_tokens` to a smaller value (recommended less than 10)&#39;</span><span class="p">)</span>

    <span class="c1"># 10. go into different generation modes</span>
    <span class="k">if</span> <span class="n">generation_mode</span> <span class="o">==</span> <span class="n">GenerationMode</span><span class="o">.</span><span class="n">ASSISTED_GENERATION</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;num_return_sequences has to be 1 when doing assisted generate, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but is </span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_return_sequences</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;assisted generate is only supported for batch_size = 1&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;use_cache&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;assisted generate requires `use_cache=True`&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">cache_implementation</span> <span class="o">==</span> <span class="s2">&quot;static&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;assisted generate is not supported with `static_cache`&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_stateful</span><span class="p">:</span>
            <span class="c1"># In assisted generation we need the ability to confirm whether the model would pick certain tokens,</span>
            <span class="c1"># which is not possible with stateful models (they can&#39;t reset to a previous subset of generated text)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;assisted generation is not supported with stateful models, such as </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># 11. Get the candidate generator, given the parameterization</span>
        <span class="n">candidate_generator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_candidate_generator</span><span class="p">(</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">inputs_tensor</span><span class="o">=</span><span class="n">inputs_tensor</span><span class="p">,</span>
            <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span>
            <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># 12. prepare logits warper (if `do_sample` is `True`)</span>
        <span class="n">prepared_logits_warper</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_logits_warper</span><span class="p">(</span>
                <span class="n">generation_config</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">do_sample</span>
            <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>

        <span class="c1"># 13. run assisted generate</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_assisted_decoding</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">candidate_generator</span><span class="o">=</span><span class="n">candidate_generator</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="o">=</span><span class="n">prepared_logits_processor</span><span class="p">,</span>
            <span class="n">logits_warper</span><span class="o">=</span><span class="n">prepared_logits_warper</span><span class="p">,</span>
            <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">prepared_stopping_criteria</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">synced_gpus</span><span class="o">=</span><span class="n">synced_gpus</span><span class="p">,</span>
            <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span>
            <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">generation_mode</span> <span class="o">==</span> <span class="n">GenerationMode</span><span class="o">.</span><span class="n">DOLA_GENERATION</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_stateful</span><span class="p">:</span>
            <span class="c1"># DoLa decoding was not designed for stateful models, and would require some changes</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;dola decoding is not supported with stateful models, such as </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="n">prepared_logits_warper</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_logits_warper</span><span class="p">(</span><span class="n">generation_config</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">do_sample</span>
            <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dola_decoding</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">dola_layers</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">dola_layers</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="o">=</span><span class="n">prepared_logits_processor</span><span class="p">,</span>
            <span class="n">logits_warper</span><span class="o">=</span><span class="n">prepared_logits_warper</span><span class="p">,</span>
            <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">prepared_stopping_criteria</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">synced_gpus</span><span class="o">=</span><span class="n">synced_gpus</span><span class="p">,</span>
            <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span>
            <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="n">generation_mode</span> <span class="o">==</span> <span class="n">GenerationMode</span><span class="o">.</span><span class="n">CONTRASTIVE_SEARCH</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;use_cache&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Contrastive search requires `use_cache=True`&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_stateful</span><span class="p">:</span>
            <span class="c1"># Just like assisted generation, we need to be able to rollback to a previous state (see comment above)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;contrastive search is not supported with stateful models, such as </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_contrastive_search</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="o">=</span><span class="n">prepared_logits_processor</span><span class="p">,</span>
            <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">prepared_stopping_criteria</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">synced_gpus</span><span class="o">=</span><span class="n">synced_gpus</span><span class="p">,</span>
            <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span>
            <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="n">generation_mode</span> <span class="ow">in</span> <span class="p">(</span><span class="n">GenerationMode</span><span class="o">.</span><span class="n">SAMPLE</span><span class="p">,</span> <span class="n">GenerationMode</span><span class="o">.</span><span class="n">GREEDY_SEARCH</span><span class="p">):</span>
        <span class="c1"># 11. prepare logits warper</span>
        <span class="n">prepared_logits_warper</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_logits_warper</span><span class="p">(</span><span class="n">generation_config</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">do_sample</span>
            <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>

        <span class="c1"># 12. expand input_ids with `num_return_sequences` additional sequences per batch</span>
        <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_inputs_for_generation</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">expand_size</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_return_sequences</span><span class="p">,</span>
            <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
            <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="o">=</span><span class="n">prepared_logits_processor</span><span class="p">,</span>
            <span class="n">logits_warper</span><span class="o">=</span><span class="n">prepared_logits_warper</span><span class="p">,</span>
            <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">prepared_stopping_criteria</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">synced_gpus</span><span class="o">=</span><span class="n">synced_gpus</span><span class="p">,</span>
            <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span>
            <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="n">generation_mode</span> <span class="ow">in</span> <span class="p">(</span><span class="n">GenerationMode</span><span class="o">.</span><span class="n">BEAM_SAMPLE</span><span class="p">,</span> <span class="n">GenerationMode</span><span class="o">.</span><span class="n">BEAM_SEARCH</span><span class="p">):</span>
        <span class="c1"># 11. prepare logits warper</span>
        <span class="n">prepared_logits_warper</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_logits_warper</span><span class="p">(</span><span class="n">generation_config</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">do_sample</span>
            <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>

        <span class="c1"># 12. prepare beam search scorer</span>
        <span class="n">beam_scorer</span> <span class="o">=</span> <span class="n">BeamSearchScorer</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">num_beams</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
            <span class="n">length_penalty</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">length_penalty</span><span class="p">,</span>
            <span class="n">do_early_stopping</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">,</span>
            <span class="n">num_beam_hyps_to_keep</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_return_sequences</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># 13. interleave input_ids with `num_beams` additional sequences per batch</span>
        <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_inputs_for_generation</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">expand_size</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
            <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
            <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># 14. run beam sample</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beam_search</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">beam_scorer</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="o">=</span><span class="n">prepared_logits_processor</span><span class="p">,</span>
            <span class="n">logits_warper</span><span class="o">=</span><span class="n">prepared_logits_warper</span><span class="p">,</span>
            <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">prepared_stopping_criteria</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">synced_gpus</span><span class="o">=</span><span class="n">synced_gpus</span><span class="p">,</span>
            <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="n">generation_mode</span> <span class="o">==</span> <span class="n">GenerationMode</span><span class="o">.</span><span class="n">GROUP_BEAM_SEARCH</span><span class="p">:</span>
        <span class="c1"># 11. prepare beam search scorer</span>
        <span class="n">beam_scorer</span> <span class="o">=</span> <span class="n">BeamSearchScorer</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">num_beams</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
            <span class="n">length_penalty</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">length_penalty</span><span class="p">,</span>
            <span class="n">do_early_stopping</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">,</span>
            <span class="n">num_beam_hyps_to_keep</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_return_sequences</span><span class="p">,</span>
            <span class="n">num_beam_groups</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beam_groups</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># 12. interleave input_ids with `num_beams` additional sequences per batch</span>
        <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_inputs_for_generation</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">expand_size</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
            <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
            <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># 13. run beam search</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_group_beam_search</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">beam_scorer</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="o">=</span><span class="n">prepared_logits_processor</span><span class="p">,</span>
            <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">prepared_stopping_criteria</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">synced_gpus</span><span class="o">=</span><span class="n">synced_gpus</span><span class="p">,</span>
            <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="n">generation_mode</span> <span class="o">==</span> <span class="n">GenerationMode</span><span class="o">.</span><span class="n">CONSTRAINED_BEAM_SEARCH</span><span class="p">:</span>
        <span class="n">final_constraints</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">constraints</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">final_constraints</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">constraints</span>

        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">force_words_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

            <span class="k">def</span> <span class="nf">typeerror</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`force_words_ids` has to either be a `List[List[List[int]]]` or `List[List[int]]` &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;of positive integers, but is </span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">force_words_ids</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">force_words_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
                <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">force_words_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="p">):</span>
                <span class="n">typeerror</span><span class="p">()</span>

            <span class="k">for</span> <span class="n">word_ids</span> <span class="ow">in</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">force_words_ids</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">word_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">word_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">typeerror</span><span class="p">()</span>
                    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">for</span> <span class="n">token_ids</span> <span class="ow">in</span> <span class="n">word_ids</span><span class="p">):</span>
                        <span class="n">typeerror</span><span class="p">()</span>
                    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
                        <span class="nb">any</span><span class="p">((</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">token_id</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">token_ids</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">token_ids</span> <span class="ow">in</span> <span class="n">word_ids</span>
                    <span class="p">):</span>
                        <span class="n">typeerror</span><span class="p">()</span>

                    <span class="n">constraint</span> <span class="o">=</span> <span class="n">DisjunctiveConstraint</span><span class="p">(</span><span class="n">word_ids</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">word_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">typeerror</span><span class="p">()</span>
                    <span class="k">if</span> <span class="nb">any</span><span class="p">((</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">token_id</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">word_ids</span><span class="p">):</span>
                        <span class="n">typeerror</span><span class="p">()</span>

                    <span class="n">constraint</span> <span class="o">=</span> <span class="n">PhrasalConstraint</span><span class="p">(</span><span class="n">word_ids</span><span class="p">)</span>
                <span class="n">final_constraints</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">constraint</span><span class="p">)</span>

        <span class="c1"># 11. prepare beam search scorer</span>
        <span class="n">constrained_beam_scorer</span> <span class="o">=</span> <span class="n">ConstrainedBeamSearchScorer</span><span class="p">(</span>
            <span class="n">constraints</span><span class="o">=</span><span class="n">final_constraints</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">num_beams</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
            <span class="n">length_penalty</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">length_penalty</span><span class="p">,</span>
            <span class="n">do_early_stopping</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">,</span>
            <span class="n">num_beam_hyps_to_keep</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_return_sequences</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># 12. interleave input_ids with `num_beams` additional sequences per batch</span>
        <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_inputs_for_generation</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">expand_size</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
            <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
            <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># 13. run beam search</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_constrained_beam_search</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">constrained_beam_scorer</span><span class="o">=</span><span class="n">constrained_beam_scorer</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="o">=</span><span class="n">prepared_logits_processor</span><span class="p">,</span>
            <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">prepared_stopping_criteria</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">synced_gpus</span><span class="o">=</span><span class="n">synced_gpus</span><span class="p">,</span>
            <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">_run_profiler</span><span class="p">:</span>
        <span class="n">_framework_profiler_step_end</span><span class="p">()</span>

    <span class="c1"># Convert to legacy cache if needed</span>
    <span class="k">if</span> <span class="n">use_dynamic_cache_by_default</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">return_legacy_cache</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">ModelOutput</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="s2">&quot;past_key_values&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span> <span class="p">(</span><span class="n">DynamicCache</span><span class="p">,</span> <span class="n">EncoderDecoderCache</span><span class="p">)):</span>
                <span class="n">result</span><span class="o">.</span><span class="n">past_key_values</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">past_key_values</span><span class="o">.</span><span class="n">to_legacy_cache</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.generation.utils.GenerationMixin.heal_tokens" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">generation</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">GenerationMixin</span><span class="o">.</span><span class="n">heal_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.generation.utils.GenerationMixin.heal_tokens" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Generates sequences of token ids for models with a language modeling head.
Parameters:
    input_ids (<code>mindspore.Tensor</code>): The sequence used as a prompt for the generation.
    tokenizer (<code>PreTrainedTokenizerBase</code>, <em>optional</em>): The tokenizer used to decode the input ids.
Return:
    <code>mindspore.Tensor</code> where each sequence has its tail token replaced with its appropriate extension.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\generation\utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2164</span>
<span class="normal">2165</span>
<span class="normal">2166</span>
<span class="normal">2167</span>
<span class="normal">2168</span>
<span class="normal">2169</span>
<span class="normal">2170</span>
<span class="normal">2171</span>
<span class="normal">2172</span>
<span class="normal">2173</span>
<span class="normal">2174</span>
<span class="normal">2175</span>
<span class="normal">2176</span>
<span class="normal">2177</span>
<span class="normal">2178</span>
<span class="normal">2179</span>
<span class="normal">2180</span>
<span class="normal">2181</span>
<span class="normal">2182</span>
<span class="normal">2183</span>
<span class="normal">2184</span>
<span class="normal">2185</span>
<span class="normal">2186</span>
<span class="normal">2187</span>
<span class="normal">2188</span>
<span class="normal">2189</span>
<span class="normal">2190</span>
<span class="normal">2191</span>
<span class="normal">2192</span>
<span class="normal">2193</span>
<span class="normal">2194</span>
<span class="normal">2195</span>
<span class="normal">2196</span>
<span class="normal">2197</span>
<span class="normal">2198</span>
<span class="normal">2199</span>
<span class="normal">2200</span>
<span class="normal">2201</span>
<span class="normal">2202</span>
<span class="normal">2203</span>
<span class="normal">2204</span>
<span class="normal">2205</span>
<span class="normal">2206</span>
<span class="normal">2207</span>
<span class="normal">2208</span>
<span class="normal">2209</span>
<span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span>
<span class="normal">2219</span>
<span class="normal">2220</span>
<span class="normal">2221</span>
<span class="normal">2222</span>
<span class="normal">2223</span>
<span class="normal">2224</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">heal_tokens</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;PreTrainedTokenizerBase&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates sequences of token ids for models with a language modeling head.</span>
<span class="sd">    Parameters:</span>
<span class="sd">        input_ids (`mindspore.Tensor`): The sequence used as a prompt for the generation.</span>
<span class="sd">        tokenizer (`PreTrainedTokenizerBase`, *optional*): The tokenizer used to decode the input ids.</span>
<span class="sd">    Return:</span>
<span class="sd">        `mindspore.Tensor` where each sequence has its tail token replaced with its appropriate extension.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">tokenizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot; When generating with token healing, you must pass the model&#39;s tokenizer to the `tokenizer` &quot;</span>
            <span class="s2">&quot;argument of `generate`.&quot;</span>
        <span class="p">)</span>

    <span class="n">bos_token_id</span><span class="p">,</span> <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span>
    <span class="n">vocab_trie</span> <span class="o">=</span> <span class="n">ExtensionsTrie</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">get_vocab</span><span class="p">())</span>
    <span class="n">generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">)</span>

    <span class="c1"># assumption: leading/trailing whitespace is not meaningful, so the prompts are</span>
    <span class="c1"># stripped before re-tokenizing to desensitize generation to whitespace artefacts</span>
    <span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
        <span class="n">prompts</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>

    <span class="c1"># replace bos with pad to not condition healing on it</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">input_ids</span> <span class="o">==</span> <span class="n">bos_token_id</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">)</span>

    <span class="n">tail_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">space_tok</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># tail tokens are used for a prefix search, thus, whitespaces are replaced with</span>
    <span class="c1"># their tokenization (e.g. &#39;Ġ&#39;) to enable search for tokens prefixed with a whitespace</span>
    <span class="n">tail_toks</span> <span class="o">=</span> <span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">space_tok</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tail_ids</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">tail_id</span><span class="p">,</span> <span class="n">tail_tok</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">tail_ids</span><span class="p">,</span> <span class="n">tail_toks</span><span class="p">)):</span>
        <span class="n">batch_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">batch_ids</span> <span class="o">==</span> <span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">():</span>
            <span class="k">continue</span>  <span class="c1"># skip empty sequences (all pad ids)</span>

        <span class="c1"># apply bias for alternatives (extensions) to the tail token</span>
        <span class="n">seq_bias</span> <span class="o">=</span> <span class="p">{(</span><span class="n">alt_tok</span><span class="p">,):</span> <span class="mf">10.0</span> <span class="k">for</span> <span class="n">alt_tok</span> <span class="ow">in</span> <span class="n">vocab_trie</span><span class="o">.</span><span class="n">values</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="n">tail_tok</span><span class="p">)}</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq_bias</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">continue</span>  <span class="c1"># skip if there are no token alternatives to heal with</span>

        <span class="c1"># slightly favor original token to limit aggressive healing e.g. &#39;http&#39; -&gt; &#39;https&#39;</span>
        <span class="n">seq_bias</span><span class="p">[(</span><span class="n">tail_id</span><span class="p">,)]</span> <span class="o">+=</span> <span class="mf">1.0</span>
        <span class="n">generation_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">sequence_bias</span><span class="o">=</span><span class="n">seq_bias</span><span class="p">)</span>

        <span class="n">trimmed_ids</span> <span class="o">=</span> <span class="n">batch_ids</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># if the prompt is a single (non-pad) token, regenerate from bos</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_ids</span><span class="p">[</span><span class="n">batch_ids</span> <span class="o">!=</span> <span class="n">pad_token_id</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">trimmed_ids</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">bos_token_id</span>

        <span class="n">input_ids</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">trimmed_ids</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">input_ids</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.generation.utils.stack_model_outputs" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">generation</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">stack_model_outputs</span><span class="p">(</span><span class="n">model_outputs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.generation.utils.stack_model_outputs" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Stack a list of ModelOutput objects (or its subclasses) along the batch_size dimension. The function infers the
specific ModelOutput subclass from the list provided.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\generation\utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4475</span>
<span class="normal">4476</span>
<span class="normal">4477</span>
<span class="normal">4478</span>
<span class="normal">4479</span>
<span class="normal">4480</span>
<span class="normal">4481</span>
<span class="normal">4482</span>
<span class="normal">4483</span>
<span class="normal">4484</span>
<span class="normal">4485</span>
<span class="normal">4486</span>
<span class="normal">4487</span>
<span class="normal">4488</span>
<span class="normal">4489</span>
<span class="normal">4490</span>
<span class="normal">4491</span>
<span class="normal">4492</span>
<span class="normal">4493</span>
<span class="normal">4494</span>
<span class="normal">4495</span>
<span class="normal">4496</span>
<span class="normal">4497</span>
<span class="normal">4498</span>
<span class="normal">4499</span>
<span class="normal">4500</span>
<span class="normal">4501</span>
<span class="normal">4502</span>
<span class="normal">4503</span>
<span class="normal">4504</span>
<span class="normal">4505</span>
<span class="normal">4506</span>
<span class="normal">4507</span>
<span class="normal">4508</span>
<span class="normal">4509</span>
<span class="normal">4510</span>
<span class="normal">4511</span>
<span class="normal">4512</span>
<span class="normal">4513</span>
<span class="normal">4514</span>
<span class="normal">4515</span>
<span class="normal">4516</span>
<span class="normal">4517</span>
<span class="normal">4518</span>
<span class="normal">4519</span>
<span class="normal">4520</span>
<span class="normal">4521</span>
<span class="normal">4522</span>
<span class="normal">4523</span>
<span class="normal">4524</span>
<span class="normal">4525</span>
<span class="normal">4526</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">stack_model_outputs</span><span class="p">(</span><span class="n">model_outputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">ModelOutput</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">ModelOutput</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stack a list of ModelOutput objects (or its subclasses) along the batch_size dimension. The function infers the</span>
<span class="sd">    specific ModelOutput subclass from the list provided.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">model_outputs</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input list is empty.&quot;</span><span class="p">)</span>

    <span class="c1"># Infer the class from the first object in the list</span>
    <span class="n">model_output_cls</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">model_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># Ensure all objects are of the same type</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">model_output_cls</span><span class="p">)</span> <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">model_outputs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;All elements in the list should be of the same type.&quot;</span><span class="p">)</span>

    <span class="c1"># Helper function to concat tensors or tuples of tensors</span>
    <span class="k">def</span> <span class="nf">_concat</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverse of `_split` function above.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">data</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># New cache format</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">DynamicCache</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">DynamicCache</span><span class="o">.</span><span class="n">from_batch_splits</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">EncoderDecoderCache</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">EncoderDecoderCache</span><span class="o">.</span><span class="n">from_batch_splits</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="c1"># If the elements of the tuple are also tuples (e.g., past_key_values in our earlier example)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span>
                    <span class="nb">tuple</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">attr</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">data</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])))</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">attr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">data</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
            <span class="c1"># If the elements are integers or floats, return a tensor</span>
            <span class="k">return</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unexpected attribute type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Use a dictionary comprehension to gather attributes from all objects and concatenate them</span>
    <span class="n">concatenated_data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">_concat</span><span class="p">([</span><span class="nb">getattr</span><span class="p">(</span><span class="n">model_output</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">model_output</span> <span class="ow">in</span> <span class="n">model_outputs</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">model_output_cls</span><span class="o">.</span><span class="n">__dataclass_fields__</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
    <span class="p">}</span>

    <span class="c1"># Return a new object of the inferred class with the concatenated attributes</span>
    <span class="k">return</span> <span class="n">model_output_cls</span><span class="p">(</span><span class="o">**</span><span class="n">concatenated_data</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="页脚" >
        
          
          <a href="../streamers/" class="md-footer__link md-footer__link--prev" aria-label="上一页: streamers">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                上一页
              </span>
              <div class="md-ellipsis">
                streamers
              </div>
            </div>
          </a>
        
        
          
          <a href="../../models/" class="md-footer__link md-footer__link--next" aria-label="下一页: Supported models">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                下一页
              </span>
              <div class="md-ellipsis">
                Supported models
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 - 2024 MindSpore Lab and CQU NLP Team.
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:lvyufeng@cqu.edu.cn" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindnlp" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/lu-yu-feng-46-1" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.indexes", "navigation.top", "navigation.footer", "navigation.path", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>