
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../configuration_utils/">
      
      
        <link rel="next" href="../tokenization_utils/">
      
      
      <link rel="icon" href="../../../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.44">
    
    
      
        <title>modeling_utils - MindNLP Docs</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../../" title="MindNLP Docs" class="md-header__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindNLP Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              modeling_utils
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="选择当前语言">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="../../../../api/transformers/modeling_utils/" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="./" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="分享" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindnlp" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../" class="md-tabs__link">
        
  
    
  
  主页

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../tutorials/quick_start/" class="md-tabs__link">
          
  
    
  
  教程

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../supported_models/" class="md-tabs__link">
        
  
    
  
  模型列表

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../contribute/" class="md-tabs__link">
        
  
    
  
  代码贡献

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../accelerate/" class="md-tabs__link">
          
  
    
  
  API文档

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../notes/changelog/" class="md-tabs__link">
          
  
    
  
  说明

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../" title="MindNLP Docs" class="md-nav__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    MindNLP Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindnlp" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    主页
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    教程
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            教程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/quick_start/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/data_preprocess/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Preprocess
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/use_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/use_mirror/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Mirror
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../supported_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    模型列表
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../contribute/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    代码贡献
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    API文档
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            API文档
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../accelerate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Accelerate
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../dataset/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Dataset
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            Dataset
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dataset/load_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    load_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dataset/BaseMapFunction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BaseMapFunction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dataset/transforms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transforms
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" >
        
          
          <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Engine
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            Engine
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_1" >
        
          
          <label class="md-nav__link" for="__nav_5_4_1" id="__nav_5_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    train_args
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_1">
            <span class="md-nav__icon md-icon"></span>
            train_args
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engine/train_args/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engine/train_args/seq2seq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seq2seq
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_2" >
        
          
          <label class="md-nav__link" for="__nav_5_4_2" id="__nav_5_4_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_2">
            <span class="md-nav__icon md-icon"></span>
            trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engine/trainer/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engine/trainer/default_func/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    default_func
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engine/callbacks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    callbacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engine/export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    export
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../engine/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modules
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../parallel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parallel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../peft/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_7" id="__nav_5_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7">
            <span class="md-nav__icon md-icon"></span>
            PEFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_2" >
        
          
          <label class="md-nav__link" for="__nav_5_7_2" id="__nav_5_7_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    tuners
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_2">
            <span class="md-nav__icon md-icon"></span>
            tuners
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../peft/tuners/adalora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AdaLoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../peft/tuners/adaption_prompt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adaption_Prompt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../peft/tuners/ia3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IA3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../peft/tuners/lokr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoKr
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../peft/tuners/lora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../peft/tuners/prompt_tuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt tuning
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_3" >
        
          
          <label class="md-nav__link" for="__nav_5_7_3" id="__nav_5_7_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_3">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../peft/utils/merge_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    merge_utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../peft/config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../peft/mapping/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mapping
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../peft/peft_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    peft_model
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sentence/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sentence
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Transformers
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9" id="__nav_5_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_9_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_2" >
        
          
          <label class="md-nav__link" for="__nav_5_9_2" id="__nav_5_9_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_2">
            <span class="md-nav__icon md-icon"></span>
            generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../api/transforemrs/generation/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generation/beam_constraints/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_constraints
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generation/beam_search/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_search
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generation/logits_process/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    logits_process
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generation/stopping_criteria/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stopping_criteria
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generation/streamers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    streamers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generation/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../models/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    models
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9_3" id="__nav_5_9_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_3">
            <span class="md-nav__icon md-icon"></span>
            models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/albert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    albert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/align/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    align
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/altclip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    altclip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/audio_spectrogram_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    audio_spectrogram_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/auto/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    auto
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/autoformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    autoformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/baichuan/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    baichuan
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/bark/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bark
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/bart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/barthez/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    barthez
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/bartpho/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bartpho
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/beit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/bert_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/bert_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/bertweet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bertweet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/bge_m3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bge_m3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/big_bird/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    big_bird
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/bigbird_pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bigbird_pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/biogpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    biogpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/bit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/blenderbot/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/blenderbot_small/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot_small
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/blip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/blip_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip_2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/bloom/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bloom
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/bridgetower/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bridgetower
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/bros/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bros
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/byt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    byt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/camembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    camembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/canine/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    canine
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/chatglm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/chatglm2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/chatglm3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/codegen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    codegen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/cogvlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cogvlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/cohere/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cohere
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/convbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/convnext/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convnext
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/cpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/cpmant/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmant
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/cpmbee/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmbee
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/ctrl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ctrl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/cvt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cvt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/data2vec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    data2vec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/deberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/deberta_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/decision_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    decision_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/distilbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    distilbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/efficientformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/efficientnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/electra/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    electra
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/encodec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    encodec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/ernie/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/ernie_m/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie_m
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/esm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    esm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/falcon/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    falcon
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/flava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    flava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/funnel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    funnel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/gemma/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gemma
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/git/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    git
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/gpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/gpt_bigcode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_bigcode
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/gpt_neo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/gpt_neox/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/gpt_neox_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/gpt_pangu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_pangu
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/gptj/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gptj
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/graphormer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    graphormer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/groupvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    groupvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/hubert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    hubert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/imagegpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    imagegpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/internlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    internlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/jamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/jetmoe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jetmoe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/layoutlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/layoutlmv2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlmv2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/led/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    led
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/llama/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llama
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/llava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/llava_next/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava_next
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/longformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/longt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/luke/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    luke
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/mamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/marian/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    marian
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/maskformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    maskformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/mbart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mbart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/megatron_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/megatron_gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/minicpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minicpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/minigpt4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minigpt4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/mistral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mistral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/mixtral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mixtral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/mobilebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/mobilevit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilevit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/moss/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    moss
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/mpnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/mpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/mt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/musicgen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/musicgen_melody/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen_melody
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/mvp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mvp
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/nezha/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nezha
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/nystromformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nystromformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/olmo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    olmo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/openelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    openelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/opt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    opt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/owlvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    owlvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/phi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/phi3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/poolformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    poolformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/pop2piano/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pop2piano
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/qwen2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/qwen2_moe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2_moe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/reformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    reformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/regnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    regnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/rembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/resnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    resnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/roc_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roc_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/rwkv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rwkv
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/sam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/seamless_m4t/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/seamless_m4t_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/segformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    segformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/seggpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seggpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/speech_encoder_decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_encoder_decoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/speech_to_text/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_to_text
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/squeezebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    squeezebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/stablelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stablelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/starcoder2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    starcoder2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/swiftformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    swiftformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/switch_transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    switch_transformers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/t5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    t5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/table_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    table_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/timesformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    timesformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/tinybert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tinybert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/van/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    van
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/vipllava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vipllava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/vision_text_dual_encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vision_text_dual_encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/visual_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    visual_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/wav2vec2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/wav2vec2_conformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_conformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/wav2vec2_with_lm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_with_lm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/wavlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wavlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/whisper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    whisper
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/x_clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    x_clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/xlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/xlm_roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/xlm_roberta_xl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta_xl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/xlnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlnet
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_4" >
        
          
          <label class="md-nav__link" for="__nav_5_9_4" id="__nav_5_9_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_4">
            <span class="md-nav__icon md-icon"></span>
            pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../api/transforemrs/pipeline/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipeline/automatic_speech_recognition/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    automatic_speech_recognition
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipeline/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipeline/document_question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    document_question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipeline/fill_mask/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    fill_mask
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipeline/pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipeline/question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipeline/text2text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text2text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipeline/text_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipeline/text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipeline/zero_shot_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    zero_shot_classification
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../configuration_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    configuration_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    modeling_utils
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    modeling_utils
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      PreTrainedModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PreTrainedModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.base_model" class="md-nav__link">
    <span class="md-ellipsis">
      base_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.dummy_inputs" class="md-nav__link">
    <span class="md-ellipsis">
      dummy_inputs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.framework" class="md-nav__link">
    <span class="md-ellipsis">
      framework
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.is_gradient_checkpointing" class="md-nav__link">
    <span class="md-ellipsis">
      is_gradient_checkpointing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.add_model_tags" class="md-nav__link">
    <span class="md-ellipsis">
      add_model_tags
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.can_generate" class="md-nav__link">
    <span class="md-ellipsis">
      can_generate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.dequantize" class="md-nav__link">
    <span class="md-ellipsis">
      dequantize
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.disable_input_require_grads" class="md-nav__link">
    <span class="md-ellipsis">
      disable_input_require_grads
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.enable_input_require_grads" class="md-nav__link">
    <span class="md-ellipsis">
      enable_input_require_grads
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.from_pretrained" class="md-nav__link">
    <span class="md-ellipsis">
      from_pretrained
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.get_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.get_memory_footprint" class="md-nav__link">
    <span class="md-ellipsis">
      get_memory_footprint
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.get_output_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_output_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.gradient_checkpointing_disable" class="md-nav__link">
    <span class="md-ellipsis">
      gradient_checkpointing_disable
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.gradient_checkpointing_enable" class="md-nav__link">
    <span class="md-ellipsis">
      gradient_checkpointing_enable
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.init_weights" class="md-nav__link">
    <span class="md-ellipsis">
      init_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.post_init" class="md-nav__link">
    <span class="md-ellipsis">
      post_init
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.prune_heads" class="md-nav__link">
    <span class="md-ellipsis">
      prune_heads
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.register_for_auto_class" class="md-nav__link">
    <span class="md-ellipsis">
      register_for_auto_class
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.resize_token_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      resize_token_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.save_pretrained" class="md-nav__link">
    <span class="md-ellipsis">
      save_pretrained
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.set_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.tie_weights" class="md-nav__link">
    <span class="md-ellipsis">
      tie_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.warn_if_padding_and_no_attention_mask" class="md-nav__link">
    <span class="md-ellipsis">
      warn_if_padding_and_no_attention_mask
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tokenization_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tokenization_utils_base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tokenization_utils_fast/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_fast
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../trl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TRL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    说明
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            说明
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../notes/changelog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    更新日志
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../notes/code_of_conduct/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    行为准则
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../notes/faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    常见问题
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      PreTrainedModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PreTrainedModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.base_model" class="md-nav__link">
    <span class="md-ellipsis">
      base_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.dummy_inputs" class="md-nav__link">
    <span class="md-ellipsis">
      dummy_inputs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.framework" class="md-nav__link">
    <span class="md-ellipsis">
      framework
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.is_gradient_checkpointing" class="md-nav__link">
    <span class="md-ellipsis">
      is_gradient_checkpointing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.add_model_tags" class="md-nav__link">
    <span class="md-ellipsis">
      add_model_tags
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.can_generate" class="md-nav__link">
    <span class="md-ellipsis">
      can_generate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.dequantize" class="md-nav__link">
    <span class="md-ellipsis">
      dequantize
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.disable_input_require_grads" class="md-nav__link">
    <span class="md-ellipsis">
      disable_input_require_grads
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.enable_input_require_grads" class="md-nav__link">
    <span class="md-ellipsis">
      enable_input_require_grads
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.from_pretrained" class="md-nav__link">
    <span class="md-ellipsis">
      from_pretrained
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.get_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.get_memory_footprint" class="md-nav__link">
    <span class="md-ellipsis">
      get_memory_footprint
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.get_output_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_output_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.gradient_checkpointing_disable" class="md-nav__link">
    <span class="md-ellipsis">
      gradient_checkpointing_disable
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.gradient_checkpointing_enable" class="md-nav__link">
    <span class="md-ellipsis">
      gradient_checkpointing_enable
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.init_weights" class="md-nav__link">
    <span class="md-ellipsis">
      init_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.post_init" class="md-nav__link">
    <span class="md-ellipsis">
      post_init
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.prune_heads" class="md-nav__link">
    <span class="md-ellipsis">
      prune_heads
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.register_for_auto_class" class="md-nav__link">
    <span class="md-ellipsis">
      register_for_auto_class
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.resize_token_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      resize_token_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.save_pretrained" class="md-nav__link">
    <span class="md-ellipsis">
      save_pretrained
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.set_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.tie_weights" class="md-nav__link">
    <span class="md-ellipsis">
      tie_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.warn_if_padding_and_no_attention_mask" class="md-nav__link">
    <span class="md-ellipsis">
      warn_if_padding_and_no_attention_mask
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/mindspore-lab/mindnlp/edit/master/docs/en/api/transformers/modeling_utils.md" title="编辑此页" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindnlp/raw/master/docs/en/api/transformers/modeling_utils.md" title="查看本页的源代码" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


  <h1>modeling_utils</h1>

<div class="doc doc-object doc-class">



<h2 id="mindnlp.transformers.modeling_utils.PreTrainedModel" class="doc doc-heading">
            <code>mindnlp.transformers.modeling_utils.PreTrainedModel</code>


<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code>, <code><span title="mindnlp.transformers.modeling_utils.ModuleUtilsMixin">ModuleUtilsMixin</span></code>, <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.GenerationMixin" href="../../../../api/transformers/generation/utils/#mindnlp.transformers.generation.utils.GenerationMixin">GenerationMixin</a></code>, <code><span title="mindnlp.transformers.integrations.PeftAdapterMixin">PeftAdapterMixin</span></code></p>


        <p>Base class for all models.</p>
<p>[<code>PreTrainedModel</code>] takes care of storing the configuration of the models and handles methods for loading,
downloading and saving models as well as a few methods common to all models to:</p>
<div class="highlight"><pre><span></span><code>- resize the input embeddings,
- prune heads in the self-attention heads.
</code></pre></div>
<p>Class attributes (overridden by derived classes):</p>
<div class="highlight"><pre><span></span><code>- **config_class** ([`PretrainedConfig`]) -- A subclass of [`PretrainedConfig`] to use as configuration class
  for this model architecture.
- **load_tf_weights** (`Callable`) -- A python *method* for loading a TensorFlow checkpoint in a PyTorch model,
  taking as arguments:

    - **model** ([`PreTrainedModel`]) -- An instance of the model on which to load the TensorFlow checkpoint.
    - **config** ([`PreTrainedConfig`]) -- An instance of the configuration associated to the model.
    - **path** (`str`) -- A path to the TensorFlow checkpoint.

- **base_model_prefix** (`str`) -- A string indicating the attribute associated to the base model in derived
  classes of the same architecture adding modules on top of the base model.
- **is_parallelizable** (`bool`) -- A flag indicating whether this model supports model parallelization.
- **main_input_name** (`str`) -- The name of the principal input to the model (often `input_ids` for NLP
  models, `pixel_values` for vision models and `input_values` for speech models).
</code></pre></div>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span>
<span class="normal">1445</span>
<span class="normal">1446</span>
<span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span>
<span class="normal">2105</span>
<span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span>
<span class="normal">2109</span>
<span class="normal">2110</span>
<span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span>
<span class="normal">2141</span>
<span class="normal">2142</span>
<span class="normal">2143</span>
<span class="normal">2144</span>
<span class="normal">2145</span>
<span class="normal">2146</span>
<span class="normal">2147</span>
<span class="normal">2148</span>
<span class="normal">2149</span>
<span class="normal">2150</span>
<span class="normal">2151</span>
<span class="normal">2152</span>
<span class="normal">2153</span>
<span class="normal">2154</span>
<span class="normal">2155</span>
<span class="normal">2156</span>
<span class="normal">2157</span>
<span class="normal">2158</span>
<span class="normal">2159</span>
<span class="normal">2160</span>
<span class="normal">2161</span>
<span class="normal">2162</span>
<span class="normal">2163</span>
<span class="normal">2164</span>
<span class="normal">2165</span>
<span class="normal">2166</span>
<span class="normal">2167</span>
<span class="normal">2168</span>
<span class="normal">2169</span>
<span class="normal">2170</span>
<span class="normal">2171</span>
<span class="normal">2172</span>
<span class="normal">2173</span>
<span class="normal">2174</span>
<span class="normal">2175</span>
<span class="normal">2176</span>
<span class="normal">2177</span>
<span class="normal">2178</span>
<span class="normal">2179</span>
<span class="normal">2180</span>
<span class="normal">2181</span>
<span class="normal">2182</span>
<span class="normal">2183</span>
<span class="normal">2184</span>
<span class="normal">2185</span>
<span class="normal">2186</span>
<span class="normal">2187</span>
<span class="normal">2188</span>
<span class="normal">2189</span>
<span class="normal">2190</span>
<span class="normal">2191</span>
<span class="normal">2192</span>
<span class="normal">2193</span>
<span class="normal">2194</span>
<span class="normal">2195</span>
<span class="normal">2196</span>
<span class="normal">2197</span>
<span class="normal">2198</span>
<span class="normal">2199</span>
<span class="normal">2200</span>
<span class="normal">2201</span>
<span class="normal">2202</span>
<span class="normal">2203</span>
<span class="normal">2204</span>
<span class="normal">2205</span>
<span class="normal">2206</span>
<span class="normal">2207</span>
<span class="normal">2208</span>
<span class="normal">2209</span>
<span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span>
<span class="normal">2219</span>
<span class="normal">2220</span>
<span class="normal">2221</span>
<span class="normal">2222</span>
<span class="normal">2223</span>
<span class="normal">2224</span>
<span class="normal">2225</span>
<span class="normal">2226</span>
<span class="normal">2227</span>
<span class="normal">2228</span>
<span class="normal">2229</span>
<span class="normal">2230</span>
<span class="normal">2231</span>
<span class="normal">2232</span>
<span class="normal">2233</span>
<span class="normal">2234</span>
<span class="normal">2235</span>
<span class="normal">2236</span>
<span class="normal">2237</span>
<span class="normal">2238</span>
<span class="normal">2239</span>
<span class="normal">2240</span>
<span class="normal">2241</span>
<span class="normal">2242</span>
<span class="normal">2243</span>
<span class="normal">2244</span>
<span class="normal">2245</span>
<span class="normal">2246</span>
<span class="normal">2247</span>
<span class="normal">2248</span>
<span class="normal">2249</span>
<span class="normal">2250</span>
<span class="normal">2251</span>
<span class="normal">2252</span>
<span class="normal">2253</span>
<span class="normal">2254</span>
<span class="normal">2255</span>
<span class="normal">2256</span>
<span class="normal">2257</span>
<span class="normal">2258</span>
<span class="normal">2259</span>
<span class="normal">2260</span>
<span class="normal">2261</span>
<span class="normal">2262</span>
<span class="normal">2263</span>
<span class="normal">2264</span>
<span class="normal">2265</span>
<span class="normal">2266</span>
<span class="normal">2267</span>
<span class="normal">2268</span>
<span class="normal">2269</span>
<span class="normal">2270</span>
<span class="normal">2271</span>
<span class="normal">2272</span>
<span class="normal">2273</span>
<span class="normal">2274</span>
<span class="normal">2275</span>
<span class="normal">2276</span>
<span class="normal">2277</span>
<span class="normal">2278</span>
<span class="normal">2279</span>
<span class="normal">2280</span>
<span class="normal">2281</span>
<span class="normal">2282</span>
<span class="normal">2283</span>
<span class="normal">2284</span>
<span class="normal">2285</span>
<span class="normal">2286</span>
<span class="normal">2287</span>
<span class="normal">2288</span>
<span class="normal">2289</span>
<span class="normal">2290</span>
<span class="normal">2291</span>
<span class="normal">2292</span>
<span class="normal">2293</span>
<span class="normal">2294</span>
<span class="normal">2295</span>
<span class="normal">2296</span>
<span class="normal">2297</span>
<span class="normal">2298</span>
<span class="normal">2299</span>
<span class="normal">2300</span>
<span class="normal">2301</span>
<span class="normal">2302</span>
<span class="normal">2303</span>
<span class="normal">2304</span>
<span class="normal">2305</span>
<span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span>
<span class="normal">2313</span>
<span class="normal">2314</span>
<span class="normal">2315</span>
<span class="normal">2316</span>
<span class="normal">2317</span>
<span class="normal">2318</span>
<span class="normal">2319</span>
<span class="normal">2320</span>
<span class="normal">2321</span>
<span class="normal">2322</span>
<span class="normal">2323</span>
<span class="normal">2324</span>
<span class="normal">2325</span>
<span class="normal">2326</span>
<span class="normal">2327</span>
<span class="normal">2328</span>
<span class="normal">2329</span>
<span class="normal">2330</span>
<span class="normal">2331</span>
<span class="normal">2332</span>
<span class="normal">2333</span>
<span class="normal">2334</span>
<span class="normal">2335</span>
<span class="normal">2336</span>
<span class="normal">2337</span>
<span class="normal">2338</span>
<span class="normal">2339</span>
<span class="normal">2340</span>
<span class="normal">2341</span>
<span class="normal">2342</span>
<span class="normal">2343</span>
<span class="normal">2344</span>
<span class="normal">2345</span>
<span class="normal">2346</span>
<span class="normal">2347</span>
<span class="normal">2348</span>
<span class="normal">2349</span>
<span class="normal">2350</span>
<span class="normal">2351</span>
<span class="normal">2352</span>
<span class="normal">2353</span>
<span class="normal">2354</span>
<span class="normal">2355</span>
<span class="normal">2356</span>
<span class="normal">2357</span>
<span class="normal">2358</span>
<span class="normal">2359</span>
<span class="normal">2360</span>
<span class="normal">2361</span>
<span class="normal">2362</span>
<span class="normal">2363</span>
<span class="normal">2364</span>
<span class="normal">2365</span>
<span class="normal">2366</span>
<span class="normal">2367</span>
<span class="normal">2368</span>
<span class="normal">2369</span>
<span class="normal">2370</span>
<span class="normal">2371</span>
<span class="normal">2372</span>
<span class="normal">2373</span>
<span class="normal">2374</span>
<span class="normal">2375</span>
<span class="normal">2376</span>
<span class="normal">2377</span>
<span class="normal">2378</span>
<span class="normal">2379</span>
<span class="normal">2380</span>
<span class="normal">2381</span>
<span class="normal">2382</span>
<span class="normal">2383</span>
<span class="normal">2384</span>
<span class="normal">2385</span>
<span class="normal">2386</span>
<span class="normal">2387</span>
<span class="normal">2388</span>
<span class="normal">2389</span>
<span class="normal">2390</span>
<span class="normal">2391</span>
<span class="normal">2392</span>
<span class="normal">2393</span>
<span class="normal">2394</span>
<span class="normal">2395</span>
<span class="normal">2396</span>
<span class="normal">2397</span>
<span class="normal">2398</span>
<span class="normal">2399</span>
<span class="normal">2400</span>
<span class="normal">2401</span>
<span class="normal">2402</span>
<span class="normal">2403</span>
<span class="normal">2404</span>
<span class="normal">2405</span>
<span class="normal">2406</span>
<span class="normal">2407</span>
<span class="normal">2408</span>
<span class="normal">2409</span>
<span class="normal">2410</span>
<span class="normal">2411</span>
<span class="normal">2412</span>
<span class="normal">2413</span>
<span class="normal">2414</span>
<span class="normal">2415</span>
<span class="normal">2416</span>
<span class="normal">2417</span>
<span class="normal">2418</span>
<span class="normal">2419</span>
<span class="normal">2420</span>
<span class="normal">2421</span>
<span class="normal">2422</span>
<span class="normal">2423</span>
<span class="normal">2424</span>
<span class="normal">2425</span>
<span class="normal">2426</span>
<span class="normal">2427</span>
<span class="normal">2428</span>
<span class="normal">2429</span>
<span class="normal">2430</span>
<span class="normal">2431</span>
<span class="normal">2432</span>
<span class="normal">2433</span>
<span class="normal">2434</span>
<span class="normal">2435</span>
<span class="normal">2436</span>
<span class="normal">2437</span>
<span class="normal">2438</span>
<span class="normal">2439</span>
<span class="normal">2440</span>
<span class="normal">2441</span>
<span class="normal">2442</span>
<span class="normal">2443</span>
<span class="normal">2444</span>
<span class="normal">2445</span>
<span class="normal">2446</span>
<span class="normal">2447</span>
<span class="normal">2448</span>
<span class="normal">2449</span>
<span class="normal">2450</span>
<span class="normal">2451</span>
<span class="normal">2452</span>
<span class="normal">2453</span>
<span class="normal">2454</span>
<span class="normal">2455</span>
<span class="normal">2456</span>
<span class="normal">2457</span>
<span class="normal">2458</span>
<span class="normal">2459</span>
<span class="normal">2460</span>
<span class="normal">2461</span>
<span class="normal">2462</span>
<span class="normal">2463</span>
<span class="normal">2464</span>
<span class="normal">2465</span>
<span class="normal">2466</span>
<span class="normal">2467</span>
<span class="normal">2468</span>
<span class="normal">2469</span>
<span class="normal">2470</span>
<span class="normal">2471</span>
<span class="normal">2472</span>
<span class="normal">2473</span>
<span class="normal">2474</span>
<span class="normal">2475</span>
<span class="normal">2476</span>
<span class="normal">2477</span>
<span class="normal">2478</span>
<span class="normal">2479</span>
<span class="normal">2480</span>
<span class="normal">2481</span>
<span class="normal">2482</span>
<span class="normal">2483</span>
<span class="normal">2484</span>
<span class="normal">2485</span>
<span class="normal">2486</span>
<span class="normal">2487</span>
<span class="normal">2488</span>
<span class="normal">2489</span>
<span class="normal">2490</span>
<span class="normal">2491</span>
<span class="normal">2492</span>
<span class="normal">2493</span>
<span class="normal">2494</span>
<span class="normal">2495</span>
<span class="normal">2496</span>
<span class="normal">2497</span>
<span class="normal">2498</span>
<span class="normal">2499</span>
<span class="normal">2500</span>
<span class="normal">2501</span>
<span class="normal">2502</span>
<span class="normal">2503</span>
<span class="normal">2504</span>
<span class="normal">2505</span>
<span class="normal">2506</span>
<span class="normal">2507</span>
<span class="normal">2508</span>
<span class="normal">2509</span>
<span class="normal">2510</span>
<span class="normal">2511</span>
<span class="normal">2512</span>
<span class="normal">2513</span>
<span class="normal">2514</span>
<span class="normal">2515</span>
<span class="normal">2516</span>
<span class="normal">2517</span>
<span class="normal">2518</span>
<span class="normal">2519</span>
<span class="normal">2520</span>
<span class="normal">2521</span>
<span class="normal">2522</span>
<span class="normal">2523</span>
<span class="normal">2524</span>
<span class="normal">2525</span>
<span class="normal">2526</span>
<span class="normal">2527</span>
<span class="normal">2528</span>
<span class="normal">2529</span>
<span class="normal">2530</span>
<span class="normal">2531</span>
<span class="normal">2532</span>
<span class="normal">2533</span>
<span class="normal">2534</span>
<span class="normal">2535</span>
<span class="normal">2536</span>
<span class="normal">2537</span>
<span class="normal">2538</span>
<span class="normal">2539</span>
<span class="normal">2540</span>
<span class="normal">2541</span>
<span class="normal">2542</span>
<span class="normal">2543</span>
<span class="normal">2544</span>
<span class="normal">2545</span>
<span class="normal">2546</span>
<span class="normal">2547</span>
<span class="normal">2548</span>
<span class="normal">2549</span>
<span class="normal">2550</span>
<span class="normal">2551</span>
<span class="normal">2552</span>
<span class="normal">2553</span>
<span class="normal">2554</span>
<span class="normal">2555</span>
<span class="normal">2556</span>
<span class="normal">2557</span>
<span class="normal">2558</span>
<span class="normal">2559</span>
<span class="normal">2560</span>
<span class="normal">2561</span>
<span class="normal">2562</span>
<span class="normal">2563</span>
<span class="normal">2564</span>
<span class="normal">2565</span>
<span class="normal">2566</span>
<span class="normal">2567</span>
<span class="normal">2568</span>
<span class="normal">2569</span>
<span class="normal">2570</span>
<span class="normal">2571</span>
<span class="normal">2572</span>
<span class="normal">2573</span>
<span class="normal">2574</span>
<span class="normal">2575</span>
<span class="normal">2576</span>
<span class="normal">2577</span>
<span class="normal">2578</span>
<span class="normal">2579</span>
<span class="normal">2580</span>
<span class="normal">2581</span>
<span class="normal">2582</span>
<span class="normal">2583</span>
<span class="normal">2584</span>
<span class="normal">2585</span>
<span class="normal">2586</span>
<span class="normal">2587</span>
<span class="normal">2588</span>
<span class="normal">2589</span>
<span class="normal">2590</span>
<span class="normal">2591</span>
<span class="normal">2592</span>
<span class="normal">2593</span>
<span class="normal">2594</span>
<span class="normal">2595</span>
<span class="normal">2596</span>
<span class="normal">2597</span>
<span class="normal">2598</span>
<span class="normal">2599</span>
<span class="normal">2600</span>
<span class="normal">2601</span>
<span class="normal">2602</span>
<span class="normal">2603</span>
<span class="normal">2604</span>
<span class="normal">2605</span>
<span class="normal">2606</span>
<span class="normal">2607</span>
<span class="normal">2608</span>
<span class="normal">2609</span>
<span class="normal">2610</span>
<span class="normal">2611</span>
<span class="normal">2612</span>
<span class="normal">2613</span>
<span class="normal">2614</span>
<span class="normal">2615</span>
<span class="normal">2616</span>
<span class="normal">2617</span>
<span class="normal">2618</span>
<span class="normal">2619</span>
<span class="normal">2620</span>
<span class="normal">2621</span>
<span class="normal">2622</span>
<span class="normal">2623</span>
<span class="normal">2624</span>
<span class="normal">2625</span>
<span class="normal">2626</span>
<span class="normal">2627</span>
<span class="normal">2628</span>
<span class="normal">2629</span>
<span class="normal">2630</span>
<span class="normal">2631</span>
<span class="normal">2632</span>
<span class="normal">2633</span>
<span class="normal">2634</span>
<span class="normal">2635</span>
<span class="normal">2636</span>
<span class="normal">2637</span>
<span class="normal">2638</span>
<span class="normal">2639</span>
<span class="normal">2640</span>
<span class="normal">2641</span>
<span class="normal">2642</span>
<span class="normal">2643</span>
<span class="normal">2644</span>
<span class="normal">2645</span>
<span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span>
<span class="normal">2652</span>
<span class="normal">2653</span>
<span class="normal">2654</span>
<span class="normal">2655</span>
<span class="normal">2656</span>
<span class="normal">2657</span>
<span class="normal">2658</span>
<span class="normal">2659</span>
<span class="normal">2660</span>
<span class="normal">2661</span>
<span class="normal">2662</span>
<span class="normal">2663</span>
<span class="normal">2664</span>
<span class="normal">2665</span>
<span class="normal">2666</span>
<span class="normal">2667</span>
<span class="normal">2668</span>
<span class="normal">2669</span>
<span class="normal">2670</span>
<span class="normal">2671</span>
<span class="normal">2672</span>
<span class="normal">2673</span>
<span class="normal">2674</span>
<span class="normal">2675</span>
<span class="normal">2676</span>
<span class="normal">2677</span>
<span class="normal">2678</span>
<span class="normal">2679</span>
<span class="normal">2680</span>
<span class="normal">2681</span>
<span class="normal">2682</span>
<span class="normal">2683</span>
<span class="normal">2684</span>
<span class="normal">2685</span>
<span class="normal">2686</span>
<span class="normal">2687</span>
<span class="normal">2688</span>
<span class="normal">2689</span>
<span class="normal">2690</span>
<span class="normal">2691</span>
<span class="normal">2692</span>
<span class="normal">2693</span>
<span class="normal">2694</span>
<span class="normal">2695</span>
<span class="normal">2696</span>
<span class="normal">2697</span>
<span class="normal">2698</span>
<span class="normal">2699</span>
<span class="normal">2700</span>
<span class="normal">2701</span>
<span class="normal">2702</span>
<span class="normal">2703</span>
<span class="normal">2704</span>
<span class="normal">2705</span>
<span class="normal">2706</span>
<span class="normal">2707</span>
<span class="normal">2708</span>
<span class="normal">2709</span>
<span class="normal">2710</span>
<span class="normal">2711</span>
<span class="normal">2712</span>
<span class="normal">2713</span>
<span class="normal">2714</span>
<span class="normal">2715</span>
<span class="normal">2716</span>
<span class="normal">2717</span>
<span class="normal">2718</span>
<span class="normal">2719</span>
<span class="normal">2720</span>
<span class="normal">2721</span>
<span class="normal">2722</span>
<span class="normal">2723</span>
<span class="normal">2724</span>
<span class="normal">2725</span>
<span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span>
<span class="normal">2733</span>
<span class="normal">2734</span>
<span class="normal">2735</span>
<span class="normal">2736</span>
<span class="normal">2737</span>
<span class="normal">2738</span>
<span class="normal">2739</span>
<span class="normal">2740</span>
<span class="normal">2741</span>
<span class="normal">2742</span>
<span class="normal">2743</span>
<span class="normal">2744</span>
<span class="normal">2745</span>
<span class="normal">2746</span>
<span class="normal">2747</span>
<span class="normal">2748</span>
<span class="normal">2749</span>
<span class="normal">2750</span>
<span class="normal">2751</span>
<span class="normal">2752</span>
<span class="normal">2753</span>
<span class="normal">2754</span>
<span class="normal">2755</span>
<span class="normal">2756</span>
<span class="normal">2757</span>
<span class="normal">2758</span>
<span class="normal">2759</span>
<span class="normal">2760</span>
<span class="normal">2761</span>
<span class="normal">2762</span>
<span class="normal">2763</span>
<span class="normal">2764</span>
<span class="normal">2765</span>
<span class="normal">2766</span>
<span class="normal">2767</span>
<span class="normal">2768</span>
<span class="normal">2769</span>
<span class="normal">2770</span>
<span class="normal">2771</span>
<span class="normal">2772</span>
<span class="normal">2773</span>
<span class="normal">2774</span>
<span class="normal">2775</span>
<span class="normal">2776</span>
<span class="normal">2777</span>
<span class="normal">2778</span>
<span class="normal">2779</span>
<span class="normal">2780</span>
<span class="normal">2781</span>
<span class="normal">2782</span>
<span class="normal">2783</span>
<span class="normal">2784</span>
<span class="normal">2785</span>
<span class="normal">2786</span>
<span class="normal">2787</span>
<span class="normal">2788</span>
<span class="normal">2789</span>
<span class="normal">2790</span>
<span class="normal">2791</span>
<span class="normal">2792</span>
<span class="normal">2793</span>
<span class="normal">2794</span>
<span class="normal">2795</span>
<span class="normal">2796</span>
<span class="normal">2797</span>
<span class="normal">2798</span>
<span class="normal">2799</span>
<span class="normal">2800</span>
<span class="normal">2801</span>
<span class="normal">2802</span>
<span class="normal">2803</span>
<span class="normal">2804</span>
<span class="normal">2805</span>
<span class="normal">2806</span>
<span class="normal">2807</span>
<span class="normal">2808</span>
<span class="normal">2809</span>
<span class="normal">2810</span>
<span class="normal">2811</span>
<span class="normal">2812</span>
<span class="normal">2813</span>
<span class="normal">2814</span>
<span class="normal">2815</span>
<span class="normal">2816</span>
<span class="normal">2817</span>
<span class="normal">2818</span>
<span class="normal">2819</span>
<span class="normal">2820</span>
<span class="normal">2821</span>
<span class="normal">2822</span>
<span class="normal">2823</span>
<span class="normal">2824</span>
<span class="normal">2825</span>
<span class="normal">2826</span>
<span class="normal">2827</span>
<span class="normal">2828</span>
<span class="normal">2829</span>
<span class="normal">2830</span>
<span class="normal">2831</span>
<span class="normal">2832</span>
<span class="normal">2833</span>
<span class="normal">2834</span>
<span class="normal">2835</span>
<span class="normal">2836</span>
<span class="normal">2837</span>
<span class="normal">2838</span>
<span class="normal">2839</span>
<span class="normal">2840</span>
<span class="normal">2841</span>
<span class="normal">2842</span>
<span class="normal">2843</span>
<span class="normal">2844</span>
<span class="normal">2845</span>
<span class="normal">2846</span>
<span class="normal">2847</span>
<span class="normal">2848</span>
<span class="normal">2849</span>
<span class="normal">2850</span>
<span class="normal">2851</span>
<span class="normal">2852</span>
<span class="normal">2853</span>
<span class="normal">2854</span>
<span class="normal">2855</span>
<span class="normal">2856</span>
<span class="normal">2857</span>
<span class="normal">2858</span>
<span class="normal">2859</span>
<span class="normal">2860</span>
<span class="normal">2861</span>
<span class="normal">2862</span>
<span class="normal">2863</span>
<span class="normal">2864</span>
<span class="normal">2865</span>
<span class="normal">2866</span>
<span class="normal">2867</span>
<span class="normal">2868</span>
<span class="normal">2869</span>
<span class="normal">2870</span>
<span class="normal">2871</span>
<span class="normal">2872</span>
<span class="normal">2873</span>
<span class="normal">2874</span>
<span class="normal">2875</span>
<span class="normal">2876</span>
<span class="normal">2877</span>
<span class="normal">2878</span>
<span class="normal">2879</span>
<span class="normal">2880</span>
<span class="normal">2881</span>
<span class="normal">2882</span>
<span class="normal">2883</span>
<span class="normal">2884</span>
<span class="normal">2885</span>
<span class="normal">2886</span>
<span class="normal">2887</span>
<span class="normal">2888</span>
<span class="normal">2889</span>
<span class="normal">2890</span>
<span class="normal">2891</span>
<span class="normal">2892</span>
<span class="normal">2893</span>
<span class="normal">2894</span>
<span class="normal">2895</span>
<span class="normal">2896</span>
<span class="normal">2897</span>
<span class="normal">2898</span>
<span class="normal">2899</span>
<span class="normal">2900</span>
<span class="normal">2901</span>
<span class="normal">2902</span>
<span class="normal">2903</span>
<span class="normal">2904</span>
<span class="normal">2905</span>
<span class="normal">2906</span>
<span class="normal">2907</span>
<span class="normal">2908</span>
<span class="normal">2909</span>
<span class="normal">2910</span>
<span class="normal">2911</span>
<span class="normal">2912</span>
<span class="normal">2913</span>
<span class="normal">2914</span>
<span class="normal">2915</span>
<span class="normal">2916</span>
<span class="normal">2917</span>
<span class="normal">2918</span>
<span class="normal">2919</span>
<span class="normal">2920</span>
<span class="normal">2921</span>
<span class="normal">2922</span>
<span class="normal">2923</span>
<span class="normal">2924</span>
<span class="normal">2925</span>
<span class="normal">2926</span>
<span class="normal">2927</span>
<span class="normal">2928</span>
<span class="normal">2929</span>
<span class="normal">2930</span>
<span class="normal">2931</span>
<span class="normal">2932</span>
<span class="normal">2933</span>
<span class="normal">2934</span>
<span class="normal">2935</span>
<span class="normal">2936</span>
<span class="normal">2937</span>
<span class="normal">2938</span>
<span class="normal">2939</span>
<span class="normal">2940</span>
<span class="normal">2941</span>
<span class="normal">2942</span>
<span class="normal">2943</span>
<span class="normal">2944</span>
<span class="normal">2945</span>
<span class="normal">2946</span>
<span class="normal">2947</span>
<span class="normal">2948</span>
<span class="normal">2949</span>
<span class="normal">2950</span>
<span class="normal">2951</span>
<span class="normal">2952</span>
<span class="normal">2953</span>
<span class="normal">2954</span>
<span class="normal">2955</span>
<span class="normal">2956</span>
<span class="normal">2957</span>
<span class="normal">2958</span>
<span class="normal">2959</span>
<span class="normal">2960</span>
<span class="normal">2961</span>
<span class="normal">2962</span>
<span class="normal">2963</span>
<span class="normal">2964</span>
<span class="normal">2965</span>
<span class="normal">2966</span>
<span class="normal">2967</span>
<span class="normal">2968</span>
<span class="normal">2969</span>
<span class="normal">2970</span>
<span class="normal">2971</span>
<span class="normal">2972</span>
<span class="normal">2973</span>
<span class="normal">2974</span>
<span class="normal">2975</span>
<span class="normal">2976</span>
<span class="normal">2977</span>
<span class="normal">2978</span>
<span class="normal">2979</span>
<span class="normal">2980</span>
<span class="normal">2981</span>
<span class="normal">2982</span>
<span class="normal">2983</span>
<span class="normal">2984</span>
<span class="normal">2985</span>
<span class="normal">2986</span>
<span class="normal">2987</span>
<span class="normal">2988</span>
<span class="normal">2989</span>
<span class="normal">2990</span>
<span class="normal">2991</span>
<span class="normal">2992</span>
<span class="normal">2993</span>
<span class="normal">2994</span>
<span class="normal">2995</span>
<span class="normal">2996</span>
<span class="normal">2997</span>
<span class="normal">2998</span>
<span class="normal">2999</span>
<span class="normal">3000</span>
<span class="normal">3001</span>
<span class="normal">3002</span>
<span class="normal">3003</span>
<span class="normal">3004</span>
<span class="normal">3005</span>
<span class="normal">3006</span>
<span class="normal">3007</span>
<span class="normal">3008</span>
<span class="normal">3009</span>
<span class="normal">3010</span>
<span class="normal">3011</span>
<span class="normal">3012</span>
<span class="normal">3013</span>
<span class="normal">3014</span>
<span class="normal">3015</span>
<span class="normal">3016</span>
<span class="normal">3017</span>
<span class="normal">3018</span>
<span class="normal">3019</span>
<span class="normal">3020</span>
<span class="normal">3021</span>
<span class="normal">3022</span>
<span class="normal">3023</span>
<span class="normal">3024</span>
<span class="normal">3025</span>
<span class="normal">3026</span>
<span class="normal">3027</span>
<span class="normal">3028</span>
<span class="normal">3029</span>
<span class="normal">3030</span>
<span class="normal">3031</span>
<span class="normal">3032</span>
<span class="normal">3033</span>
<span class="normal">3034</span>
<span class="normal">3035</span>
<span class="normal">3036</span>
<span class="normal">3037</span>
<span class="normal">3038</span>
<span class="normal">3039</span>
<span class="normal">3040</span>
<span class="normal">3041</span>
<span class="normal">3042</span>
<span class="normal">3043</span>
<span class="normal">3044</span>
<span class="normal">3045</span>
<span class="normal">3046</span>
<span class="normal">3047</span>
<span class="normal">3048</span>
<span class="normal">3049</span>
<span class="normal">3050</span>
<span class="normal">3051</span>
<span class="normal">3052</span>
<span class="normal">3053</span>
<span class="normal">3054</span>
<span class="normal">3055</span>
<span class="normal">3056</span>
<span class="normal">3057</span>
<span class="normal">3058</span>
<span class="normal">3059</span>
<span class="normal">3060</span>
<span class="normal">3061</span>
<span class="normal">3062</span>
<span class="normal">3063</span>
<span class="normal">3064</span>
<span class="normal">3065</span>
<span class="normal">3066</span>
<span class="normal">3067</span>
<span class="normal">3068</span>
<span class="normal">3069</span>
<span class="normal">3070</span>
<span class="normal">3071</span>
<span class="normal">3072</span>
<span class="normal">3073</span>
<span class="normal">3074</span>
<span class="normal">3075</span>
<span class="normal">3076</span>
<span class="normal">3077</span>
<span class="normal">3078</span>
<span class="normal">3079</span>
<span class="normal">3080</span>
<span class="normal">3081</span>
<span class="normal">3082</span>
<span class="normal">3083</span>
<span class="normal">3084</span>
<span class="normal">3085</span>
<span class="normal">3086</span>
<span class="normal">3087</span>
<span class="normal">3088</span>
<span class="normal">3089</span>
<span class="normal">3090</span>
<span class="normal">3091</span>
<span class="normal">3092</span>
<span class="normal">3093</span>
<span class="normal">3094</span>
<span class="normal">3095</span>
<span class="normal">3096</span>
<span class="normal">3097</span>
<span class="normal">3098</span>
<span class="normal">3099</span>
<span class="normal">3100</span>
<span class="normal">3101</span>
<span class="normal">3102</span>
<span class="normal">3103</span>
<span class="normal">3104</span>
<span class="normal">3105</span>
<span class="normal">3106</span>
<span class="normal">3107</span>
<span class="normal">3108</span>
<span class="normal">3109</span>
<span class="normal">3110</span>
<span class="normal">3111</span>
<span class="normal">3112</span>
<span class="normal">3113</span>
<span class="normal">3114</span>
<span class="normal">3115</span>
<span class="normal">3116</span>
<span class="normal">3117</span>
<span class="normal">3118</span>
<span class="normal">3119</span>
<span class="normal">3120</span>
<span class="normal">3121</span>
<span class="normal">3122</span>
<span class="normal">3123</span>
<span class="normal">3124</span>
<span class="normal">3125</span>
<span class="normal">3126</span>
<span class="normal">3127</span>
<span class="normal">3128</span>
<span class="normal">3129</span>
<span class="normal">3130</span>
<span class="normal">3131</span>
<span class="normal">3132</span>
<span class="normal">3133</span>
<span class="normal">3134</span>
<span class="normal">3135</span>
<span class="normal">3136</span>
<span class="normal">3137</span>
<span class="normal">3138</span>
<span class="normal">3139</span>
<span class="normal">3140</span>
<span class="normal">3141</span>
<span class="normal">3142</span>
<span class="normal">3143</span>
<span class="normal">3144</span>
<span class="normal">3145</span>
<span class="normal">3146</span>
<span class="normal">3147</span>
<span class="normal">3148</span>
<span class="normal">3149</span>
<span class="normal">3150</span>
<span class="normal">3151</span>
<span class="normal">3152</span>
<span class="normal">3153</span>
<span class="normal">3154</span>
<span class="normal">3155</span>
<span class="normal">3156</span>
<span class="normal">3157</span>
<span class="normal">3158</span>
<span class="normal">3159</span>
<span class="normal">3160</span>
<span class="normal">3161</span>
<span class="normal">3162</span>
<span class="normal">3163</span>
<span class="normal">3164</span>
<span class="normal">3165</span>
<span class="normal">3166</span>
<span class="normal">3167</span>
<span class="normal">3168</span>
<span class="normal">3169</span>
<span class="normal">3170</span>
<span class="normal">3171</span>
<span class="normal">3172</span>
<span class="normal">3173</span>
<span class="normal">3174</span>
<span class="normal">3175</span>
<span class="normal">3176</span>
<span class="normal">3177</span>
<span class="normal">3178</span>
<span class="normal">3179</span>
<span class="normal">3180</span>
<span class="normal">3181</span>
<span class="normal">3182</span>
<span class="normal">3183</span>
<span class="normal">3184</span>
<span class="normal">3185</span>
<span class="normal">3186</span>
<span class="normal">3187</span>
<span class="normal">3188</span>
<span class="normal">3189</span>
<span class="normal">3190</span>
<span class="normal">3191</span>
<span class="normal">3192</span>
<span class="normal">3193</span>
<span class="normal">3194</span>
<span class="normal">3195</span>
<span class="normal">3196</span>
<span class="normal">3197</span>
<span class="normal">3198</span>
<span class="normal">3199</span>
<span class="normal">3200</span>
<span class="normal">3201</span>
<span class="normal">3202</span>
<span class="normal">3203</span>
<span class="normal">3204</span>
<span class="normal">3205</span>
<span class="normal">3206</span>
<span class="normal">3207</span>
<span class="normal">3208</span>
<span class="normal">3209</span>
<span class="normal">3210</span>
<span class="normal">3211</span>
<span class="normal">3212</span>
<span class="normal">3213</span>
<span class="normal">3214</span>
<span class="normal">3215</span>
<span class="normal">3216</span>
<span class="normal">3217</span>
<span class="normal">3218</span>
<span class="normal">3219</span>
<span class="normal">3220</span>
<span class="normal">3221</span>
<span class="normal">3222</span>
<span class="normal">3223</span>
<span class="normal">3224</span>
<span class="normal">3225</span>
<span class="normal">3226</span>
<span class="normal">3227</span>
<span class="normal">3228</span>
<span class="normal">3229</span>
<span class="normal">3230</span>
<span class="normal">3231</span>
<span class="normal">3232</span>
<span class="normal">3233</span>
<span class="normal">3234</span>
<span class="normal">3235</span>
<span class="normal">3236</span>
<span class="normal">3237</span>
<span class="normal">3238</span>
<span class="normal">3239</span>
<span class="normal">3240</span>
<span class="normal">3241</span>
<span class="normal">3242</span>
<span class="normal">3243</span>
<span class="normal">3244</span>
<span class="normal">3245</span>
<span class="normal">3246</span>
<span class="normal">3247</span>
<span class="normal">3248</span>
<span class="normal">3249</span>
<span class="normal">3250</span>
<span class="normal">3251</span>
<span class="normal">3252</span>
<span class="normal">3253</span>
<span class="normal">3254</span>
<span class="normal">3255</span>
<span class="normal">3256</span>
<span class="normal">3257</span>
<span class="normal">3258</span>
<span class="normal">3259</span>
<span class="normal">3260</span>
<span class="normal">3261</span>
<span class="normal">3262</span>
<span class="normal">3263</span>
<span class="normal">3264</span>
<span class="normal">3265</span>
<span class="normal">3266</span>
<span class="normal">3267</span>
<span class="normal">3268</span>
<span class="normal">3269</span>
<span class="normal">3270</span>
<span class="normal">3271</span>
<span class="normal">3272</span>
<span class="normal">3273</span>
<span class="normal">3274</span>
<span class="normal">3275</span>
<span class="normal">3276</span>
<span class="normal">3277</span>
<span class="normal">3278</span>
<span class="normal">3279</span>
<span class="normal">3280</span>
<span class="normal">3281</span>
<span class="normal">3282</span>
<span class="normal">3283</span>
<span class="normal">3284</span>
<span class="normal">3285</span>
<span class="normal">3286</span>
<span class="normal">3287</span>
<span class="normal">3288</span>
<span class="normal">3289</span>
<span class="normal">3290</span>
<span class="normal">3291</span>
<span class="normal">3292</span>
<span class="normal">3293</span>
<span class="normal">3294</span>
<span class="normal">3295</span>
<span class="normal">3296</span>
<span class="normal">3297</span>
<span class="normal">3298</span>
<span class="normal">3299</span>
<span class="normal">3300</span>
<span class="normal">3301</span>
<span class="normal">3302</span>
<span class="normal">3303</span>
<span class="normal">3304</span>
<span class="normal">3305</span>
<span class="normal">3306</span>
<span class="normal">3307</span>
<span class="normal">3308</span>
<span class="normal">3309</span>
<span class="normal">3310</span>
<span class="normal">3311</span>
<span class="normal">3312</span>
<span class="normal">3313</span>
<span class="normal">3314</span>
<span class="normal">3315</span>
<span class="normal">3316</span>
<span class="normal">3317</span>
<span class="normal">3318</span>
<span class="normal">3319</span>
<span class="normal">3320</span>
<span class="normal">3321</span>
<span class="normal">3322</span>
<span class="normal">3323</span>
<span class="normal">3324</span>
<span class="normal">3325</span>
<span class="normal">3326</span>
<span class="normal">3327</span>
<span class="normal">3328</span>
<span class="normal">3329</span>
<span class="normal">3330</span>
<span class="normal">3331</span>
<span class="normal">3332</span>
<span class="normal">3333</span>
<span class="normal">3334</span>
<span class="normal">3335</span>
<span class="normal">3336</span>
<span class="normal">3337</span>
<span class="normal">3338</span>
<span class="normal">3339</span>
<span class="normal">3340</span>
<span class="normal">3341</span>
<span class="normal">3342</span>
<span class="normal">3343</span>
<span class="normal">3344</span>
<span class="normal">3345</span>
<span class="normal">3346</span>
<span class="normal">3347</span>
<span class="normal">3348</span>
<span class="normal">3349</span>
<span class="normal">3350</span>
<span class="normal">3351</span>
<span class="normal">3352</span>
<span class="normal">3353</span>
<span class="normal">3354</span>
<span class="normal">3355</span>
<span class="normal">3356</span>
<span class="normal">3357</span>
<span class="normal">3358</span>
<span class="normal">3359</span>
<span class="normal">3360</span>
<span class="normal">3361</span>
<span class="normal">3362</span>
<span class="normal">3363</span>
<span class="normal">3364</span>
<span class="normal">3365</span>
<span class="normal">3366</span>
<span class="normal">3367</span>
<span class="normal">3368</span>
<span class="normal">3369</span>
<span class="normal">3370</span>
<span class="normal">3371</span>
<span class="normal">3372</span>
<span class="normal">3373</span>
<span class="normal">3374</span>
<span class="normal">3375</span>
<span class="normal">3376</span>
<span class="normal">3377</span>
<span class="normal">3378</span>
<span class="normal">3379</span>
<span class="normal">3380</span>
<span class="normal">3381</span>
<span class="normal">3382</span>
<span class="normal">3383</span>
<span class="normal">3384</span>
<span class="normal">3385</span>
<span class="normal">3386</span>
<span class="normal">3387</span>
<span class="normal">3388</span>
<span class="normal">3389</span>
<span class="normal">3390</span>
<span class="normal">3391</span>
<span class="normal">3392</span>
<span class="normal">3393</span>
<span class="normal">3394</span>
<span class="normal">3395</span>
<span class="normal">3396</span>
<span class="normal">3397</span>
<span class="normal">3398</span>
<span class="normal">3399</span>
<span class="normal">3400</span>
<span class="normal">3401</span>
<span class="normal">3402</span>
<span class="normal">3403</span>
<span class="normal">3404</span>
<span class="normal">3405</span>
<span class="normal">3406</span>
<span class="normal">3407</span>
<span class="normal">3408</span>
<span class="normal">3409</span>
<span class="normal">3410</span>
<span class="normal">3411</span>
<span class="normal">3412</span>
<span class="normal">3413</span>
<span class="normal">3414</span>
<span class="normal">3415</span>
<span class="normal">3416</span>
<span class="normal">3417</span>
<span class="normal">3418</span>
<span class="normal">3419</span>
<span class="normal">3420</span>
<span class="normal">3421</span>
<span class="normal">3422</span>
<span class="normal">3423</span>
<span class="normal">3424</span>
<span class="normal">3425</span>
<span class="normal">3426</span>
<span class="normal">3427</span>
<span class="normal">3428</span>
<span class="normal">3429</span>
<span class="normal">3430</span>
<span class="normal">3431</span>
<span class="normal">3432</span>
<span class="normal">3433</span>
<span class="normal">3434</span>
<span class="normal">3435</span>
<span class="normal">3436</span>
<span class="normal">3437</span>
<span class="normal">3438</span>
<span class="normal">3439</span>
<span class="normal">3440</span>
<span class="normal">3441</span>
<span class="normal">3442</span>
<span class="normal">3443</span>
<span class="normal">3444</span>
<span class="normal">3445</span>
<span class="normal">3446</span>
<span class="normal">3447</span>
<span class="normal">3448</span>
<span class="normal">3449</span>
<span class="normal">3450</span>
<span class="normal">3451</span>
<span class="normal">3452</span>
<span class="normal">3453</span>
<span class="normal">3454</span>
<span class="normal">3455</span>
<span class="normal">3456</span>
<span class="normal">3457</span>
<span class="normal">3458</span>
<span class="normal">3459</span>
<span class="normal">3460</span>
<span class="normal">3461</span>
<span class="normal">3462</span>
<span class="normal">3463</span>
<span class="normal">3464</span>
<span class="normal">3465</span>
<span class="normal">3466</span>
<span class="normal">3467</span>
<span class="normal">3468</span>
<span class="normal">3469</span>
<span class="normal">3470</span>
<span class="normal">3471</span>
<span class="normal">3472</span>
<span class="normal">3473</span>
<span class="normal">3474</span>
<span class="normal">3475</span>
<span class="normal">3476</span>
<span class="normal">3477</span>
<span class="normal">3478</span>
<span class="normal">3479</span>
<span class="normal">3480</span>
<span class="normal">3481</span>
<span class="normal">3482</span>
<span class="normal">3483</span>
<span class="normal">3484</span>
<span class="normal">3485</span>
<span class="normal">3486</span>
<span class="normal">3487</span>
<span class="normal">3488</span>
<span class="normal">3489</span>
<span class="normal">3490</span>
<span class="normal">3491</span>
<span class="normal">3492</span>
<span class="normal">3493</span>
<span class="normal">3494</span>
<span class="normal">3495</span>
<span class="normal">3496</span>
<span class="normal">3497</span>
<span class="normal">3498</span>
<span class="normal">3499</span>
<span class="normal">3500</span>
<span class="normal">3501</span>
<span class="normal">3502</span>
<span class="normal">3503</span>
<span class="normal">3504</span>
<span class="normal">3505</span>
<span class="normal">3506</span>
<span class="normal">3507</span>
<span class="normal">3508</span>
<span class="normal">3509</span>
<span class="normal">3510</span>
<span class="normal">3511</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">PreTrainedModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">ModuleUtilsMixin</span><span class="p">,</span> <span class="n">GenerationMixin</span><span class="p">,</span> <span class="n">PeftAdapterMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for all models.</span>

<span class="sd">    [`PreTrainedModel`] takes care of storing the configuration of the models and handles methods for loading,</span>
<span class="sd">    downloading and saving models as well as a few methods common to all models to:</span>

<span class="sd">        - resize the input embeddings,</span>
<span class="sd">        - prune heads in the self-attention heads.</span>

<span class="sd">    Class attributes (overridden by derived classes):</span>

<span class="sd">        - **config_class** ([`PretrainedConfig`]) -- A subclass of [`PretrainedConfig`] to use as configuration class</span>
<span class="sd">          for this model architecture.</span>
<span class="sd">        - **load_tf_weights** (`Callable`) -- A python *method* for loading a TensorFlow checkpoint in a PyTorch model,</span>
<span class="sd">          taking as arguments:</span>

<span class="sd">            - **model** ([`PreTrainedModel`]) -- An instance of the model on which to load the TensorFlow checkpoint.</span>
<span class="sd">            - **config** ([`PreTrainedConfig`]) -- An instance of the configuration associated to the model.</span>
<span class="sd">            - **path** (`str`) -- A path to the TensorFlow checkpoint.</span>

<span class="sd">        - **base_model_prefix** (`str`) -- A string indicating the attribute associated to the base model in derived</span>
<span class="sd">          classes of the same architecture adding modules on top of the base model.</span>
<span class="sd">        - **is_parallelizable** (`bool`) -- A flag indicating whether this model supports model parallelization.</span>
<span class="sd">        - **main_input_name** (`str`) -- The name of the principal input to the model (often `input_ids` for NLP</span>
<span class="sd">          models, `pixel_values` for vision models and `input_values` for speech models).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">config_class</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="n">main_input_name</span> <span class="o">=</span> <span class="s2">&quot;input_ids&quot;</span>
    <span class="n">model_tags</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">_auto_class</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">_skip_keys_device_placement</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">_keep_in_fp32_modules</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># a list of `re` patterns of `state_dict` keys that should be removed from the list of missing</span>
    <span class="c1"># keys we find (keys inside the model but not in the checkpoint) and avoid unnecessary warnings.</span>
    <span class="n">_keys_to_ignore_on_load_missing</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># a list of `re` patterns of `state_dict` keys that should be removed from the list of</span>
    <span class="c1"># unexpected keys we find (keys inside the checkpoint but not the model) and avoid unnecessary</span>
    <span class="c1"># warnings.</span>
    <span class="n">_keys_to_ignore_on_load_unexpected</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># a list of `state_dict` keys to ignore when saving the model (useful for keys that aren&#39;t</span>
    <span class="c1"># trained, but which are either deterministic or tied variables)</span>
    <span class="n">_keys_to_ignore_on_save</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># a list of `state_dict` keys that are potentially tied to another key in the state_dict.</span>
    <span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">is_parallelizable</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">supports_gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">_is_stateful</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># Flash Attention 2 support</span>
    <span class="n">_supports_flash_attn_2</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># SDPA support</span>
    <span class="n">_supports_sdpa</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># Has support for a `Cache` instance as `past_key_values`? Does it support a `StaticCache`?</span>
    <span class="n">_supports_cache_class</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">_supports_static_cache</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># Has support for a `QuantoQuantizedCache` instance as `past_key_values`</span>
    <span class="n">_supports_quantized_cache</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dummy_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `Dict[str, mindspore.Tensor]`: Dummy inputs to do a forward pass in the network.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">DUMMY_INPUTS</span><span class="p">)}</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">framework</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :str: Identifies that this is a PyTorch model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;ms&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">PretrainedConfig</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PretrainedConfig</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Parameter config in `</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(config)` should be an instance of class &quot;</span>
                <span class="s2">&quot;`PretrainedConfig`. To create a model from a pretrained model use &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`model = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.from_pretrained(PRETRAINED_MODEL_NAME)`&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Save config and origin of the pretrained weights if given in model</span>
        <span class="n">config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_autoset_attn_implementation</span><span class="p">(</span>
            <span class="n">config</span><span class="p">,</span> <span class="n">ms_dtype</span><span class="o">=</span><span class="n">get_default_dtype</span><span class="p">(),</span> <span class="n">check_device_map</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name_or_path</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">name_or_path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warnings_issued</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_model_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">can_generate</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="c1"># Overwrite the class attribute to make it an instance attribute, so models like</span>
        <span class="c1"># `InstructBlipForConditionalGeneration` can dynamically update it without modifying the class attribute</span>
        <span class="c1"># when a different component (e.g. language_model) is used.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_keep_in_fp32_modules</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">_keep_in_fp32_modules</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">post_init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A method executed at the end of each Transformer model initialization, to execute code that needs the model&#39;s</span>
<span class="sd">        modules properly initialized (such as weight initialization).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_compatibility_gradient_checkpointing</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">dequantize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Potentially dequantize the model in case it has been quantized by a quantization method that support</span>
<span class="sd">        dequantization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">hf_quantizer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;hf_quantizer&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">hf_quantizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You need to first quantize your model in order to dequantize it&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hf_quantizer</span><span class="o">.</span><span class="n">dequantize</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_backward_compatibility_gradient_checkpointing</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">supports_gradient_checkpointing</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;gradient_checkpointing&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
            <span class="c1"># Remove the attribute now that is has been consumed, so it&#39;s no saved in the config.</span>
            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;gradient_checkpointing&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_model_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tags</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add custom tags into the model that gets pushed to the Hugging Face Hub. Will</span>
<span class="sd">        not overwrite existing tags in the model.</span>

<span class="sd">        Args:</span>
<span class="sd">            tags (`Union[List[str], str]`):</span>
<span class="sd">                The desired tags to inject in the model</span>

<span class="sd">        Examples:</span>

<span class="sd">        ```python</span>
<span class="sd">        from transformers import AutoModel</span>

<span class="sd">        model = AutoModel.from_pretrained(&quot;google-bert/bert-base-cased&quot;)</span>

<span class="sd">        model.add_model_tags([&quot;custom&quot;, &quot;custom-bert&quot;])</span>

<span class="sd">        # Push the model to your namespace with the name &quot;my-custom-bert&quot;.</span>
<span class="sd">        model.push_to_hub(&quot;my-custom-bert&quot;)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tags</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">tags</span> <span class="o">=</span> <span class="p">[</span><span class="n">tags</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_tags</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_tags</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">tag</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_tags</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model_tags</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_from_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        All context managers that the model should be initialized under go here.</span>

<span class="sd">        Args:</span>
<span class="sd">            ms_dtype (`mindspore.dtype.TensorType`, *optional*):</span>
<span class="sd">                Override the default `mindspore.dtype.TensorType` and load the model under this dtype.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ms_dtype</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;ms_dtype&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_flash_attention_2</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_flash_attention_2&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="c1"># override default dtype if needed</span>
        <span class="n">dtype_orig</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">ms_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dtype_orig</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_set_default_ms_dtype</span><span class="p">(</span><span class="n">ms_dtype</span><span class="p">)</span>

        <span class="n">config</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># We do not want to modify the config inplace in _from_config.</span>
        <span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">config</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_autoset_attn_implementation</span><span class="p">(</span>
            <span class="n">config</span><span class="p">,</span>
            <span class="n">use_flash_attention_2</span><span class="o">=</span><span class="n">use_flash_attention_2</span><span class="p">,</span>
            <span class="n">check_device_map</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">ms_dtype</span><span class="o">=</span><span class="n">ms_dtype</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># restore default dtype if it was modified</span>
        <span class="k">if</span> <span class="n">dtype_orig</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">set_default_dtype</span><span class="p">(</span><span class="n">dtype_orig</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">model</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_autoset_attn_implementation</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">config</span><span class="p">,</span>
        <span class="n">use_flash_attention_2</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">ms_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device_map</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">check_device_map</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Automatically checks and dispatches to a default attention implementation. In order of priority:</span>
<span class="sd">            1. An implementation specified in `config._attn_implementation` (due for example to the argument attn_implementation=&quot;sdpa&quot; in from_pretrained).</span>
<span class="sd">            2. DEPRECATED: if use_flash_attention_2 is set to `True` and `flash_attn` is available, flash attention. (`LlamaFlashAttention` for example)</span>
<span class="sd">            3. SDPA implementation, if available and supported by the model type. (`LlamaSdpaAttention` for example)</span>
<span class="sd">            4. The default model&#39;s implementation otherwise (`LlamaAttention` for example) .</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Here we use config._attn_implementation_internal to check whether the attention implementation was explicitely set by the user.</span>
        <span class="c1"># The property `PretrainedConfig._attn_implementation` is never `None`, for backward compatibility (always fall back on &quot;eager&quot;).</span>
        <span class="c1"># The `hasattr` here is used as some Transformers tests for some reason do not call PretrainedConfig __init__ (e.g. test_no_super_init_config_and_model)</span>
        <span class="n">requested_attn_implementation</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;_attn_implementation_internal&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation_internal</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">!=</span> <span class="s2">&quot;flash_attention_2&quot;</span> <span class="ow">and</span> <span class="n">use_flash_attention_2</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s1">&#39;Both attn_implementation=&quot;</span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span><span class="si">}</span><span class="s1">&quot; and `use_flash_attention_2=True` were used when loading the model, which are not compatible.&#39;</span>
                    <span class="s1">&#39; We recommend to just use `attn_implementation=&quot;flash_attention_2&quot;` when loading the model.&#39;</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;eager&quot;</span><span class="p">,</span> <span class="s2">&quot;sdpa&quot;</span><span class="p">,</span> <span class="s2">&quot;flash_attention_2&quot;</span><span class="p">]:</span>
                <span class="n">message</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Specified `attn_implementation=&quot;</span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span><span class="si">}</span><span class="s1">&quot;` is not supported. The only possible arguments are `attn_implementation=&quot;eager&quot;` (manual attention implementation)&#39;</span>
                <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_supports_flash_attn_2</span><span class="p">:</span>
                    <span class="n">message</span> <span class="o">+=</span> <span class="s1">&#39;, `&quot;attn_implementation=flash_attention_2&quot;` (implementation using flash attention 2)&#39;</span>
                <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_supports_sdpa</span><span class="p">:</span>
                    <span class="n">message</span> <span class="o">+=</span> <span class="s1">&#39;, `&quot;attn_implementation=sdpa&quot;` (implementation using nn.functional.scaled_dot_product_attention)&#39;</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">message</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>

            <span class="c1"># If a config is passed with a preset attn_implementation, we skip the automatic dispatch and use the user-provided config, with hard checks that the requested attention implementation is available.</span>
            <span class="n">requested_attn_implementation</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation_internal</span>

        <span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">=</span> <span class="s2">&quot;eager&quot;</span>

        <span class="k">return</span> <span class="n">config</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_set_default_ms_dtype</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Change the default dtype and return the previous one. This is needed when wanting to instantiate the model</span>
<span class="sd">        under specific dtype.</span>

<span class="sd">        Args:</span>
<span class="sd">            dtype (`mindspore.dtype.TensorType`):</span>
<span class="sd">                a floating dtype to set to.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `mindspore.dtype.TensorType`: the original `dtype` that can be used to restore `torch.set_default_dtype(dtype)` if it was</span>
<span class="sd">            modified. If it wasn&#39;t, returns `None`.</span>

<span class="sd">        Note `set_default_dtype` currently only works with floating-point types and asserts if for example,</span>
<span class="sd">        `torch.int64` is passed. So if a non-float `dtype` is passed this functions will throw an exception.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">typing</span><span class="o">.</span><span class="n">Float</span><span class="p">,</span> <span class="n">typing</span><span class="o">.</span><span class="n">BFloat</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Can&#39;t instantiate </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> model under dtype=</span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> since it is not a floating point dtype&quot;</span>
            <span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Instantiating </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> model under default dtype </span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">dtype_orig</span> <span class="o">=</span> <span class="n">get_default_dtype</span><span class="p">()</span>
        <span class="n">set_default_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dtype_orig</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">base_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `nn.Module`: The main body of the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">can_generate</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns whether this model can generate sequences with `.generate()`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `bool`: Whether this model can generate sequences with `.generate()`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Detects whether `prepare_inputs_for_generation` has been overwritten, which is a requirement for generation.</span>
        <span class="c1"># Alternativelly, the model can also have a custom `generate` function.</span>
        <span class="k">if</span> <span class="s2">&quot;GenerationMixin&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;GenerationMixin&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">generate</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">enable_input_require_grads</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while keeping</span>
<span class="sd">        the model weights fixed.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">make_inputs_require_grads</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
            <span class="n">output</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_require_grads_hook</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">make_inputs_require_grads</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">disable_input_require_grads</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Removes the `_require_grads_hook`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_require_grads_hook</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the model&#39;s input embeddings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `nn.Module`: A torch module mapping vocabulary to hidden states.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">base_model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">base_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">base_model</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set model&#39;s input embeddings.</span>

<span class="sd">        Args:</span>
<span class="sd">            value (`nn.Module`): A module mapping vocabulary to hidden states.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">base_model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">base_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">:</span>
            <span class="n">base_model</span><span class="o">.</span><span class="n">set_input_embeddings</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the model&#39;s output embeddings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `nn.Module`: A torch module mapping hidden states to vocabulary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Overwrite for models with output embeddings</span>

    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the weights. This method should be overridden by derived class and is</span>
<span class="sd">        the only initialization method that will be called when loading a checkpoint</span>
<span class="sd">        using `from_pretrained`. Any attempt to initialize outside of this function</span>
<span class="sd">        will be useless as the nn.init function are all replaced with skip.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the weights if they are not already initialized.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;_is_initialized&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
        <span class="n">module</span><span class="o">.</span><span class="n">_is_initialized</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">tie_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tie the weights between the input embeddings and the output embeddings.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;tie_word_embeddings&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
            <span class="n">output_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">output_embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="n">output_embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">())</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;is_encoder_decoder&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;tie_encoder_decoder&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">):</span>
                <span class="bp">self</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span> <span class="c1"># pylint: disable=self-cls-assignment</span>
            <span class="n">tied_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tie_encoder_decoder_weights</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="s2">&quot;encoder&quot;</span>
            <span class="p">)</span>
            <span class="c1"># Setting a dynamic variable instead of `_tied_weights_keys` because it&#39;s a class</span>
            <span class="c1"># attributed not an instance member, therefore modifying it will modify the entire class</span>
            <span class="c1"># Leading to issues on subsequent calls by different tests or subsequent calls.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_tied_weights_keys</span> <span class="o">=</span> <span class="n">tied_weights</span>

        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;_tie_weights&quot;</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">_tie_weights</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_tie_encoder_decoder_weights</span><span class="p">(</span>
        <span class="n">encoder</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">decoder</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">base_model_prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">base_encoder_name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="p">):</span>
        <span class="n">uninitialized_encoder_weights</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">tied_weights</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">decoder</span><span class="o">.</span><span class="vm">__class__</span> <span class="o">!=</span> <span class="n">encoder</span><span class="o">.</span><span class="vm">__class__</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">decoder</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">encoder</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2"> are not equal. In this case make sure that all encoder&quot;</span>
                <span class="s2">&quot; weights are correctly initialized.&quot;</span>
            <span class="p">)</span>

        <span class="k">def</span> <span class="nf">tie_encoder_to_decoder_recursively</span><span class="p">(</span>
            <span class="n">decoder_pointer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
            <span class="n">encoder_pointer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
            <span class="n">module_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
            <span class="n">base_encoder_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
            <span class="n">uninitialized_encoder_weights</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
            <span class="n">depth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">total_decoder_name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="n">total_encoder_name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">decoder_pointer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">encoder_pointer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">decoder_pointer</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">encoder_pointer</span><span class="si">}</span><span class="s2"> have to be of type nn.Module&quot;</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">decoder_pointer</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">encoder_pointer</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">)</span>
                <span class="n">encoder_pointer</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">decoder_pointer</span><span class="o">.</span><span class="n">weight</span>
                <span class="n">tied_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">base_encoder_name</span><span class="si">}{</span><span class="n">total_encoder_name</span><span class="si">}</span><span class="s2">.weight&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">decoder_pointer</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">):</span>
                    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">encoder_pointer</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">)</span>
                    <span class="n">tied_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">base_encoder_name</span><span class="si">}{</span><span class="n">total_encoder_name</span><span class="si">}</span><span class="s2">.bias&quot;</span><span class="p">)</span>
                    <span class="n">encoder_pointer</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">decoder_pointer</span><span class="o">.</span><span class="n">bias</span>
                <span class="k">return</span>

            <span class="n">encoder_modules</span> <span class="o">=</span> <span class="n">encoder_pointer</span><span class="o">.</span><span class="n">_modules</span>
            <span class="n">decoder_modules</span> <span class="o">=</span> <span class="n">decoder_pointer</span><span class="o">.</span><span class="n">_modules</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">decoder_modules</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">encoder_modules</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
                <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Encoder module </span><span class="si">{</span><span class="n">encoder_pointer</span><span class="si">}</span><span class="s2"> does not match decoder module </span><span class="si">{</span><span class="n">decoder_pointer</span><span class="si">}</span><span class="s2">&quot;</span>

                <span class="n">all_encoder_weights</span> <span class="o">=</span> <span class="p">{</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">sub_name</span> <span class="k">for</span> <span class="n">sub_name</span> <span class="ow">in</span> <span class="n">encoder_modules</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
                <span class="n">encoder_layer_pos</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">decoder_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">isdigit</span><span class="p">():</span>
                        <span class="n">encoder_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="o">+</span> <span class="n">encoder_layer_pos</span><span class="p">)</span>
                        <span class="n">decoder_name</span> <span class="o">=</span> <span class="n">name</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">decoder_modules</span><span class="p">[</span><span class="n">decoder_name</span><span class="p">],</span> <span class="nb">type</span><span class="p">(</span><span class="n">encoder_modules</span><span class="p">[</span><span class="n">encoder_name</span><span class="p">]))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span>
                            <span class="n">encoder_modules</span>
                        <span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">decoder_modules</span><span class="p">):</span>
                            <span class="c1"># this can happen if the name corresponds to the position in a list module list of layers</span>
                            <span class="c1"># in this case the decoder has added a cross-attention that the encoder does not have</span>
                            <span class="c1"># thus skip this step and subtract one layer pos from encoder</span>
                            <span class="n">encoder_layer_pos</span> <span class="o">-=</span> <span class="mi">1</span>
                            <span class="k">continue</span>
                    <span class="k">elif</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">encoder_modules</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="k">elif</span> <span class="n">depth</span> <span class="o">&gt;</span> <span class="mi">500</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;Max depth of recursive function `tie_encoder_to_decoder` reached. It seems that there is&quot;</span>
                            <span class="s2">&quot; a circular dependency between two or more `nn.Modules` of your model.&quot;</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">decoder_name</span> <span class="o">=</span> <span class="n">encoder_name</span> <span class="o">=</span> <span class="n">name</span>
                    <span class="n">tie_encoder_to_decoder_recursively</span><span class="p">(</span>
                        <span class="n">decoder_modules</span><span class="p">[</span><span class="n">decoder_name</span><span class="p">],</span>
                        <span class="n">encoder_modules</span><span class="p">[</span><span class="n">encoder_name</span><span class="p">],</span>
                        <span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">name</span><span class="p">,</span>
                        <span class="n">base_encoder_name</span><span class="p">,</span>
                        <span class="n">uninitialized_encoder_weights</span><span class="p">,</span>
                        <span class="n">depth</span><span class="o">=</span><span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                        <span class="n">total_encoder_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">total_encoder_name</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">encoder_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="n">total_decoder_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">total_decoder_name</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">decoder_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">all_encoder_weights</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">encoder_name</span><span class="p">)</span>

                <span class="n">uninitialized_encoder_weights</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="n">all_encoder_weights</span><span class="p">)</span>

        <span class="c1"># tie weights recursively</span>
        <span class="n">tie_encoder_to_decoder_recursively</span><span class="p">(</span>
            <span class="n">decoder</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">base_model_prefix</span><span class="p">,</span> <span class="n">base_encoder_name</span><span class="p">,</span> <span class="n">uninitialized_encoder_weights</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">uninitialized_encoder_weights</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The following encoder weights were not tied to the decoder </span><span class="si">{</span><span class="n">uninitialized_encoder_weights</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">tied_weights</span>

    <span class="k">def</span> <span class="nf">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_embeddings</span><span class="p">,</span> <span class="n">input_embeddings</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Tie or clone module weights&quot;&quot;&quot;</span>
        <span class="c1"># output_embeddings.weight = nn.Parameter(input_embeddings.weight.clone())</span>
        <span class="c1"># # else:</span>
        <span class="n">output_embeddings</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">input_embeddings</span><span class="o">.</span><span class="n">weight</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">output_embeddings</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">output_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">output_embeddings</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">new_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                    <span class="n">output_embeddings</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                    <span class="p">(</span>
                        <span class="mi">0</span><span class="p">,</span>
                        <span class="n">output_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">output_embeddings</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                    <span class="p">),</span>
                    <span class="s2">&quot;constant&quot;</span><span class="p">,</span>
                    <span class="mi">0</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_bias</span> <span class="o">=</span> <span class="n">output_embeddings</span><span class="o">.</span><span class="n">bias</span><span class="p">[:</span><span class="n">output_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
            <span class="n">output_embeddings</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">assign_value</span><span class="p">(</span><span class="n">new_bias</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">output_embeddings</span><span class="p">,</span> <span class="s2">&quot;out_features&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">input_embeddings</span><span class="p">,</span> <span class="s2">&quot;num_embeddings&quot;</span><span class="p">):</span>
            <span class="n">output_embeddings</span><span class="o">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">input_embeddings</span><span class="o">.</span><span class="n">num_embeddings</span>

    <span class="k">def</span> <span class="nf">_get_no_split_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_map</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the modules of the model that should not be spit when using device_map. We iterate through the modules to</span>
<span class="sd">        get the underlying `_no_split_modules`.</span>

<span class="sd">        Args:</span>
<span class="sd">            device_map (`str`):</span>
<span class="sd">                The device map value. Options are [&quot;auto&quot;, &quot;balanced&quot;, &quot;balanced_low_0&quot;, &quot;sequential&quot;]</span>

<span class="sd">        Returns:</span>
<span class="sd">            `List[str]`: List of modules that should not be split</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">modules_to_check</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">]</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">modules_to_check</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">module</span> <span class="o">=</span> <span class="n">modules_to_check</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># if the module does not appear in _no_split_modules, we also check the children</span>
            <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_no_split_modules</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">PreTrainedModel</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">_no_split_modules</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">module</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> does not support `device_map=&#39;</span><span class="si">{</span><span class="n">device_map</span><span class="si">}</span><span class="s2">&#39;`. To implement support, the model &quot;</span>
                            <span class="s2">&quot;class needs to implement the `_no_split_modules` attribute.&quot;</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="n">_no_split_modules</span> <span class="o">|</span> <span class="nb">set</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">_no_split_modules</span><span class="p">)</span>
                <span class="n">modules_to_check</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">children</span><span class="p">())</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">_no_split_modules</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">resize_token_embeddings</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.</span>

<span class="sd">        Takes care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            new_num_tokens (`int`, *optional*):</span>
<span class="sd">                The new number of tokens in the embedding matrix. Increasing the size will add newly initialized</span>
<span class="sd">                vectors at the end. Reducing the size will remove vectors from the end. If not provided or `None`, just</span>
<span class="sd">                returns a pointer to the input tokens `nn.Embedding` module of the model without doing anything.</span>
<span class="sd">            pad_to_multiple_of (`int`, *optional*):</span>
<span class="sd">                If set will pad the embedding matrix to a multiple of the provided value.If `new_num_tokens` is set to</span>
<span class="sd">                `None` will just pad the embedding to a multiple of `pad_to_multiple_of`.</span>

<span class="sd">                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</span>
<span class="sd">                `&gt;= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128. For more</span>
<span class="sd">                details about this, or help on choosing the correct value for resizing, refer to this guide:</span>
<span class="sd">                https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc</span>

<span class="sd">        Return:</span>
<span class="sd">            `nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_resize_token_embeddings</span><span class="p">(</span><span class="n">new_num_tokens</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">new_num_tokens</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pad_to_multiple_of</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">model_embeds</span>

        <span class="c1"># Since we are basically resuing the same old embeddings with new weight values, gathering is required</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">model_embeds</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Update base model and current model config</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;text_config&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">text_config</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>

        <span class="c1"># Tie weights again if needed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">model_embeds</span>

    <span class="k">def</span> <span class="nf">_resize_token_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">old_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>
        <span class="n">new_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_resized_embeddings</span><span class="p">(</span><span class="n">old_embeddings</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="p">)</span>
        <span class="c1"># if hasattr(old_embeddings, &quot;_hf_hook&quot;):</span>
        <span class="c1">#     hook = old_embeddings._hf_hook</span>
        <span class="c1">#     add_hook_to_module(new_embeddings, hook)</span>
        <span class="n">old_embeddings_requires_grad</span> <span class="o">=</span> <span class="n">old_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="n">new_embeddings</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">old_embeddings_requires_grad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_input_embeddings</span><span class="p">(</span><span class="n">new_embeddings</span><span class="p">)</span>

        <span class="c1"># Update new_num_tokens with the actual size of new_embeddings</span>
        <span class="k">if</span> <span class="n">pad_to_multiple_of</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">new_num_tokens</span> <span class="o">=</span> <span class="n">new_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># if word embeddings are not tied, make sure that lm head is resized as well</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">tie_word_embeddings</span><span class="p">:</span>
            <span class="n">old_lm_head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_lm_head</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
                <span class="n">new_lm_head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_resized_embeddings</span><span class="p">(</span><span class="n">old_lm_head</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_lm_head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_resized_lm_head</span><span class="p">(</span><span class="n">old_lm_head</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">)</span>
            <span class="c1"># if hasattr(old_lm_head, &quot;_hf_hook&quot;):</span>
            <span class="c1">#     hook = old_lm_head._hf_hook</span>
            <span class="c1">#     add_hook_to_module(new_lm_head, hook)</span>
            <span class="n">old_lm_head_requires_grad</span> <span class="o">=</span> <span class="n">old_lm_head</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="n">new_lm_head</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">old_lm_head_requires_grad</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_output_embeddings</span><span class="p">(</span><span class="n">new_lm_head</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_get_resized_embeddings</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">old_embeddings</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">,</span>
        <span class="n">new_num_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build a resized Embedding Module from a provided token Embedding Module. Increasing the size will add newly</span>
<span class="sd">        initialized vectors at the end. Reducing the size will remove vectors from the end</span>

<span class="sd">        Args:</span>
<span class="sd">            old_embeddings (`nn.Embedding`):</span>
<span class="sd">                Old embeddings to be resized.</span>
<span class="sd">            new_num_tokens (`int`, *optional*):</span>
<span class="sd">                New number of tokens in the embedding matrix.</span>

<span class="sd">                Increasing the size will add newly initialized vectors at the end. Reducing the size will remove</span>
<span class="sd">                vectors from the end. If not provided or `None`, just returns a pointer to the input tokens</span>
<span class="sd">                `nn.Embedding` module of the model without doing anything.</span>
<span class="sd">            pad_to_multiple_of (`int`, *optional*):</span>
<span class="sd">                If set will pad the embedding matrix to a multiple of the provided value. If `new_num_tokens` is set to</span>
<span class="sd">                `None` will just pad the embedding to a multiple of `pad_to_multiple_of`.</span>

<span class="sd">                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</span>
<span class="sd">                `&gt;= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128. For more</span>
<span class="sd">                details about this, or help on choosing the correct value for resizing, refer to this guide:</span>
<span class="sd">                https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc</span>


<span class="sd">        Return:</span>
<span class="sd">            `nn.Embedding`: Pointer to the resized Embedding Module or the old Embedding Module if</span>
<span class="sd">            `new_num_tokens` is `None`</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">pad_to_multiple_of</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad_to_multiple_of</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Asking to pad the embedding matrix to a multiple of `</span><span class="si">{</span><span class="n">pad_to_multiple_of</span><span class="si">}</span><span class="s2">`, which is not and integer. Please make sure to pass an integer&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">new_num_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">new_num_tokens</span> <span class="o">=</span> <span class="n">old_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">new_num_tokens</span> <span class="o">=</span> <span class="p">((</span><span class="n">new_num_tokens</span> <span class="o">+</span> <span class="n">pad_to_multiple_of</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">pad_to_multiple_of</span><span class="p">)</span> <span class="o">*</span> <span class="n">pad_to_multiple_of</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; dimension will be </span><span class="si">{</span><span class="n">new_num_tokens</span><span class="si">}</span><span class="s2">. This might induce some performance reduction as *Tensor Cores* will not be available.&quot;</span>
                <span class="s2">&quot; For more details about this, or help on choosing the correct value for resizing, refer to this guide:&quot;</span>
                <span class="s2">&quot; https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">new_num_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">old_embeddings</span>

        <span class="n">old_num_tokens</span><span class="p">,</span> <span class="n">old_embedding_dim</span> <span class="o">=</span> <span class="n">old_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="n">old_num_tokens</span> <span class="o">==</span> <span class="n">new_num_tokens</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">old_embeddings</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_embeddings</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Old embeddings are of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">old_embeddings</span><span class="p">)</span><span class="si">}</span><span class="s2">, which is not an instance of </span><span class="si">{</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="si">}</span><span class="s2">. You&quot;</span>
                <span class="s2">&quot; should either use a different resize function or make sure that `old_embeddings` are an instance of&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Build new embeddings</span>

        <span class="c1"># When using DeepSpeed ZeRO-3, we shouldn&#39;t create new embeddings with DeepSpeed init</span>
        <span class="c1"># because the shape of the new embedding layer is used across various modeling files</span>
        <span class="c1"># as well as to update config vocab size. Shape will be 0 when using DeepSpeed init leading</span>
        <span class="c1"># to errors when training.</span>
        <span class="n">new_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
            <span class="n">new_num_tokens</span><span class="p">,</span>
            <span class="n">old_embedding_dim</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">old_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># initialize all new embeddings (in particular added tokens)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="n">new_embeddings</span><span class="p">)</span>

        <span class="c1"># Copy token embeddings from the previous weights</span>

        <span class="c1"># numbers of tokens to copy</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">old_num_tokens</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">)</span>

        <span class="n">new_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="p">[:</span><span class="n">n</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">old_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="p">[:</span><span class="n">n</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># Replace weights in old_embeddings and return to maintain the same embedding type.</span>
        <span class="c1"># This ensures correct functionality when a Custom Embedding class is passed as input.</span>
        <span class="c1"># The input and output embedding types remain consistent. (c.f. https://github.com/huggingface/transformers/pull/31979)</span>

        <span class="n">old_embeddings</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">new_embeddings</span><span class="o">.</span><span class="n">weight</span>
        <span class="n">old_embeddings</span><span class="o">.</span><span class="n">num_embeddings</span> <span class="o">=</span> <span class="n">new_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">old_embeddings</span><span class="o">.</span><span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">new_num_tokens</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">old_embeddings</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">:</span>
            <span class="n">old_embeddings</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="n">old_embeddings</span>

    <span class="k">def</span> <span class="nf">_get_resized_lm_head</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">old_lm_head</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">transposed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build a resized Linear Module from a provided old Linear Module. Increasing the size will add newly initialized</span>
<span class="sd">        vectors at the end. Reducing the size will remove vectors from the end</span>

<span class="sd">        Args:</span>
<span class="sd">            old_lm_head (`nn.Linear`):</span>
<span class="sd">                Old lm head liner layer to be resized.</span>
<span class="sd">            new_num_tokens (`int`, *optional*):</span>
<span class="sd">                New number of tokens in the linear matrix.</span>

<span class="sd">                Increasing the size will add newly initialized vectors at the end. Reducing the size will remove</span>
<span class="sd">                vectors from the end. If not provided or `None`, just returns a pointer to the input tokens</span>
<span class="sd">                `nn.Linear` module of the model without doing anything. transposed (`bool`, *optional*, defaults</span>
<span class="sd">                to `False`): Whether `old_lm_head` is transposed or not. If True `old_lm_head.shape` is `lm_head_dim,</span>
<span class="sd">                vocab_size` else `vocab_size, lm_head_dim`.</span>

<span class="sd">        Return:</span>
<span class="sd">            `nn.Linear`: Pointer to the resized Linear Module or the old Linear Module if `new_num_tokens` is</span>
<span class="sd">            `None`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">new_num_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">old_lm_head</span>

        <span class="n">is_quantized</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;hf_quantizer&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_quantizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="n">old_num_tokens</span><span class="p">,</span> <span class="n">old_lm_head_dim</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">old_lm_head</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">transposed</span> <span class="k">else</span> <span class="n">old_lm_head</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">old_num_tokens</span> <span class="o">==</span> <span class="n">new_num_tokens</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">old_lm_head</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_lm_head</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Old language model head is of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">old_lm_head</span><span class="p">)</span><span class="si">}</span><span class="s2">, which is not an instance of </span><span class="si">{</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="si">}</span><span class="s2">. You&quot;</span>
                <span class="s2">&quot; should either use a different resize function or make sure that `old_lm_head` are an instance of&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Build new lm head</span>
        <span class="n">new_lm_head_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">old_lm_head_dim</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">transposed</span> <span class="k">else</span> <span class="p">(</span><span class="n">new_num_tokens</span><span class="p">,</span> <span class="n">old_lm_head_dim</span><span class="p">)</span>
        <span class="n">has_new_lm_head_bias</span> <span class="o">=</span> <span class="n">old_lm_head</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="c1"># When using DeepSpeed ZeRO-3, we shouldn&#39;t create new embeddings with DeepSpeed init</span>
        <span class="c1"># because the shape of the new embedding layer is used across various modeling files</span>
        <span class="c1"># as well as to update config vocab size. Shape will be 0 when using DeepSpeed init leading</span>
        <span class="c1"># to errors when training.</span>
        <span class="n">new_lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="o">*</span><span class="n">new_lm_head_shape</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">has_new_lm_head_bias</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">old_lm_head</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># initialize new lm head (in particular added tokens)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="n">new_lm_head</span><span class="p">)</span>

        <span class="n">num_tokens_to_copy</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">old_num_tokens</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_copy_lm_head_original_to_resized</span><span class="p">(</span>
            <span class="n">new_lm_head</span><span class="p">,</span> <span class="n">old_lm_head</span><span class="p">,</span> <span class="n">num_tokens_to_copy</span><span class="p">,</span> <span class="n">transposed</span><span class="p">,</span> <span class="n">has_new_lm_head_bias</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">new_lm_head</span>

    <span class="k">def</span> <span class="nf">_copy_lm_head_original_to_resized</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">new_lm_head</span><span class="p">,</span> <span class="n">old_lm_head</span><span class="p">,</span> <span class="n">num_tokens_to_copy</span><span class="p">,</span> <span class="n">transposed</span><span class="p">,</span> <span class="n">has_new_lm_head_bias</span>
    <span class="p">):</span>
        <span class="c1"># Copy old lm head weights to new lm head</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">transposed</span><span class="p">:</span>
            <span class="n">new_lm_head</span><span class="o">.</span><span class="n">weight</span><span class="p">[:</span><span class="n">num_tokens_to_copy</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">old_lm_head</span><span class="o">.</span><span class="n">weight</span><span class="p">[:</span><span class="n">num_tokens_to_copy</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_lm_head</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="p">:</span><span class="n">num_tokens_to_copy</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_lm_head</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="p">:</span><span class="n">num_tokens_to_copy</span><span class="p">]</span>

        <span class="c1"># Copy bias weights to new lm head</span>
        <span class="k">if</span> <span class="n">has_new_lm_head_bias</span><span class="p">:</span>
            <span class="n">new_lm_head</span><span class="o">.</span><span class="n">bias</span><span class="p">[:</span><span class="n">num_tokens_to_copy</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_lm_head</span><span class="o">.</span><span class="n">bias</span><span class="p">[:</span><span class="n">num_tokens_to_copy</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">resize_position_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_num_position_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;`resize_position_embeddings` is not implemented for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2">`. To implement it, you should &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;overwrite this method in the class </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2"> in `modeling_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__module__</span><span class="si">}</span><span class="s2">.py`&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_position_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">]]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;`get_position_embeddings` is not implemented for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2">`. To implement it, you should &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;overwrite this method in the class </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2"> in `modeling_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__module__</span><span class="si">}</span><span class="s2">.py`&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        If needed prunes and maybe initializes weights. If using a custom `PreTrainedModel`, you need to implement any</span>
<span class="sd">        initialization logic in `_init_weights`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Prune heads if needed</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">_init_weights</span><span class="p">:</span>
            <span class="c1"># Initialize weights</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">)</span>

            <span class="c1"># Tie weights should be skipped when not initializing all weights</span>
            <span class="c1"># since from_pretrained(...) calls tie weights anyways</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">heads_to_prune</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prunes heads of the base model.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            heads_to_prune (`Dict[int, List[int]]`):</span>
<span class="sd">                Dictionary with keys being selected layer indices (`int`) and associated values being the list of heads</span>
<span class="sd">                to prune in said layer (list of `int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on</span>
<span class="sd">                layer 1 and heads 2 and 3 on layer 2.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># save new sets of pruned heads as union of previously stored pruned heads and newly pruned heads</span>
        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">heads</span> <span class="ow">in</span> <span class="n">heads_to_prune</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">union_heads</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pruned_heads</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="p">[]))</span> <span class="o">|</span> <span class="nb">set</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">union_heads</span><span class="p">)</span>  <span class="c1"># Unfortunately we have to store it as list for JSON</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">_prune_heads</span><span class="p">(</span><span class="n">heads_to_prune</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">gradient_checkpointing_enable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient_checkpointing_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Activates gradient checkpointing for the current model.</span>

<span class="sd">        Note that in other frameworks this feature can be referred to as &quot;activation checkpointing&quot; or &quot;checkpoint</span>
<span class="sd">        activations&quot;.</span>

<span class="sd">        We pass the `__call__` method of the modules instead of `forward` because `__call__` attaches all the hooks of</span>
<span class="sd">        the module. https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2</span>

<span class="sd">        Args:</span>
<span class="sd">            gradient_checkpointing_kwargs (dict, *optional*):</span>
<span class="sd">                Additional keyword arguments passed along to the `torch.utils.checkpoint.checkpoint` function.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">supports_gradient_checkpointing</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> does not support gradient checkpointing.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">gradient_checkpointing_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">gradient_checkpointing_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;use_reentrant&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>

        <span class="k">if</span> <span class="n">GENERATOR_SEED</span><span class="p">:</span>
            <span class="n">gradient_checkpointing_func</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">recompute</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">def</span> <span class="nf">function</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="n">gradient_checkpointing_func</span> <span class="o">=</span> <span class="n">function</span>

        <span class="c1"># For old GC format (transformers &lt; 4.35.0) for models that live on the Hub</span>
        <span class="c1"># we will fall back to the overwritten `_set_gradient_checkpointing` method</span>
        <span class="n">_is_using_old_format</span> <span class="o">=</span> <span class="s2">&quot;value&quot;</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_set_gradient_checkpointing</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_using_old_format</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_gradient_checkpointing</span><span class="p">(</span><span class="n">enable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">gradient_checkpointing_func</span><span class="o">=</span><span class="n">gradient_checkpointing_func</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_set_gradient_checkpointing</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).&quot;</span>
                <span class="s2">&quot;Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_hf_peft_config_loaded&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="c1"># When using PEFT + gradient checkpointing + Trainer we need to make sure the input has requires_grad=True</span>
            <span class="c1"># we do it also on PEFT: https://github.com/huggingface/peft/blob/85013987aa82aa1af3da1236b6902556ce3e483e/src/peft/peft_model.py#L334</span>
            <span class="c1"># When training with PEFT, only LoRA layers will have requires grad set to True, but the output of frozen layers need to propagate</span>
            <span class="c1"># the gradients to make sure the gradient flows.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">enable_input_require_grads</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_set_gradient_checkpointing</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">gradient_checkpointing_func</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">is_gradient_checkpointing_set</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Apply it on the top-level module in case the top-level modules supports it</span>
        <span class="c1"># for example, LongT5Stack inherits from `PreTrainedModel`.</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;gradient_checkpointing&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_checkpointing_func</span> <span class="o">=</span> <span class="n">gradient_checkpointing_func</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="n">enable</span>
            <span class="n">is_gradient_checkpointing_set</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;gradient_checkpointing&quot;</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">_gradient_checkpointing_func</span> <span class="o">=</span> <span class="n">gradient_checkpointing_func</span>
                <span class="n">module</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="n">enable</span>
                <span class="n">is_gradient_checkpointing_set</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_gradient_checkpointing_set</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> is not compatible with gradient checkpointing. Make sure all the architecture support it by setting a boolean attribute&quot;</span>
                <span class="s2">&quot; `gradient_checkpointing` to modules of the model that uses checkpointing.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">gradient_checkpointing_disable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deactivates gradient checkpointing for the current model.</span>

<span class="sd">        Note that in other frameworks this feature can be referred to as &quot;activation checkpointing&quot; or &quot;checkpoint</span>
<span class="sd">        activations&quot;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">supports_gradient_checkpointing</span><span class="p">:</span>
            <span class="c1"># For old GC format (transformers &lt; 4.35.0) for models that live on the Hub</span>
            <span class="c1"># we will fall back to the overwritten `_set_gradient_checkpointing` methid</span>
            <span class="n">_is_using_old_format</span> <span class="o">=</span> <span class="s2">&quot;value&quot;</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_set_gradient_checkpointing</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_using_old_format</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_set_gradient_checkpointing</span><span class="p">(</span><span class="n">enable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).&quot;</span>
                    <span class="s2">&quot;Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.&quot;</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_set_gradient_checkpointing</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_hf_peft_config_loaded&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">disable_input_require_grads</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_gradient_checkpointing</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Whether gradient checkpointing is activated for this model or not.</span>

<span class="sd">        Note that in other frameworks this feature can be referred to as &quot;activation checkpointing&quot; or &quot;checkpoint</span>
<span class="sd">        activations&quot;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">any</span><span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="s2">&quot;gradient_checkpointing&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">m</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">save_pretrained</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">save_checkpoint</span><span class="p">,</span>
        <span class="n">push_to_hub</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">max_shard_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;5GB&quot;</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">variant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">token</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_peft_format</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save a model and its configuration file to a directory, so that it can be re-loaded using the</span>
<span class="sd">        [`~PreTrainedModel.from_pretrained`] class method.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to which to save. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful when in distributed training like</span>
<span class="sd">                TPUs and need to call this function on all processes. In this case, set `is_main_process=True` only on</span>
<span class="sd">                the main process to avoid race conditions.</span>
<span class="sd">            state_dict (nested dictionary of `mindspore.Tensor`):</span>
<span class="sd">                The state dictionary of the model to save. Will default to `self.state_dict()`, but can be used to only</span>
<span class="sd">                save parts of the model or if special precautions need to be taken when recovering the state dictionary</span>
<span class="sd">                of a model (like when using model parallelism).</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful on distributed training like TPUs when one</span>
<span class="sd">                need to replace `torch.save` by another method.</span>
<span class="sd">            push_to_hub (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the</span>
<span class="sd">                repository you want to push to with `repo_id` (will default to the name of `save_directory` in your</span>
<span class="sd">                namespace).</span>
<span class="sd">            max_shard_size (`int` or `str`, *optional*, defaults to `&quot;5GB&quot;`):</span>
<span class="sd">                The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size</span>
<span class="sd">                lower than this size. If expressed as a string, needs to be digits followed by a unit (like `&quot;5MB&quot;`).</span>
<span class="sd">                We default it to 5GB in order for models to be able to run easily on free-tier google colab instances</span>
<span class="sd">                without CPU OOM issues.</span>

<span class="sd">                &lt;Tip warning={true}&gt;</span>

<span class="sd">                If a single weight of the model is bigger than `max_shard_size`, it will be in its own checkpoint shard</span>
<span class="sd">                which will be bigger than `max_shard_size`.</span>

<span class="sd">                &lt;/Tip&gt;</span>

<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).</span>
<span class="sd">            variant (`str`, *optional*):</span>
<span class="sd">                If specified, weights are saved in the format pytorch_model.&lt;variant&gt;.bin.</span>
<span class="sd">            token (`str` or `bool`, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use</span>
<span class="sd">                the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).</span>
<span class="sd">            save_peft_format (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                For backward compatibility with PEFT library, in case adapter weights are attached to the model, all</span>
<span class="sd">                keys of the state dict of adapters needs to be pre-pended with `base_model.model`. Advanced users can</span>
<span class="sd">                disable this behaviours by setting `save_peft_format` to `False`.</span>
<span class="sd">            kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">                Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">use_auth_token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_auth_token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">ignore_metadata_errors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;ignore_metadata_errors&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">use_auth_token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The `use_auth_token` argument is deprecated. Please use `token` instead.&quot;</span><span class="p">,</span>
                <span class="ne">FutureWarning</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`token` and `use_auth_token` are both specified. Please set only the argument `token`.&quot;</span>
                <span class="p">)</span>
            <span class="n">token</span> <span class="o">=</span> <span class="n">use_auth_token</span>

        <span class="k">if</span> <span class="n">token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">token</span>

        <span class="n">_hf_peft_config_loaded</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_hf_peft_config_loaded&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;save_config&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;`save_config` is deprecated. Use `is_main_process` instead.&quot;</span>
            <span class="p">)</span>
            <span class="n">is_main_process</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;save_config&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">safe_serialization</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_safetensors_available</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;`safe_serialization` requires the `safetensors library: `pip install safetensors`.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Provided path (</span><span class="si">{</span><span class="n">save_directory</span><span class="si">}</span><span class="s2">) should be a directory, not a file&quot;</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">push_to_hub</span><span class="p">:</span>
            <span class="n">commit_message</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;commit_message&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">repo_id</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;repo_id&quot;</span><span class="p">,</span> <span class="n">save_directory</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">sep</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">repo_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_repo</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">files_timestamps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_files_timestamps</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>

        <span class="c1"># Only save the model itself if we are using distributed training</span>
        <span class="n">model_to_save</span> <span class="o">=</span> <span class="n">unwrap_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="c1"># save the string version of dtype to the config, e.g. convert mindspore.float32 =&gt; &quot;float32&quot;</span>
        <span class="c1"># we currently don&#39;t use this setting automatically, but may start to use with v5</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">get_parameter_dtype</span><span class="p">(</span><span class="n">model_to_save</span><span class="p">)</span>
        <span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

        <span class="c1"># Attach architecture to the config</span>
        <span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">architectures</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_to_save</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">]</span>

        <span class="c1"># Save the config</span>
        <span class="k">if</span> <span class="n">is_main_process</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">_hf_peft_config_loaded</span><span class="p">:</span>
                <span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">can_generate</span><span class="p">():</span>
                <span class="c1"># generation config built from the model config + the model config holds generation kwargs -&gt; generate</span>
                <span class="c1"># may revert to legacy behavior if the two don&#39;t match</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">model_to_save</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">_from_model_config</span>
                    <span class="ow">and</span> <span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_has_non_default_generation_parameters</span><span class="p">()</span>
                <span class="p">):</span>
                    <span class="n">new_generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_model_config</span><span class="p">(</span><span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">new_generation_config</span> <span class="o">!=</span> <span class="n">model_to_save</span><span class="o">.</span><span class="n">generation_config</span><span class="p">:</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                            <span class="s2">&quot;Your generation config was originally created from the model config, but the model &quot;</span>
                            <span class="s2">&quot;config has changed since then. Unless you pass the `generation_config` argument to this &quot;</span>
                            <span class="s2">&quot;model&#39;s `generate` calls, they will revert to the legacy behavior where the base &quot;</span>
                            <span class="s2">&quot;`generate` parameterization is loaded from the model config instead. &quot;</span>
                            <span class="s2">&quot;To avoid this behavior and this warning, we recommend you to overwrite the generation &quot;</span>
                            <span class="s2">&quot;config model attribute before calling the model&#39;s `save_pretrained`, preferably also &quot;</span>
                            <span class="s2">&quot;removing any generation kwargs from the model config. This warning will be raised to an &quot;</span>
                            <span class="s2">&quot;exception in v4.41.&quot;</span>
                        <span class="p">)</span>
                <span class="n">model_to_save</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">_hf_peft_config_loaded</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="s2">&quot;Detected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.&quot;</span>
                <span class="p">)</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">model_to_save</span><span class="o">.</span><span class="n">get_adapter_state_dict</span><span class="p">()</span>

                <span class="k">if</span> <span class="n">save_peft_format</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="s2">&quot;To match the expected format of the PEFT library, all keys of the state dict of adapters will be pre-pended with `base_model.model`.&quot;</span>
                    <span class="p">)</span>
                    <span class="n">peft_state_dict</span> <span class="o">=</span> <span class="p">{}</span>
                    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="n">peft_state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;base_model.model.</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
                    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">peft_state_dict</span>

                <span class="n">active_adapter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapters</span><span class="p">()</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">active_adapter</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Multiple active adapters detected, saving multiple active adapters is not supported yet. You can save adapters separately one by one &quot;</span>
                        <span class="s2">&quot;by iteratively calling `model.set_adapter(adapter_name)` then `model.save_pretrained(...)`&quot;</span>
                    <span class="p">)</span>
                <span class="n">active_adapter</span> <span class="o">=</span> <span class="n">active_adapter</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="n">current_peft_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">active_adapter</span><span class="p">]</span>
                <span class="n">current_peft_config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>

        <span class="c1"># for offloaded modules</span>
        <span class="n">module_map</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Save the model</span>
        <span class="k">if</span> <span class="n">state_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># if any model parameters are offloaded, make module map</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;hf_device_map&quot;</span><span class="p">)</span>
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_device_map</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span> <span class="o">&gt;</span> <span class="mi">1</span>
                <span class="ow">and</span> <span class="p">(</span><span class="s2">&quot;cpu&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_device_map</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="ow">or</span> <span class="s2">&quot;disk&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_device_map</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
            <span class="p">):</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)&quot;</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model_to_save</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">module_state_dict</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

                    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">module_state_dict</span><span class="p">:</span>
                        <span class="n">module_map</span><span class="p">[</span><span class="n">name</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;.</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">model_to_save</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

        <span class="c1"># Handle the case where some state_dict keys shouldn&#39;t be saved</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_keys_to_ignore_on_save</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">ignore_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_keys_to_ignore_on_save</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">ignore_key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                    <span class="k">del</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">ignore_key</span><span class="p">]</span>

        <span class="c1"># Shard the model if it is too big.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_hf_peft_config_loaded</span><span class="p">:</span>
            <span class="n">weights_name</span> <span class="o">=</span> <span class="n">SAFE_WEIGHTS_NAME</span> <span class="k">if</span> <span class="n">safe_serialization</span> <span class="k">else</span> <span class="n">WEIGHTS_NAME</span>
            <span class="n">weights_name</span> <span class="o">=</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">weights_name</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weights_name</span> <span class="o">=</span> <span class="n">ADAPTER_SAFE_WEIGHTS_NAME</span> <span class="k">if</span> <span class="n">safe_serialization</span> <span class="k">else</span> <span class="n">ADAPTER_WEIGHTS_NAME</span>

        <span class="n">filename_pattern</span> <span class="o">=</span> <span class="n">weights_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.bin&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{suffix}</span><span class="s2">.bin&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.safetensors&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{suffix}</span><span class="s2">.safetensors&quot;</span><span class="p">)</span>
        <span class="n">state_dict_split</span> <span class="o">=</span> <span class="n">split_state_dict_into_shards</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">filename_pattern</span><span class="o">=</span><span class="n">filename_pattern</span><span class="p">,</span> <span class="n">max_shard_size</span><span class="o">=</span><span class="n">max_shard_size</span>
        <span class="p">)</span>
        <span class="c1"># Save index if sharded</span>
        <span class="n">index</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">state_dict_split</span><span class="o">.</span><span class="n">is_sharded</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;metadata&quot;</span><span class="p">:</span> <span class="n">state_dict_split</span><span class="o">.</span><span class="n">metadata</span><span class="p">,</span>
                <span class="s2">&quot;weight_map&quot;</span><span class="p">:</span> <span class="n">state_dict_split</span><span class="o">.</span><span class="n">tensor_to_filename</span><span class="p">,</span>
            <span class="p">}</span>

        <span class="c1"># Clean the folder from a previous save</span>
        <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
            <span class="n">full_filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
            <span class="c1"># If we have a shard file that is not going to be replaced, we delete it, but only from the main process</span>
            <span class="c1"># in distributed settings to avoid race conditions.</span>
            <span class="n">weights_no_suffix</span> <span class="o">=</span> <span class="n">weights_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.bin&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.safetensors&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>

            <span class="c1"># make sure that file to be deleted matches format of sharded file, e.g. pytorch_model-00001-of-00005</span>
            <span class="n">filename_no_suffix</span> <span class="o">=</span> <span class="n">filename</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.bin&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.safetensors&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="n">reg</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;(.*?)-\d</span><span class="si">{5}</span><span class="s2">-of-\d</span><span class="si">{5}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="n">filename</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">weights_no_suffix</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">full_filename</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">filename</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state_dict_split</span><span class="o">.</span><span class="n">filename_to_tensors</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                <span class="ow">and</span> <span class="n">is_main_process</span>
                <span class="ow">and</span> <span class="n">reg</span><span class="o">.</span><span class="n">fullmatch</span><span class="p">(</span><span class="n">filename_no_suffix</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">):</span>
                <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">full_filename</span><span class="p">)</span>
        <span class="c1"># Save the model</span>
        <span class="n">filename_to_tensors</span> <span class="o">=</span> <span class="n">state_dict_split</span><span class="o">.</span><span class="n">filename_to_tensors</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">module_map</span><span class="p">:</span>
            <span class="n">filename_to_tensors</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">filename_to_tensors</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Saving checkpoint shards&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">shard_file</span><span class="p">,</span> <span class="n">tensors</span> <span class="ow">in</span> <span class="n">filename_to_tensors</span><span class="p">:</span>
            <span class="n">shard</span> <span class="o">=</span> <span class="p">{</span><span class="n">tensor</span><span class="p">:</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">tensor</span><span class="p">]</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">}</span>
            <span class="c1"># remake shard with onloaded parameters if necessary</span>
            <span class="k">if</span> <span class="n">module_map</span><span class="p">:</span>
                <span class="c1"># init state_dict for this shard</span>
                <span class="n">shard_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="s2">&quot;&quot;</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">shard</span><span class="p">}</span>
                <span class="k">for</span> <span class="n">module_name</span> <span class="ow">in</span> <span class="n">shard</span><span class="p">:</span>
                    <span class="n">module</span> <span class="o">=</span> <span class="n">module_map</span><span class="p">[</span><span class="n">module_name</span><span class="p">]</span>
                    <span class="c1"># update state dict with onloaded parameters</span>
                    <span class="c1"># shard_state_dict = get_state_dict_from_offload(module, module_name, shard_state_dict)</span>

                <span class="c1"># assign shard to be the completed state dict</span>
                <span class="n">shard</span> <span class="o">=</span> <span class="n">shard_state_dict</span>
                <span class="k">del</span> <span class="n">shard_state_dict</span>
                <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">safe_serialization</span><span class="p">:</span>
                <span class="c1"># At some point we will need to deal better with save_function (used for TPU and other distributed</span>
                <span class="c1"># joyfulness), but for now this enough.</span>
                <span class="n">safe_save_file</span><span class="p">(</span><span class="n">shard</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">shard_file</span><span class="p">),</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;np&quot;</span><span class="p">})</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">save_function</span><span class="p">(</span><span class="n">shard</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">shard_file</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">path_to_weights</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">weights_name</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model weights saved in </span><span class="si">{</span><span class="n">path_to_weights</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">save_index_file</span> <span class="o">=</span> <span class="n">SAFE_WEIGHTS_INDEX_NAME</span> <span class="k">if</span> <span class="n">safe_serialization</span> <span class="k">else</span> <span class="n">WEIGHTS_INDEX_NAME</span>
            <span class="n">save_index_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">save_index_file</span><span class="p">,</span> <span class="n">variant</span><span class="p">))</span>
            <span class="c1"># Save the index as well</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">save_index_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">content</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The model is bigger than the maximum size per checkpoint (</span><span class="si">{</span><span class="n">max_shard_size</span><span class="si">}</span><span class="s2">) and is going to be &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;split in </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">state_dict_split</span><span class="o">.</span><span class="n">filename_to_tensors</span><span class="p">)</span><span class="si">}</span><span class="s2"> checkpoint shards. You can find where each parameters has been saved in the &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;index located at </span><span class="si">{</span><span class="n">save_index_file</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_memory_footprint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">return_buffers</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the memory footprint of a model. This will return the memory footprint of the current model in bytes.</span>
<span class="sd">        Useful to benchmark the memory footprint of the current model and design some tests. Solution inspired from the</span>
<span class="sd">        PyTorch discussions: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822/2</span>

<span class="sd">        Arguments:</span>
<span class="sd">            return_buffers (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to return the size of the buffer tensors in the computation of the memory footprint. Buffers</span>
<span class="sd">                are tensors that do not require gradients and not registered as parameters. E.g. mean and std in batch</span>
<span class="sd">                norm layers. Please see: https://discuss.pytorch.org/t/what-pytorch-means-by-buffers/120266/2</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mem</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="o">*</span> <span class="n">param</span><span class="o">.</span><span class="n">itemsize</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">return_buffers</span><span class="p">:</span>
            <span class="n">mem_bufs</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="o">*</span> <span class="n">buf</span><span class="o">.</span><span class="n">itemsize</span> <span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffers</span><span class="p">())</span>
            <span class="n">mem</span> <span class="o">=</span> <span class="n">mem</span> <span class="o">+</span> <span class="n">mem_bufs</span>
        <span class="k">return</span> <span class="n">mem</span>

    <span class="k">def</span> <span class="nf">half</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># Checks if the model is quantized</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;is_quantized&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`.half()` is not supported for quantized model. Please use the model as it is, since the&quot;</span>
                <span class="s2">&quot; model has already been casted to the correct `dtype`.&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">half</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">float</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># Checks if the model is quantized</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;is_quantized&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`.float()` is not supported for quantized model. Please use the model as it is, since the&quot;</span>
                <span class="s2">&quot; model has already been casted to the correct `dtype`.&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_pretrained</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]],</span>
        <span class="o">*</span><span class="n">model_args</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">PretrainedConfig</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ignore_mismatched_sizes</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">force_download</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">token</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">revision</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;main&quot;</span><span class="p">,</span>
        <span class="n">use_safetensors</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;PreTrainedModel&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Instantiate a pretrained pytorch model from a pre-trained model configuration.</span>

<span class="sd">        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train</span>
<span class="sd">        the model, you should first set it back in training mode with `model.train()`.</span>

<span class="sd">        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come</span>
<span class="sd">        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning</span>
<span class="sd">        task.</span>

<span class="sd">        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those</span>
<span class="sd">        weights are discarded.</span>

<span class="sd">        If model weights are the same precision as the base model (and is a supported model), weights will be lazily loaded</span>
<span class="sd">        in using the `meta` device and brought into memory once an input is passed through that layer regardless of</span>
<span class="sd">        `low_cpu_mem_usage`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.</span>
<span class="sd">                    - A path to a *directory* containing model weights saved using</span>
<span class="sd">                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.</span>
<span class="sd">                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In</span>
<span class="sd">                      this case, `from_tf` should be set to `True` and a configuration object should be provided as</span>
<span class="sd">                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a</span>
<span class="sd">                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</span>
<span class="sd">                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,</span>
<span class="sd">                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to</span>
<span class="sd">                      `True`.</span>
<span class="sd">                    - `None` if you are both providing the configuration and state dictionary (resp. with keyword</span>
<span class="sd">                      arguments `config` and `state_dict`).</span>
<span class="sd">            model_args (sequence of positional arguments, *optional*):</span>
<span class="sd">                All remaining positional arguments will be passed to the underlying model&#39;s `__init__` method.</span>
<span class="sd">            config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - an instance of a class derived from [`PretrainedConfig`],</span>
<span class="sd">                    - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].</span>

<span class="sd">                Configuration for the model to use instead of an automatically loaded configuration. Configuration can</span>
<span class="sd">                be automatically loaded when:</span>

<span class="sd">                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained</span>
<span class="sd">                      model).</span>
<span class="sd">                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the</span>
<span class="sd">                      save directory.</span>
<span class="sd">                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a</span>
<span class="sd">                      configuration JSON file named *config.json* is found in the directory.</span>
<span class="sd">            state_dict (`Dict[str, mindspore.Tensor]`, *optional*):</span>
<span class="sd">                A state dictionary to use instead of a state dictionary loaded from saved weights file.</span>

<span class="sd">                This option can be used if you want to create a model from a pretrained configuration but load your own</span>
<span class="sd">                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and</span>
<span class="sd">                [`~PreTrainedModel.from_pretrained`] is not a simpler option.</span>
<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory in which a downloaded pretrained model configuration should be cached if the</span>
<span class="sd">                standard cache should not be used.</span>
<span class="sd">            from_tf (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Load the model weights from a TensorFlow checkpoint save file (see docstring of</span>
<span class="sd">                `pretrained_model_name_or_path` argument).</span>
<span class="sd">            from_flax (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Load the model weights from a Flax checkpoint save file (see docstring of</span>
<span class="sd">                `pretrained_model_name_or_path` argument).</span>
<span class="sd">            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size</span>
<span class="sd">                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a</span>
<span class="sd">                checkpoint with 3 labels).</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>
<span class="sd">            resume_download:</span>
<span class="sd">                Deprecated and ignored. All downloads are now resumed by default when possible.</span>
<span class="sd">                Will be removed in v5 of Transformers.</span>
<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            output_loading_info(`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</span>
<span class="sd">            local_files_only(`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to only look at local files (i.e., do not try to download the model).</span>
<span class="sd">            token (`str` or `bool`, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use</span>
<span class="sd">                the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a</span>
<span class="sd">                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any</span>
<span class="sd">                identifier allowed by git.</span>

<span class="sd">                &lt;Tip&gt;</span>

<span class="sd">                To test a pull request you made on the Hub, you can pass `revision=&quot;refs/pr/&lt;pr_number&gt;&quot;.</span>

<span class="sd">                &lt;/Tip&gt;</span>

<span class="sd">            mirror (`str`, *optional*):</span>
<span class="sd">                Mirror source to accelerate downloads in China. If you are from China and have an accessibility</span>
<span class="sd">                problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.</span>
<span class="sd">                Please refer to the mirror site for more information.</span>
<span class="sd">            _fast_init(`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether or not to disable fast initialization.</span>

<span class="sd">                &lt;Tip warning={true}&gt;</span>

<span class="sd">                One should only disable *_fast_init* to ensure backwards compatibility with `transformers.__version__ &lt;</span>
<span class="sd">                4.6.0` for seeded model initialization. This argument will be removed at the next major version. See</span>
<span class="sd">                [pull request 11471](https://github.com/huggingface/transformers/pull/11471) for more information.</span>

<span class="sd">                &lt;/Tip&gt;</span>
<span class="sd">            attn_implementation (`str`, *optional*):</span>
<span class="sd">                The attention implementation to use in the model (if relevant). The default is otherwise the manual `&quot;eager&quot;` implementation.</span>

<span class="sd">            &gt; Parameters for big model inference</span>

<span class="sd">            low_cpu_mem_usage(`bool`, *optional*):</span>
<span class="sd">                Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.</span>
<span class="sd">                Generally should be combined with a `device_map` (such as `&quot;auto&quot;`) for best results.</span>
<span class="sd">                This is an experimental feature and a subject to change at any moment.</span>
<span class="sd">                &lt;/Tip&gt;</span>
<span class="sd">                    If the model weights are in the same precision as the model loaded in, `low_cpu_mem_usage` (without</span>
<span class="sd">                    `device_map`) is redundant and will not provide any benefit in regards to CPU memory usage. However,</span>
<span class="sd">                    this should still be enabled if you are passing in a `device_map`.</span>
<span class="sd">                &lt;/Tip&gt;</span>
<span class="sd">            ms_dtype (`str` or `mindspore.dtype.TensorType`, *optional*):</span>
<span class="sd">                Override the default `mindspore.dtype.TensorType` and load the model under a specific `dtype`. The different options</span>
<span class="sd">                are:</span>

<span class="sd">                1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified</span>
<span class="sd">                  `dtype`, ignoring the model&#39;s `config.ms_dtype` if one exists. If not specified</span>
<span class="sd">                  - the model will get loaded in `torch.float` (fp32).</span>

<span class="sd">                2. `&quot;auto&quot;` - A `ms_dtype` entry in the `config.json` file of the model will be</span>
<span class="sd">                  attempted to be used. If this entry isn&#39;t found then next check the `dtype` of the first weight in</span>
<span class="sd">                  the checkpoint that&#39;s of a floating point type and use that as `dtype`. This will load the model</span>
<span class="sd">                  using the `dtype` it was saved in at the end of the training. It can&#39;t be used as an indicator of how</span>
<span class="sd">                  the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.</span>

<span class="sd">                3. A string that is a valid `mindspore.dtype.TensorType`. E.g. &quot;float32&quot; loads the model in `mindspore.float32`, &quot;float16&quot; loads in `torch.float16` etc.</span>

<span class="sd">                &lt;Tip&gt;</span>

<span class="sd">                For some models the `dtype` they were trained in is unknown - you may try to check the model&#39;s paper or</span>
<span class="sd">                reach out to the authors and ask them to add this information to the model&#39;s card and to insert the</span>
<span class="sd">                `ms_dtype` entry in `config.json` on the hub.</span>

<span class="sd">                &lt;/Tip&gt;</span>

<span class="sd">            device_map (`str` or `Dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):</span>
<span class="sd">                A map that specifies where each submodule should go. It doesn&#39;t need to be refined to each</span>
<span class="sd">                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the</span>
<span class="sd">                same device. If we only pass the device (*e.g.*, `&quot;cpu&quot;`, `&quot;cuda:1&quot;`, `&quot;mps&quot;`, or a GPU ordinal rank</span>
<span class="sd">                like `1`) on which the model will be allocated, the device map will map the entire model to this</span>
<span class="sd">                device. Passing `device_map = 0` means put the whole model on GPU 0.</span>

<span class="sd">                To have Accelerate compute the most optimized `device_map` automatically, set `device_map=&quot;auto&quot;`. For</span>
<span class="sd">                more information about each option see [designing a device</span>
<span class="sd">                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).</span>
<span class="sd">            max_memory (`Dict`, *optional*):</span>
<span class="sd">                A dictionary device identifier to maximum memory. Will default to the maximum memory available for each</span>
<span class="sd">                GPU and the available CPU RAM if unset.</span>
<span class="sd">            offload_folder (`str` or `os.PathLike`, *optional*):</span>
<span class="sd">                If the `device_map` contains any value `&quot;disk&quot;`, the folder where we will offload weights.</span>
<span class="sd">            offload_state_dict (`bool`, *optional*):</span>
<span class="sd">                If `True`, will temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU</span>
<span class="sd">                RAM if the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to</span>
<span class="sd">                `True` when there is some disk offload.</span>
<span class="sd">            offload_buffers (`bool`, *optional*):</span>
<span class="sd">                Whether or not to offload the buffers with the model parameters.</span>
<span class="sd">            quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):</span>
<span class="sd">                A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g</span>
<span class="sd">                bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and</span>
<span class="sd">                `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes</span>
<span class="sd">                quantizations and not preferred. consider inserting all such arguments into quantization_config</span>
<span class="sd">                instead.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can</span>
<span class="sd">                specify the folder name here.</span>
<span class="sd">            variant (`str`, *optional*):</span>
<span class="sd">                If specified load weights from `variant` filename, *e.g.* pytorch_model.&lt;variant&gt;.bin. `variant` is</span>
<span class="sd">                ignored when using `from_tf` or `from_flax`.</span>
<span class="sd">            use_safetensors (`bool`, *optional*, defaults to `None`):</span>
<span class="sd">                Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`</span>
<span class="sd">                is not installed, it will be set to `False`.</span>

<span class="sd">            kwargs (remaining dictionary of keyword arguments, *optional*):</span>
<span class="sd">                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,</span>
<span class="sd">                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or</span>
<span class="sd">                automatically loaded:</span>

<span class="sd">                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the</span>
<span class="sd">                      underlying model&#39;s `__init__` method (we assume all relevant updates to the configuration have</span>
<span class="sd">                      already been done)</span>
<span class="sd">                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class</span>
<span class="sd">                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that</span>
<span class="sd">                      corresponds to a configuration attribute will be used to override said attribute with the</span>
<span class="sd">                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute</span>
<span class="sd">                      will be passed to the underlying model&#39;s `__init__` function.</span>

<span class="sd">        &lt;Tip&gt;</span>

<span class="sd">        Activate the special [&quot;offline-mode&quot;](https://huggingface.co/transformers/installation.html#offline-mode) to</span>
<span class="sd">        use this method in a firewalled environment.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Examples:</span>

<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; from transformers import BertConfig, BertModel</span>

<span class="sd">        &gt;&gt;&gt; # Download model and configuration from huggingface.co and cache.</span>
<span class="sd">        &gt;&gt;&gt; model = BertModel.from_pretrained(&quot;google-bert/bert-base-uncased&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # Model was saved using *save_pretrained(&#39;./test/saved_model/&#39;)* (for example purposes, not runnable).</span>
<span class="sd">        &gt;&gt;&gt; model = BertModel.from_pretrained(&quot;./test/saved_model/&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # Update configuration during loading.</span>
<span class="sd">        &gt;&gt;&gt; model = BertModel.from_pretrained(&quot;google-bert/bert-base-uncased&quot;, output_attentions=True)</span>
<span class="sd">        &gt;&gt;&gt; assert model.config.output_attentions == True</span>
<span class="sd">        &gt;&gt;&gt; # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).</span>
<span class="sd">        &gt;&gt;&gt; config = BertConfig.from_json_file(&quot;./tf_model/my_tf_model_config.json&quot;)</span>
<span class="sd">        &gt;&gt;&gt; model = BertModel.from_pretrained(&quot;./tf_model/my_tf_checkpoint.ckpt.index&quot;, from_tf=True, config=config)</span>
<span class="sd">        &gt;&gt;&gt; # Loading from a Flax checkpoint file instead of a PyTorch model (slower)</span>
<span class="sd">        &gt;&gt;&gt; model = BertModel.from_pretrained(&quot;google-bert/bert-base-uncased&quot;, from_flax=True)</span>
<span class="sd">        ```</span>

<span class="sd">        * `low_cpu_mem_usage` algorithm:</span>

<span class="sd">        This is an experimental function that loads the model using ~1x model size CPU memory</span>

<span class="sd">        Here is how it works:</span>

<span class="sd">        1. save which state_dict keys we have</span>
<span class="sd">        2. drop state_dict before the model is created, since the latter takes 1x model size CPU memory</span>
<span class="sd">        3. after the model has been instantiated switch to the meta device all params/buffers that</span>
<span class="sd">        are going to be replaced from the loaded state_dict</span>
<span class="sd">        4. load state_dict 2nd time</span>
<span class="sd">        5. replace the params/buffers from the state_dict</span>

<span class="sd">        Currently, it can&#39;t handle deepspeed ZeRO stage 3 and ignores loading errors</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;state_dict&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">from_pt</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;from_pt&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">resume_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;resume_download&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">output_loading_info</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;output_loading_info&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">from_pipeline</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_from_pipeline&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">from_auto_class</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_from_auto&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">_fast_init</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_fast_init&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">ms_dtype</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;ms_dtype&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">low_cpu_mem_usage</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">device_map</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;device_map&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">max_memory</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;max_memory&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">offload_folder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;offload_folder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">offload_state_dict</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;offload_state_dict&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">offload_buffers</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;offload_buffers&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">load_in_8bit</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;load_in_8bit&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">load_in_4bit</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;load_in_4bit&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">quantization_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;quantization_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">mirror</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;mirror&#39;</span><span class="p">,</span> <span class="s1">&#39;huggingface&#39;</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="n">commit_hash</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_commit_hash&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">variant</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;variant&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">adapter_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;adapter_kwargs&quot;</span><span class="p">,</span> <span class="p">{})</span>
        <span class="n">adapter_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;adapter_name&quot;</span><span class="p">,</span> <span class="s2">&quot;default&quot;</span><span class="p">)</span>
        <span class="n">use_flash_attention_2</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_flash_attention_2&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">gguf_file</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;gguf_file&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">adapter_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;token&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">adapter_kwargs</span><span class="p">:</span>
            <span class="n">adapter_kwargs</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">token</span>

        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_safetensors_available</span><span class="p">():</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">gguf_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;MindNLP not support GGUF file.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">commit_hash</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PretrainedConfig</span><span class="p">):</span>
                <span class="c1"># We make a call to the config file first (which may be absent) to get the commit hash as soon as possible</span>
                <span class="n">resolved_config_file</span> <span class="o">=</span> <span class="n">cached_file</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                    <span class="n">CONFIG_NAME</span><span class="p">,</span>
                    <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                    <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                    <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                    <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                    <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                    <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                    <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
                    <span class="n">_raise_exceptions_for_gated_repo</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">_raise_exceptions_for_missing_entries</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">_raise_exceptions_for_connection_errors</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">commit_hash</span> <span class="o">=</span> <span class="n">extract_commit_hash</span><span class="p">(</span><span class="n">resolved_config_file</span><span class="p">,</span> <span class="n">commit_hash</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">commit_hash</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;_commit_hash&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="n">_adapter_model_path</span> <span class="o">=</span> <span class="n">adapter_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_adapter_model_path&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">_adapter_model_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_adapter_model_path</span> <span class="o">=</span> <span class="n">find_adapter_config_file</span><span class="p">(</span>
                <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                <span class="n">_commit_hash</span><span class="o">=</span><span class="n">commit_hash</span><span class="p">,</span>
                <span class="n">mirror</span><span class="o">=</span><span class="n">mirror</span><span class="p">,</span>
                <span class="o">**</span><span class="n">adapter_kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">_adapter_model_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">_adapter_model_path</span><span class="p">):</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">_adapter_model_path</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">_adapter_model_path</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>
                <span class="n">pretrained_model_name_or_path</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)[</span><span class="s2">&quot;base_model_name_or_path&quot;</span><span class="p">]</span>

        <span class="c1"># change device_map into a map if we passed an int, a str or a torch.device</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device_map</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">device_map</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="s2">&quot;balanced&quot;</span><span class="p">,</span> <span class="s2">&quot;balanced_low_0&quot;</span><span class="p">,</span> <span class="s2">&quot;sequential&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;When passing device_map as a string, the value needs to be &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;&#39;auto&#39;, &#39;balanced&#39;, &#39;balanced_low_0&#39;, &#39;sequential&#39; but found </span><span class="si">{</span><span class="n">device_map</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device_map</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">device_map</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;You can&#39;t pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = &#39;cpu&#39; &quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">device_map</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;&quot;</span><span class="p">:</span> <span class="n">device_map</span><span class="p">}</span>

        <span class="k">if</span> <span class="n">device_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">low_cpu_mem_usage</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">low_cpu_mem_usage</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">low_cpu_mem_usage</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Passing along a `device_map` requires `low_cpu_mem_usage=True`&quot;</span><span class="p">)</span>

        <span class="c1"># handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.</span>
        <span class="k">if</span> <span class="n">load_in_4bit</span> <span class="ow">or</span> <span class="n">load_in_8bit</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;MindNLP do not support 4bit or 8bit for now.&quot;</span><span class="p">)</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">,</span> <span class="s2">&quot;from_auto_class&quot;</span><span class="p">:</span> <span class="n">from_auto_class</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">from_pipeline</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">user_agent</span><span class="p">[</span><span class="s2">&quot;using_pipeline&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">from_pipeline</span>

        <span class="k">if</span> <span class="n">is_offline_mode</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">local_files_only</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Offline mode: forcing local_files_only=True&quot;</span><span class="p">)</span>
            <span class="n">local_files_only</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Load config if we don&#39;t provide a configuration</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PretrainedConfig</span><span class="p">):</span>
            <span class="n">config_path</span> <span class="o">=</span> <span class="n">config</span> <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">pretrained_model_name_or_path</span>
            <span class="n">config</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">config_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                <span class="n">config_path</span><span class="p">,</span>
                <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                <span class="n">return_unused_kwargs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
                <span class="n">mirror</span><span class="o">=</span><span class="n">mirror</span><span class="p">,</span>
                <span class="n">_from_auto</span><span class="o">=</span><span class="n">from_auto_class</span><span class="p">,</span>
                <span class="n">_from_pipeline</span><span class="o">=</span><span class="n">from_pipeline</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># In case one passes a config to `from_pretrained` + &quot;attn_implementation&quot;</span>
            <span class="c1"># override the `_attn_implementation` attribute to `attn_implementation` of the kwargs</span>
            <span class="c1"># Please see: https://github.com/huggingface/transformers/issues/28038</span>

            <span class="c1"># Overwrite `config._attn_implementation` by the one from the kwargs --&gt; in auto-factory</span>
            <span class="c1"># we pop attn_implementation from the kwargs but this handles the case where users</span>
            <span class="c1"># passes manually the config to `from_pretrained`.</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

            <span class="n">kwarg_attn_imp</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">kwarg_attn_imp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">=</span> <span class="n">kwarg_attn_imp</span>

            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

        <span class="c1"># This variable will flag if we&#39;re loading a sharded checkpoint. In this case the archive file is just the</span>
        <span class="c1"># index of the files.</span>
        <span class="n">is_sharded</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">sharded_metadata</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Load model</span>
        <span class="n">loading_info</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Keep in fp32 modules</span>
        <span class="n">keep_in_fp32_modules</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">use_keep_in_fp32_modules</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">gguf_file</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pretrained_model_name_or_path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span>
            <span class="n">is_local</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_local</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">False</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">))</span>
                <span class="p">):</span>
                    <span class="c1"># Load from a safetensors checkpoint</span>
                    <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">False</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="p">):</span>
                    <span class="c1"># Load from a sharded safetensors checkpoint</span>
                    <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">is_sharded</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">elif</span> <span class="ow">not</span> <span class="n">use_safetensors</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">))</span>
                <span class="p">):</span>
                    <span class="c1"># Load from a MindSpore checkpoint</span>
                    <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="ow">not</span> <span class="n">use_safetensors</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">))</span>
                <span class="p">):</span>
                    <span class="c1"># Load from a sharded MindSpore checkpoint</span>
                    <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">is_sharded</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">elif</span> <span class="ow">not</span> <span class="n">use_safetensors</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">))</span>
                <span class="p">):</span>
                    <span class="c1"># Load from a PyTorch checkpoint</span>
                    <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="ow">not</span> <span class="n">use_safetensors</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">))</span>
                <span class="p">):</span>
                    <span class="c1"># Load from a sharded PyTorch checkpoint</span>
                    <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">is_sharded</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">elif</span> <span class="n">use_safetensors</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Error no file named </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2"> found in directory&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Error no file named </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2">,&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot; found in directory </span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>
            <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">subfolder</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">)):</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>
                <span class="n">is_local</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">elif</span> <span class="n">is_remote_url</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">):</span>
                <span class="n">filename</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>
                <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">download_url</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># set correct filename</span>
                <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">False</span><span class="p">:</span>
                    <span class="n">filename</span> <span class="o">=</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">from_pt</span><span class="p">:</span>
                    <span class="n">filename</span> <span class="o">=</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">filename</span> <span class="o">=</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>

                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># Load from URL or cache if already cached</span>
                    <span class="n">cached_file_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="s2">&quot;cache_dir&quot;</span><span class="p">:</span> <span class="n">cache_dir</span><span class="p">,</span>
                        <span class="s2">&quot;force_download&quot;</span><span class="p">:</span> <span class="n">force_download</span><span class="p">,</span>
                        <span class="s2">&quot;proxies&quot;</span><span class="p">:</span> <span class="n">proxies</span><span class="p">,</span>
                        <span class="s2">&quot;resume_download&quot;</span><span class="p">:</span> <span class="n">resume_download</span><span class="p">,</span>
                        <span class="s2">&quot;local_files_only&quot;</span><span class="p">:</span> <span class="n">local_files_only</span><span class="p">,</span>
                        <span class="s2">&quot;token&quot;</span><span class="p">:</span> <span class="n">token</span><span class="p">,</span>
                        <span class="s2">&quot;user_agent&quot;</span><span class="p">:</span> <span class="n">user_agent</span><span class="p">,</span>
                        <span class="s2">&quot;revision&quot;</span><span class="p">:</span> <span class="n">revision</span><span class="p">,</span>
                        <span class="s2">&quot;subfolder&quot;</span><span class="p">:</span> <span class="n">subfolder</span><span class="p">,</span>
                        <span class="s2">&quot;_raise_exceptions_for_gated_repo&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                        <span class="s2">&quot;_raise_exceptions_for_missing_entries&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                        <span class="c1"># &quot;_commit_hash&quot;: commit_hash,</span>
                        <span class="s2">&quot;mirror&quot;</span><span class="p">:</span> <span class="n">mirror</span>
                    <span class="p">}</span>
                    <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">cached_file</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">cached_file_kwargs</span><span class="p">)</span>

                    <span class="c1"># Since we set _raise_exceptions_for_missing_entries=False, we don&#39;t get an exception but a None</span>
                    <span class="c1"># result when internet is up, the repo and revision exist, but the file does not.</span>
                    <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">filename</span> <span class="o">==</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">):</span>
                        <span class="c1"># Maybe the checkpoint is sharded, we try to grab the index name in this case.</span>
                        <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">cached_file</span><span class="p">(</span>
                            <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                            <span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">),</span>
                            <span class="o">**</span><span class="n">cached_file_kwargs</span><span class="p">,</span>
                        <span class="p">)</span>
                        <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="n">is_sharded</span> <span class="o">=</span> <span class="kc">True</span>
                        <span class="k">elif</span> <span class="n">use_safetensors</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">revision</span> <span class="o">==</span> <span class="s2">&quot;main&quot;</span><span class="p">:</span>
                                <span class="n">resolved_archive_file</span><span class="p">,</span> <span class="n">revision</span><span class="p">,</span> <span class="n">is_sharded</span> <span class="o">=</span> <span class="n">auto_conversion</span><span class="p">(</span>
                                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">**</span><span class="n">cached_file_kwargs</span>
                                <span class="p">)</span>
                            <span class="n">cached_file_kwargs</span><span class="p">[</span><span class="s2">&quot;revision&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">revision</span>
                            <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                                <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2"> does not appear to have a file named&quot;</span>
                                    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2"> or </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_INDEX_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2"> &quot;</span>
                                    <span class="s2">&quot;and thus cannot be loaded with `safetensors`. Please make sure that the model has &quot;</span>
                                    <span class="s2">&quot;been saved with `safe_serialization=True` or do not set `use_safetensors=True`.&quot;</span>
                                <span class="p">)</span>
                        <span class="k">elif</span> <span class="n">from_pt</span><span class="p">:</span>
                            <span class="n">filename</span> <span class="o">=</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                            <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">cached_file</span><span class="p">(</span>
                                <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">cached_file_kwargs</span>
                            <span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="c1"># This repo has no safetensors file of any kind, we switch to PyTorch.</span>
                            <span class="n">filename</span> <span class="o">=</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                            <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">cached_file</span><span class="p">(</span>
                                <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">cached_file_kwargs</span>
                            <span class="p">)</span>

                    <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">filename</span> <span class="o">==</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">):</span>
                        <span class="c1"># Maybe the checkpoint is sharded, we try to grab the index name in this case.</span>
                        <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">cached_file</span><span class="p">(</span>
                            <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                            <span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">),</span>
                            <span class="o">**</span><span class="n">cached_file_kwargs</span><span class="p">,</span>
                        <span class="p">)</span>
                        <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="n">is_sharded</span> <span class="o">=</span> <span class="kc">True</span>

                    <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">filename</span> <span class="o">==</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">):</span>
                        <span class="c1"># Maybe the checkpoint is sharded, we try to grab the index name in this case.</span>
                        <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">cached_file</span><span class="p">(</span>
                            <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                            <span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">),</span>
                            <span class="o">**</span><span class="n">cached_file_kwargs</span><span class="p">,</span>
                        <span class="p">)</span>
                        <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="n">is_sharded</span> <span class="o">=</span> <span class="kc">True</span>

                    <span class="k">if</span> <span class="ow">not</span> <span class="n">local_files_only</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_offline_mode</span><span class="p">():</span>
                        <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">filename</span> <span class="ow">in</span> <span class="p">[</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">PT_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">PT_WEIGHTS_INDEX_NAME</span><span class="p">]:</span>
                                <span class="c1"># If the PyTorch file was found, check if there is a safetensors file on the repository</span>
                                <span class="c1"># If there is no safetensors file on the repositories, start an auto conversion</span>
                                <span class="n">safe_weights_name</span> <span class="o">=</span> <span class="n">SAFE_WEIGHTS_INDEX_NAME</span> <span class="k">if</span> <span class="n">is_sharded</span> <span class="k">else</span> <span class="n">SAFE_WEIGHTS_NAME</span>
                                <span class="n">has_file_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                                    <span class="s2">&quot;revision&quot;</span><span class="p">:</span> <span class="n">revision</span><span class="p">,</span>
                                    <span class="s2">&quot;proxies&quot;</span><span class="p">:</span> <span class="n">proxies</span><span class="p">,</span>
                                    <span class="s2">&quot;token&quot;</span><span class="p">:</span> <span class="n">token</span><span class="p">,</span>
                                    <span class="s2">&quot;cache_dir&quot;</span><span class="p">:</span> <span class="n">cache_dir</span><span class="p">,</span>
                                    <span class="s2">&quot;local_files_only&quot;</span><span class="p">:</span> <span class="n">local_files_only</span><span class="p">,</span>
                                    <span class="s2">&quot;mirror&quot;</span><span class="p">:</span> <span class="n">mirror</span><span class="p">,</span>
                                <span class="p">}</span>
                                <span class="n">cached_file_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                                    <span class="s2">&quot;cache_dir&quot;</span><span class="p">:</span> <span class="n">cache_dir</span><span class="p">,</span>
                                    <span class="s2">&quot;force_download&quot;</span><span class="p">:</span> <span class="n">force_download</span><span class="p">,</span>
                                    <span class="s2">&quot;resume_download&quot;</span><span class="p">:</span> <span class="n">resume_download</span><span class="p">,</span>
                                    <span class="s2">&quot;local_files_only&quot;</span><span class="p">:</span> <span class="n">local_files_only</span><span class="p">,</span>
                                    <span class="s2">&quot;user_agent&quot;</span><span class="p">:</span> <span class="n">user_agent</span><span class="p">,</span>
                                    <span class="s2">&quot;subfolder&quot;</span><span class="p">:</span> <span class="n">subfolder</span><span class="p">,</span>
                                    <span class="s2">&quot;_raise_exceptions_for_gated_repo&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                                    <span class="s2">&quot;_raise_exceptions_for_missing_entries&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                                    <span class="c1"># &quot;_commit_hash&quot;: commit_hash,</span>
                                    <span class="o">**</span><span class="n">has_file_kwargs</span><span class="p">,</span>
                                <span class="p">}</span>

                                <span class="k">if</span> <span class="ow">not</span> <span class="n">has_file</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">safe_weights_name</span><span class="p">,</span> <span class="o">**</span><span class="n">has_file_kwargs</span><span class="p">):</span>
                                    <span class="n">Thread</span><span class="p">(</span>
                                        <span class="n">target</span><span class="o">=</span><span class="n">auto_conversion</span><span class="p">,</span>
                                        <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,),</span>
                                        <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;ignore_errors_during_conversion&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">cached_file_kwargs</span><span class="p">},</span>
                                        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Thread-autoconversion&quot;</span><span class="p">,</span>
                                    <span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="c1"># Otherwise, no PyTorch file was found, maybe there is a TF or Flax model file.</span>
                            <span class="c1"># We try those to give a helpful error message.</span>
                            <span class="n">has_file_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                                <span class="s2">&quot;mirror&quot;</span><span class="p">:</span> <span class="n">mirror</span><span class="p">,</span>
                                <span class="s2">&quot;revision&quot;</span><span class="p">:</span> <span class="n">revision</span><span class="p">,</span>
                                <span class="s2">&quot;proxies&quot;</span><span class="p">:</span> <span class="n">proxies</span><span class="p">,</span>
                                <span class="s2">&quot;token&quot;</span><span class="p">:</span> <span class="n">token</span><span class="p">,</span>
                                <span class="s2">&quot;cache_dir&quot;</span><span class="p">:</span> <span class="n">cache_dir</span><span class="p">,</span>
                                <span class="s2">&quot;local_files_only&quot;</span><span class="p">:</span> <span class="n">local_files_only</span><span class="p">,</span>
                            <span class="p">}</span>
                            <span class="k">if</span> <span class="n">variant</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">has_file</span><span class="p">(</span>
                                <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="o">**</span><span class="n">has_file_kwargs</span>
                            <span class="p">):</span>
                                <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2"> does not appear to have a file named&quot;</span>
                                    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2"> but there is a file without the variant&quot;</span>
                                    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">variant</span><span class="si">}</span><span class="s2">. Use `variant=None` to load this model from those weights.&quot;</span>
                                <span class="p">)</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2"> does not appear to have a file named&quot;</span>
                                    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                                <span class="p">)</span>

                <span class="k">except</span> <span class="ne">EnvironmentError</span><span class="p">:</span>
                    <span class="c1"># Raise any environment error raise by `cached_file`. It will have a helpful error message adapted</span>
                    <span class="c1"># to the original exception.</span>
                    <span class="k">raise</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="c1"># For any other exception, we throw a generic error.</span>
                    <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Can&#39;t load the model for &#39;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">&#39;. If you were trying to load it&quot;</span>
                        <span class="s2">&quot; from &#39;https://huggingface.co/models&#39;, make sure you don&#39;t have a local directory with the&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot; same name. Otherwise, make sure &#39;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">&#39; is the correct path to a&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot; directory containing a file named </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

            <span class="k">if</span> <span class="n">is_local</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;loading weights file </span><span class="si">{</span><span class="n">archive_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">archive_file</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;loading weights file </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2"> from cache at </span><span class="si">{</span><span class="n">resolved_archive_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># We&#39;ll need to download and cache each checkpoint shard if the checkpoint is sharded.</span>
        <span class="k">if</span> <span class="n">is_sharded</span><span class="p">:</span>
            <span class="c1"># resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.</span>
            <span class="n">resolved_archive_file</span><span class="p">,</span> <span class="n">sharded_metadata</span> <span class="o">=</span> <span class="n">get_checkpoint_shard_files</span><span class="p">(</span>
                <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                <span class="n">resolved_archive_file</span><span class="p">,</span>
                <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
                <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
                <span class="c1"># _commit_hash=commit_hash,</span>
                <span class="n">mirror</span><span class="o">=</span><span class="n">mirror</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">is_safetensors_available</span><span class="p">()</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">resolved_archive_file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.safetensors&quot;</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">with</span> <span class="n">safe_open</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">,</span> <span class="n">framework</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">metadata</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">metadata</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;format&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;pt&#39;</span><span class="p">,</span> <span class="s1">&#39;tf&#39;</span><span class="p">,</span> <span class="s1">&#39;flax&#39;</span><span class="p">,</span> <span class="s1">&#39;mlx&#39;</span><span class="p">,</span> <span class="s1">&#39;np&#39;</span><span class="p">]:</span>
                <span class="k">pass</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Incompatible safetensors file. File metadata is not [&#39;np&#39;, &#39;pt&#39;, &#39;tf&#39;, &#39;flax&#39;, &#39;mlx&#39;] but </span><span class="si">{</span><span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;format&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

        <span class="c1"># load pt weights early so that we know which dtype to init the model under</span>
        <span class="k">if</span> <span class="n">from_pt</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_sharded</span> <span class="ow">and</span> <span class="n">state_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Time to load the checkpoint</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">load_state_dict</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">)</span>

            <span class="c1"># set dtype to instantiate the model under:</span>
            <span class="c1"># 1. If ms_dtype is not None, we use that dtype</span>
            <span class="c1"># 2. If ms_dtype is &quot;auto&quot;, we auto-detect dtype from the loaded state_dict, by checking its first</span>
            <span class="c1">#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype</span>
            <span class="c1"># we also may have config.ms_dtype available, but we won&#39;t rely on it till v5</span>
            <span class="n">dtype_orig</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">ms_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ms_dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">ms_dtype</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
                        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;ms_dtype&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="n">ms_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span>
                            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Will use ms_dtype=</span><span class="si">{</span><span class="n">ms_dtype</span><span class="si">}</span><span class="s2"> as defined in model&#39;s config object&quot;</span><span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">is_sharded</span> <span class="ow">and</span> <span class="s2">&quot;dtype&quot;</span> <span class="ow">in</span> <span class="n">sharded_metadata</span><span class="p">:</span>
                                <span class="n">ms_dtype</span> <span class="o">=</span> <span class="n">sharded_metadata</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">]</span>
                            <span class="k">elif</span> <span class="ow">not</span> <span class="n">is_sharded</span><span class="p">:</span>
                                <span class="n">ms_dtype</span> <span class="o">=</span> <span class="n">get_state_dict_dtype</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="n">one_state_dict</span> <span class="o">=</span> <span class="n">load_state_dict</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                                <span class="n">ms_dtype</span> <span class="o">=</span> <span class="n">get_state_dict_dtype</span><span class="p">(</span><span class="n">one_state_dict</span><span class="p">)</span>
                                <span class="k">del</span> <span class="n">one_state_dict</span>  <span class="c1"># free CPU memory</span>
                            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                                <span class="s2">&quot;Since the `ms_dtype` attribute can&#39;t be found in model&#39;s config object, &quot;</span>
                                <span class="s2">&quot;will use ms_dtype=</span><span class="si">{ms_dtype}</span><span class="s2"> as derived from model&#39;s weights&quot;</span>
                            <span class="p">)</span>
                    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mindspore</span><span class="p">,</span> <span class="n">ms_dtype</span><span class="p">):</span>
                        <span class="n">ms_dtype</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mindspore</span><span class="p">,</span> <span class="n">ms_dtype</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s1">&#39;`ms_dtype` can be one of: `mindspore.dtype.TensorType`, `&quot;auto&quot;` or a string of a valid `mindspore.dtype.TensorType`, but received </span><span class="si">{</span><span class="n">ms_dtype</span><span class="si">}</span><span class="s1">&#39;</span>
                        <span class="p">)</span>
                <span class="n">dtype_orig</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_set_default_ms_dtype</span><span class="p">(</span><span class="n">ms_dtype</span><span class="p">)</span>

            <span class="c1"># Check if `_keep_in_fp32_modules` is not None</span>
            <span class="n">use_keep_in_fp32_modules</span> <span class="o">=</span> <span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">_keep_in_fp32_modules</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">ms_dtype</span> <span class="o">==</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">is_sharded</span><span class="p">:</span>
                <span class="n">loaded_state_dict_keys</span> <span class="o">=</span> <span class="n">sharded_metadata</span><span class="p">[</span><span class="s2">&quot;all_checkpoint_keys&quot;</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loaded_state_dict_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

        <span class="n">config</span><span class="o">.</span><span class="n">name_or_path</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>

        <span class="c1"># Instantiate model.</span>
        <span class="n">init_contexts</span> <span class="o">=</span> <span class="p">[</span><span class="n">no_init_weights</span><span class="p">(</span><span class="n">_enable</span><span class="o">=</span><span class="n">_fast_init</span><span class="p">)]</span>

        <span class="n">config</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># We do not want to modify the config inplace in from_pretrained.</span>
        <span class="n">config</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_autoset_attn_implementation</span><span class="p">(</span>
            <span class="n">config</span><span class="p">,</span> <span class="n">use_flash_attention_2</span><span class="o">=</span><span class="n">use_flash_attention_2</span><span class="p">,</span> <span class="n">ms_dtype</span><span class="o">=</span><span class="n">ms_dtype</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="n">device_map</span>
        <span class="p">)</span>

        <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;mirror&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">ContextManagers</span><span class="p">(</span><span class="n">init_contexts</span><span class="p">):</span>
            <span class="c1"># Let&#39;s make sure we don&#39;t run the init function of buffer modules</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">model_args</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>
        <span class="c1"># make sure we use the model&#39;s config since the __init__ call might have copied it</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>

        <span class="c1"># Check first if we are `from_pt`</span>
        <span class="k">if</span> <span class="n">use_keep_in_fp32_modules</span><span class="p">:</span>
            <span class="n">keep_in_fp32_modules</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_keep_in_fp32_modules</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">keep_in_fp32_modules</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">group_size</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">group_size</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device_map</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">special_dtypes</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="n">special_dtypes</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="n">name</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span>
                    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span>
                    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">m</span> <span class="ow">in</span> <span class="n">name</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">keep_in_fp32_modules</span><span class="p">)</span>
                <span class="p">}</span>
            <span class="p">)</span>

            <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">ms_dtype</span>

            <span class="n">no_split_modules</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_get_no_split_modules</span><span class="p">(</span><span class="n">device_map</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">device_map</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="s2">&quot;balanced&quot;</span><span class="p">,</span> <span class="s2">&quot;balanced_low_0&quot;</span><span class="p">,</span> <span class="s2">&quot;sequential&quot;</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;If passing a string for `device_map`, please choose &#39;auto&#39;, &#39;balanced&#39;, &#39;balanced_low_0&#39; or &quot;</span>
                    <span class="s2">&quot;&#39;sequential&#39;.&quot;</span>
                <span class="p">)</span>

            <span class="n">device_map_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;no_split_module_classes&quot;</span><span class="p">:</span> <span class="n">no_split_modules</span><span class="p">}</span>
            <span class="k">if</span> <span class="s2">&quot;special_dtypes&quot;</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">infer_auto_device_map</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
                <span class="n">device_map_kwargs</span><span class="p">[</span><span class="s2">&quot;special_dtypes&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">special_dtypes</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">special_dtypes</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;This model has some weights that should be kept in higher precision, you need to upgrade &quot;</span>
                    <span class="s2">&quot;`accelerate` to properly deal with them (`pip install --upgrade accelerate`).&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">device_map</span> <span class="o">!=</span> <span class="s2">&quot;sequential&quot;</span><span class="p">:</span>
                <span class="n">max_memory</span> <span class="o">=</span> <span class="n">get_balanced_memory</span><span class="p">(</span>
                    <span class="n">model</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">target_dtype</span><span class="p">,</span>
                    <span class="n">low_zero</span><span class="o">=</span><span class="p">(</span><span class="n">device_map</span> <span class="o">==</span> <span class="s2">&quot;balanced_low_0&quot;</span><span class="p">),</span>
                    <span class="n">max_memory</span><span class="o">=</span><span class="n">max_memory</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">device_map_kwargs</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">max_memory</span> <span class="o">=</span> <span class="n">get_max_memory</span><span class="p">(</span><span class="n">max_memory</span><span class="p">)</span>

            <span class="n">device_map_kwargs</span><span class="p">[</span><span class="s2">&quot;max_memory&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_memory</span>

            <span class="c1"># Make sure tied weights are tied before creating the device map.</span>
            <span class="n">model</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>
            <span class="n">device_map</span> <span class="o">=</span> <span class="n">infer_auto_device_map</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">target_dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">device_map_kwargs</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">device_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>
            <span class="n">tied_params</span> <span class="o">=</span> <span class="n">find_tied_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="c1"># check if we don&#39;t have tied param in different devices</span>
            <span class="n">check_tied_parameters_on_same_device</span><span class="p">(</span><span class="n">tied_params</span><span class="p">,</span> <span class="n">device_map</span><span class="p">)</span>

        <span class="c1"># replace unnessasery modules to nn.Identity and insert send/recv for pipeline inference.</span>
        <span class="k">if</span> <span class="n">device_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">group_size</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">modify_model_for_pp_infer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_map</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">_no_split_modules</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">from_pt</span><span class="p">:</span>
            <span class="c1"># restore default dtype</span>
            <span class="k">if</span> <span class="n">dtype_orig</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">set_default_dtype</span><span class="p">(</span><span class="n">dtype_orig</span><span class="p">)</span>

            <span class="p">(</span>
                <span class="n">model</span><span class="p">,</span>
                <span class="n">missing_keys</span><span class="p">,</span>
                <span class="n">unexpected_keys</span><span class="p">,</span>
                <span class="n">mismatched_keys</span><span class="p">,</span>
                <span class="n">offload_index</span><span class="p">,</span>
                <span class="n">error_msgs</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_load_pretrained_model</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span>
                <span class="n">state_dict</span><span class="p">,</span>
                <span class="n">loaded_state_dict_keys</span><span class="p">,</span>
                <span class="n">resolved_archive_file</span><span class="p">,</span>
                <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                <span class="n">ignore_mismatched_sizes</span><span class="o">=</span><span class="n">ignore_mismatched_sizes</span><span class="p">,</span>
                <span class="n">sharded_metadata</span><span class="o">=</span><span class="n">sharded_metadata</span><span class="p">,</span>
                <span class="n">_fast_init</span><span class="o">=</span><span class="n">_fast_init</span><span class="p">,</span>
                <span class="n">low_cpu_mem_usage</span><span class="o">=</span><span class="n">low_cpu_mem_usage</span><span class="p">,</span>
                <span class="n">device_map</span><span class="o">=</span><span class="n">device_map</span><span class="p">,</span>
                <span class="n">offload_folder</span><span class="o">=</span><span class="n">offload_folder</span><span class="p">,</span>
                <span class="n">offload_state_dict</span><span class="o">=</span><span class="n">offload_state_dict</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">ms_dtype</span><span class="p">,</span>
                <span class="n">keep_in_fp32_modules</span><span class="o">=</span><span class="n">keep_in_fp32_modules</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># make sure token embedding weights are still tied if needed</span>
        <span class="n">model</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>

        <span class="c1"># Set model in evaluation mode to deactivate DropOut modules by default</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="c1"># If it is a model with generation capabilities, attempt to load the generation config</span>
        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">can_generate</span><span class="p">()</span> <span class="ow">and</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">model</span><span class="o">.</span><span class="n">generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                    <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                    <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                    <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                    <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                    <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                    <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                    <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
                    <span class="n">mirror</span><span class="o">=</span><span class="n">mirror</span><span class="p">,</span>
                    <span class="n">_from_auto</span><span class="o">=</span><span class="n">from_auto_class</span><span class="p">,</span>
                    <span class="n">_from_pipeline</span><span class="o">=</span><span class="n">from_pipeline</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="s2">&quot;Generation config file not found, using a generation config created from the model config.&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">_adapter_model_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span>
                <span class="n">_adapter_model_path</span><span class="p">,</span>
                <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
                <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                <span class="n">adapter_kwargs</span><span class="o">=</span><span class="n">adapter_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">output_loading_info</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">loading_info</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">loading_info</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">&quot;missing_keys&quot;</span><span class="p">:</span> <span class="n">missing_keys</span><span class="p">,</span>
                    <span class="s2">&quot;unexpected_keys&quot;</span><span class="p">:</span> <span class="n">unexpected_keys</span><span class="p">,</span>
                    <span class="s2">&quot;mismatched_keys&quot;</span><span class="p">:</span> <span class="n">mismatched_keys</span><span class="p">,</span>
                    <span class="s2">&quot;error_msgs&quot;</span><span class="p">:</span> <span class="n">error_msgs</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">loading_info</span>

        <span class="k">return</span> <span class="n">model</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_load_pretrained_model</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">loaded_keys</span><span class="p">,</span>
        <span class="n">resolved_archive_file</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
        <span class="n">ignore_mismatched_sizes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">sharded_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_fast_init</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">low_cpu_mem_usage</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">device_map</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">offload_folder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">offload_state_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hf_quantizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">keep_in_fp32_modules</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">gguf_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">is_safetensors</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">is_quantized</span> <span class="o">=</span> <span class="n">hf_quantizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">state_dict_folder</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">state_dict_index</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">device_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;disk&quot;</span> <span class="ow">in</span> <span class="n">device_map</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">archive_file</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">resolved_archive_file</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="k">else</span> <span class="n">resolved_archive_file</span>
            <span class="p">)</span>
            <span class="n">is_safetensors</span> <span class="o">=</span> <span class="n">archive_file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.safetensors&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">offload_folder</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_safetensors</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder`&quot;</span>
                    <span class="s2">&quot; for them. Alternatively, make sure you have `safetensors` installed if the model you are using&quot;</span>
                    <span class="s2">&quot; offers the weights in this format.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">offload_folder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">offload_folder</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">offload_state_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">offload_state_dict</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">is_sharded_safetensors</span> <span class="o">=</span> <span class="n">is_safetensors</span> <span class="ow">and</span> <span class="n">sharded_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="c1"># tie the model weights before retrieving the state_dict</span>
        <span class="n">model</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>

        <span class="c1"># Retrieve missing &amp; unexpected_keys</span>
        <span class="n">model_state_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">expected_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model_state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">prefix</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">base_model_prefix</span>

        <span class="k">def</span> <span class="nf">_fix_key</span><span class="p">(</span><span class="n">key</span><span class="p">):</span>
            <span class="k">if</span> <span class="s2">&quot;beta&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;gamma&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;gamma&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">key</span>

        <span class="n">original_loaded_keys</span> <span class="o">=</span> <span class="n">loaded_keys</span>
        <span class="n">loaded_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">_fix_key</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">loaded_keys</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">has_prefix_module</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">loaded_keys</span><span class="p">)</span>
            <span class="n">expects_prefix_module</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">expected_keys</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">has_prefix_module</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">expects_prefix_module</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># key re-naming operations are never done on the keys</span>
        <span class="c1"># that are loaded, but always on the keys of the newly initialized model</span>
        <span class="n">remove_prefix_from_model</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">has_prefix_module</span> <span class="ow">and</span> <span class="n">expects_prefix_module</span>
        <span class="n">add_prefix_to_model</span> <span class="o">=</span> <span class="n">has_prefix_module</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">expects_prefix_module</span>

        <span class="k">if</span> <span class="n">remove_prefix_from_model</span><span class="p">:</span>
            <span class="n">_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="n">expected_keys_not_prefixed</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">expected_keys</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">s</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">_prefix</span><span class="p">)]</span>
            <span class="n">expected_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">_prefix</span><span class="p">)</span> <span class="p">:]</span> <span class="k">if</span> <span class="n">s</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">_prefix</span><span class="p">)</span> <span class="k">else</span> <span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">expected_keys</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">add_prefix_to_model</span><span class="p">:</span>
            <span class="n">expected_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">prefix</span><span class="p">,</span> <span class="n">s</span><span class="p">])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">expected_keys</span><span class="p">]</span>

        <span class="n">missing_keys</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">expected_keys</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">loaded_keys</span><span class="p">))</span>
        <span class="n">unexpected_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">loaded_keys</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">expected_keys</span><span class="p">)</span>

        <span class="c1"># Remove nonpersistent buffers from unexpected keys: they are not in the state dict but will be in the model</span>
        <span class="c1"># buffers</span>
        <span class="n">model_buffers</span> <span class="o">=</span> <span class="p">{</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">()}</span>
        <span class="k">if</span> <span class="n">remove_prefix_from_model</span><span class="p">:</span>
            <span class="n">model_buffers</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">_prefix</span><span class="p">)</span> <span class="p">:]</span> <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">_prefix</span><span class="p">)</span> <span class="k">else</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">model_buffers</span><span class="p">}</span>
        <span class="k">elif</span> <span class="n">add_prefix_to_model</span><span class="p">:</span>
            <span class="n">model_buffers</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">prefix</span><span class="p">,</span> <span class="n">key</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">model_buffers</span><span class="p">}</span>
        <span class="n">unexpected_keys</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">unexpected_keys</span> <span class="o">-</span> <span class="n">model_buffers</span><span class="p">)</span>

        <span class="n">model</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">device_map</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ptrs</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">id_tensor</span> <span class="o">=</span> <span class="nb">id</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
                <span class="n">ptrs</span><span class="p">[</span><span class="n">id_tensor</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

            <span class="c1"># These are all the pointers of shared tensors.</span>
            <span class="n">tied_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">names</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">names</span> <span class="ow">in</span> <span class="n">ptrs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># id function doesn&#39;t work for meta tensor so we need this function</span>
            <span class="n">tied_params</span> <span class="o">=</span> <span class="n">find_tied_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">tied_params</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">remove_prefix_from_model</span><span class="p">:</span>
                <span class="n">group</span> <span class="o">=</span> <span class="p">[</span><span class="n">key</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">_prefix</span><span class="p">)</span> <span class="p">:]</span> <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">_prefix</span><span class="p">)</span> <span class="k">else</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">group</span><span class="p">]</span>
            <span class="k">elif</span> <span class="n">add_prefix_to_model</span><span class="p">:</span>
                <span class="n">group</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">prefix</span><span class="p">,</span> <span class="n">key</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">group</span><span class="p">]</span>
            <span class="n">missing_in_group</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">missing_keys</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">group</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_in_group</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_in_group</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
                <span class="n">missing_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">missing_keys</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">missing_in_group</span><span class="p">]</span>

        <span class="c1"># Some models may have keys that are not in the state by design, removing them before needlessly warning</span>
        <span class="c1"># the user.</span>
        <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_keys_to_ignore_on_load_missing</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">pat</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_keys_to_ignore_on_load_missing</span><span class="p">:</span>
                <span class="n">missing_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">missing_keys</span> <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">pat</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_keys_to_ignore_on_load_unexpected</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">pat</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_keys_to_ignore_on_load_unexpected</span><span class="p">:</span>
                <span class="n">unexpected_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">unexpected_keys</span> <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">pat</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">]</span>

        <span class="c1"># retrieve uninitialized modules and initialize before maybe overriding that with the pretrained weights.</span>
        <span class="k">if</span> <span class="n">_fast_init</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">ignore_mismatched_sizes</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">remove_prefix_from_model</span><span class="p">:</span>
                    <span class="n">_loaded_keys</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">loaded_keys</span><span class="p">]</span>
                <span class="k">elif</span> <span class="n">add_prefix_to_model</span><span class="p">:</span>
                    <span class="n">_loaded_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">loaded_keys</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">_loaded_keys</span> <span class="o">=</span> <span class="n">loaded_keys</span>
                <span class="n">not_initialized_submodules</span> <span class="o">=</span> <span class="n">set_initialized_submodules</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">_loaded_keys</span><span class="p">)</span>
                <span class="c1"># If we&#39;re about to tie the output embeds to the input embeds we don&#39;t need to init them</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;tie_word_embeddings&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">tie_word_embeddings</span><span class="p">:</span>
                    <span class="n">output_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">output_embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="c1"># Still need to initialize if there is a bias term since biases are not tied.</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">output_embeddings</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">output_embeddings</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="n">output_embeddings</span><span class="o">.</span><span class="n">_is_initialized</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">not_initialized_submodules</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">())</span>
            <span class="c1"># This will only initialize submodules that are not marked as initialized by the line above.</span>
            <span class="n">model</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">)</span>

        <span class="c1"># Set some modules to fp32 if any</span>
        <span class="k">if</span> <span class="n">keep_in_fp32_modules</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">module_to_keep_in_fp32</span> <span class="ow">in</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">module_to_keep_in_fp32</span> <span class="ow">in</span> <span class="n">keep_in_fp32_modules</span><span class="p">):</span>
                    <span class="c1"># param = param.to(mindspore.float32) does not work here as only in the local scope.</span>
                    <span class="c1"># param.data = param.data.to(mindspore.float32)</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">set_dtype</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># Make sure we are able to load base models as well as derived models (with heads)</span>
        <span class="n">start_prefix</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="n">model_to_load</span> <span class="o">=</span> <span class="n">model</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span> <span class="ow">and</span> <span class="n">has_prefix_module</span><span class="p">:</span>
            <span class="n">start_prefix</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">has_prefix_module</span><span class="p">:</span>
            <span class="n">model_to_load</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span>
            <span class="n">base_model_expected_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model_to_load</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">key</span> <span class="ow">in</span> <span class="n">expected_keys_not_prefixed</span> <span class="ow">and</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">base_model_expected_keys</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">loaded_keys</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The state dictionary of the model you are trying to load is corrupted. Are you sure it was &quot;</span>
                    <span class="s2">&quot;properly saved?&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">device_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">device_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">device_map</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="k">def</span> <span class="nf">_find_mismatched_keys</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">model_state_dict</span><span class="p">,</span>
            <span class="n">loaded_keys</span><span class="p">,</span>
            <span class="n">add_prefix_to_model</span><span class="p">,</span>
            <span class="n">remove_prefix_from_model</span><span class="p">,</span>
            <span class="n">ignore_mismatched_sizes</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">mismatched_keys</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="n">ignore_mismatched_sizes</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">checkpoint_key</span> <span class="ow">in</span> <span class="n">loaded_keys</span><span class="p">:</span>
                    <span class="c1"># If the checkpoint is sharded, we may not have the key here.</span>
                    <span class="k">if</span> <span class="n">checkpoint_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">model_key</span> <span class="o">=</span> <span class="n">checkpoint_key</span>
                    <span class="k">if</span> <span class="n">remove_prefix_from_model</span><span class="p">:</span>
                        <span class="c1"># The model key starts with `prefix` but `checkpoint_key` doesn&#39;t so we add it.</span>
                        <span class="n">model_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">checkpoint_key</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="k">elif</span> <span class="n">add_prefix_to_model</span><span class="p">:</span>
                        <span class="c1"># The model key doesn&#39;t start with `prefix` but `checkpoint_key` does so we remove it.</span>
                        <span class="n">model_key</span> <span class="o">=</span> <span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:])</span>

                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">model_key</span> <span class="ow">in</span> <span class="n">model_state_dict</span>
                        <span class="ow">and</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">checkpoint_key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">model_state_dict</span><span class="p">[</span><span class="n">model_key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
                    <span class="p">):</span>
                        <span class="k">if</span> <span class="p">(</span>
                            <span class="n">state_dict</span><span class="p">[</span><span class="n">checkpoint_key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
                            <span class="ow">and</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">checkpoint_key</span><span class="p">]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">==</span> <span class="n">model_state_dict</span><span class="p">[</span><span class="n">model_key</span><span class="p">]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                        <span class="p">):</span>
                            <span class="c1"># This skips size mismatches for 4-bit weights. Two 4-bit values share an 8-bit container, causing size differences.</span>
                            <span class="c1"># Without matching with module type or paramter type it seems like a practical way to detect valid 4bit weights.</span>
                            <span class="k">pass</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">mismatched_keys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                                <span class="p">(</span><span class="n">checkpoint_key</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">checkpoint_key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">model_state_dict</span><span class="p">[</span><span class="n">model_key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                            <span class="p">)</span>
                            <span class="k">del</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">checkpoint_key</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">mismatched_keys</span>

        <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">folder</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">sep</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">sep</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">folder</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">device_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">is_safetensors</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;MindNLP do not support device_map for now.&quot;</span><span class="p">)</span>
            <span class="c1"># param_device_map = expand_device_map(device_map, original_loaded_keys, start_prefix)</span>
            <span class="c1"># str_dtype = str(dtype).replace(&quot;torch.&quot;, &quot;&quot;) if dtype is not None else &quot;float32&quot;</span>
            <span class="c1"># if sharded_metadata is None:</span>
            <span class="c1">#     archive_file = (</span>
            <span class="c1">#         resolved_archive_file[0]</span>
            <span class="c1">#         if isinstance(resolved_archive_file, (list, tuple))</span>
            <span class="c1">#         else resolved_archive_file</span>
            <span class="c1">#     )</span>
            <span class="c1">#     weight_map = {p: archive_file for p in original_loaded_keys}</span>
            <span class="c1"># else:</span>
            <span class="c1">#     weight_map = {p: os.path.join(folder, f) for p, f in sharded_metadata[&quot;weight_map&quot;].items()}</span>
            <span class="c1"># offload_index = {</span>
            <span class="c1">#     p[len(start_prefix) :]: {&quot;safetensors_file&quot;: f, &quot;weight_name&quot;: p, &quot;dtype&quot;: str_dtype}</span>
            <span class="c1">#     for p, f in weight_map.items()</span>
            <span class="c1">#     if p.startswith(start_prefix) and param_device_map[p[len(start_prefix) :]] == &quot;disk&quot;</span>
            <span class="c1"># }</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">offload_index</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">state_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Whole checkpoint</span>
            <span class="n">mismatched_keys</span> <span class="o">=</span> <span class="n">_find_mismatched_keys</span><span class="p">(</span>
                <span class="n">state_dict</span><span class="p">,</span>
                <span class="n">model_state_dict</span><span class="p">,</span>
                <span class="n">original_loaded_keys</span><span class="p">,</span>
                <span class="n">add_prefix_to_model</span><span class="p">,</span>
                <span class="n">remove_prefix_from_model</span><span class="p">,</span>
                <span class="n">ignore_mismatched_sizes</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># For GGUF models `state_dict` is never set to None as the state dict is always small</span>
            <span class="k">if</span> <span class="n">gguf_path</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;MindNLP do not support gguf for now&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Sharded checkpoint or whole but low_cpu_mem_usage==True</span>
                <span class="n">assign_to_params_buffers</span> <span class="o">=</span> <span class="n">check_support_param_buffer_assignment</span><span class="p">(</span>
                    <span class="n">model_to_load</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">start_prefix</span>
                <span class="p">)</span>
                <span class="n">error_msgs</span> <span class="o">=</span> <span class="n">_load_state_dict_into_model</span><span class="p">(</span>
                    <span class="n">model_to_load</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">start_prefix</span><span class="p">,</span> <span class="n">assign_to_params_buffers</span>
                <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># This should always be a list but, just to be sure.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="p">[</span><span class="n">resolved_archive_file</span><span class="p">]</span>

            <span class="n">error_msgs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">mismatched_keys</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_safetensors</span><span class="p">:</span>
                <span class="n">offload_index</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">device_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;disk&quot;</span> <span class="ow">in</span> <span class="n">device_map</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">offload_state_dict</span><span class="p">:</span>
                <span class="n">state_dict_folder</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">()</span>
                <span class="n">state_dict_index</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">state_dict_folder</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">state_dict_index</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="n">is_sharded_safetensors</span><span class="p">:</span>
                <span class="n">disk_only_shard_files</span> <span class="o">=</span> <span class="n">get_disk_only_shard_files</span><span class="p">(</span>
                    <span class="n">device_map</span><span class="p">,</span> <span class="n">sharded_metadata</span><span class="o">=</span><span class="n">sharded_metadata</span><span class="p">,</span> <span class="n">start_prefix</span><span class="o">=</span><span class="n">start_prefix</span>
                <span class="p">)</span>
                <span class="n">disk_only_shard_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">disk_only_shard_files</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">disk_only_shard_files</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Loading checkpoint shards&quot;</span><span class="p">)</span>
            <span class="n">assign_to_params_buffers</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">for</span> <span class="n">shard_file</span> <span class="ow">in</span> <span class="n">resolved_archive_file</span><span class="p">:</span>
                <span class="c1"># Skip the load for shards that only contain disk-offloaded weights when using safetensors for the offload.</span>
                <span class="k">if</span> <span class="n">shard_file</span> <span class="ow">in</span> <span class="n">disk_only_shard_files</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">load_state_dict</span><span class="p">(</span><span class="n">shard_file</span><span class="p">,</span> <span class="n">is_quantized</span><span class="o">=</span><span class="n">is_quantized</span><span class="p">)</span>

                <span class="c1"># Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not</span>
                <span class="c1"># matching the weights in the model.</span>
                <span class="n">mismatched_keys</span> <span class="o">+=</span> <span class="n">_find_mismatched_keys</span><span class="p">(</span>
                    <span class="n">state_dict</span><span class="p">,</span>
                    <span class="n">model_state_dict</span><span class="p">,</span>
                    <span class="n">original_loaded_keys</span><span class="p">,</span>
                    <span class="n">add_prefix_to_model</span><span class="p">,</span>
                    <span class="n">remove_prefix_from_model</span><span class="p">,</span>
                    <span class="n">ignore_mismatched_sizes</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">low_cpu_mem_usage</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;low_cpu_mem usage is not avaliable.&#39;</span><span class="p">)</span>

                <span class="c1"># Sharded checkpoint or whole but low_cpu_mem_usage==True</span>
                <span class="k">if</span> <span class="n">assign_to_params_buffers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">assign_to_params_buffers</span> <span class="o">=</span> <span class="n">check_support_param_buffer_assignment</span><span class="p">(</span>
                        <span class="n">model_to_load</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">start_prefix</span>
                    <span class="p">)</span>
                <span class="n">error_msgs</span> <span class="o">+=</span> <span class="n">_load_state_dict_into_model</span><span class="p">(</span>
                    <span class="n">model_to_load</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">start_prefix</span><span class="p">,</span> <span class="n">assign_to_params_buffers</span>
                <span class="p">)</span>

                <span class="c1"># force memory release</span>
                <span class="k">del</span> <span class="n">state_dict</span>
                <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_msg</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n\t</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;size mismatch&quot;</span> <span class="ow">in</span> <span class="n">error_msg</span><span class="p">:</span>
                <span class="n">error_msg</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="s2">&quot;</span><span class="se">\n\t</span><span class="s2">You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.&quot;</span>
                <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error(s) in loading state_dict for </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">:</span><span class="se">\n\t</span><span class="si">{</span><span class="n">error_msg</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unexpected_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">archs</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">architectures</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">architectures</span>
            <span class="n">warner</span> <span class="o">=</span> <span class="n">logger</span><span class="o">.</span><span class="n">warning</span> <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="ow">in</span> <span class="n">archs</span> <span class="k">else</span> <span class="n">logger</span><span class="o">.</span><span class="n">info</span>
            <span class="n">warner</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Some weights of the model checkpoint at </span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2"> were not used when&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; initializing </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">unexpected_keys</span><span class="si">}</span><span class="se">\n</span><span class="s2">- This IS expected if you are&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; initializing </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> from the checkpoint of a model trained on another task or&quot;</span>
                <span class="s2">&quot; with another architecture (e.g. initializing a BertForSequenceClassification model from a&quot;</span>
                <span class="s2">&quot; BertForPreTraining model).</span><span class="se">\n</span><span class="s2">- This IS NOT expected if you are initializing&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> from the checkpoint of a model that you expect to be exactly identical&quot;</span>
                <span class="s2">&quot; (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;All model checkpoint weights were used when initializing </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Some weights of </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> were not initialized from the model checkpoint at&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2"> and are newly initialized: </span><span class="si">{</span><span class="n">missing_keys</span><span class="si">}</span><span class="se">\n</span><span class="s2">You should probably&quot;</span>
                <span class="s2">&quot; TRAIN this model on a down-stream task to be able to use it for predictions and inference.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">mismatched_keys</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;All the weights of </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> were initialized from the model checkpoint at&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">.</span><span class="se">\n</span><span class="s2">If your task is similar to the task the model of the checkpoint&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; was trained on, you can already use </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> for predictions without further&quot;</span>
                <span class="s2">&quot; training.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mismatched_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">mismatched_warning</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="sa">f</span><span class="s2">&quot;- </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: found shape </span><span class="si">{</span><span class="n">shape1</span><span class="si">}</span><span class="s2"> in the checkpoint and </span><span class="si">{</span><span class="n">shape2</span><span class="si">}</span><span class="s2"> in the model instantiated&quot;</span>
                    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">shape1</span><span class="p">,</span> <span class="n">shape2</span> <span class="ow">in</span> <span class="n">mismatched_keys</span>
                <span class="p">]</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Some weights of </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> were not initialized from the model checkpoint at&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2"> and are newly initialized because the shapes did not&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; match:</span><span class="se">\n</span><span class="si">{</span><span class="n">mismatched_warning</span><span class="si">}</span><span class="se">\n</span><span class="s2">You should probably TRAIN this model on a down-stream task to be able&quot;</span>
                <span class="s2">&quot; to use it for predictions and inference.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">,</span> <span class="n">mismatched_keys</span><span class="p">,</span> <span class="n">offload_index</span><span class="p">,</span> <span class="n">error_msgs</span>

    <span class="k">def</span> <span class="nf">retrieve_modules_from_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">add_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">remove_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">module_keys</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">names</span><span class="p">}</span>

        <span class="c1"># nn.ParameterList is a special case where two parameter keywords</span>
        <span class="c1"># are appended to the module name, *e.g.* bert.special_embeddings.0</span>
        <span class="n">module_keys</span> <span class="o">=</span> <span class="n">module_keys</span><span class="o">.</span><span class="n">union</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">names</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">key</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">isdigit</span><span class="p">()}</span>
        <span class="p">)</span>

        <span class="n">retrieved_modules</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># retrieve all modules that has at least one missing weight name</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">remove_prefix</span><span class="p">:</span>
                <span class="n">_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="n">name</span> <span class="o">=</span> <span class="n">name</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">_prefix</span><span class="p">)</span> <span class="p">:]</span> <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">_prefix</span><span class="p">)</span> <span class="k">else</span> <span class="n">name</span>
            <span class="k">elif</span> <span class="n">add_prefix</span><span class="p">:</span>
                <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="n">name</span><span class="p">])</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span>

            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">module_keys</span><span class="p">:</span>
                <span class="n">retrieved_modules</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">retrieved_modules</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">register_for_auto_class</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">auto_class</span><span class="o">=</span><span class="s2">&quot;AutoModel&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Register this class with a given auto class. This should only be used for custom models as the ones in the</span>
<span class="sd">        library are already mapped with an auto class.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This API is experimental and may have some slight breaking changes in the next releases.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            auto_class (`str` or `type`, *optional*, defaults to `&quot;AutoModel&quot;`):</span>
<span class="sd">                The auto class to register this new model with.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">auto_class</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">auto_class</span> <span class="o">=</span> <span class="n">auto_class</span><span class="o">.</span><span class="vm">__name__</span>

        <span class="kn">import</span> <span class="nn">mindnlp.transformers.models.auto</span> <span class="k">as</span> <span class="nn">auto_module</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">auto_module</span><span class="p">,</span> <span class="n">auto_class</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">auto_class</span><span class="si">}</span><span class="s2"> is not a valid auto class.&quot;</span><span class="p">)</span>

        <span class="bp">cls</span><span class="o">.</span><span class="n">_auto_class</span> <span class="o">=</span> <span class="n">auto_class</span>

    <span class="k">def</span> <span class="nf">warn_if_padding_and_no_attention_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Shows a one-time warning if the input_ids appear to contain padding and no attention mask was given.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Check only the first and last input IDs to reduce overhead.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()[:,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]:</span>
            <span class="n">warn_string</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See &quot;</span>
                <span class="s2">&quot;https://huggingface.co/docs/transformers/troubleshooting&quot;</span>
                <span class="s2">&quot;#incorrect-output-when-padding-tokens-arent-masked.&quot;</span>
            <span class="p">)</span>

            <span class="c1"># If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an</span>
            <span class="c1"># attention_mask or not. In this case, we should still show a warning because this is a rare case.</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bos_token_id</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
                <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
                <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sep_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sep_token_id</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">warn_string</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">You may ignore this warning if your `pad_token_id` (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="si">}</span><span class="s2">) is identical &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;to the `bos_token_id` (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bos_token_id</span><span class="si">}</span><span class="s2">), `eos_token_id` (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="si">}</span><span class="s2">), &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;or the `sep_token_id` (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sep_token_id</span><span class="si">}</span><span class="s2">), and your input is not padded.&quot;</span>
                <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span><span class="n">warn_string</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.base_model" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">base_model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.base_model" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p><code>nn.Module</code>: The main body of the model.</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.dummy_inputs" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">dummy_inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.dummy_inputs" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p><code>Dict[str, mindspore.Tensor]</code>: Dummy inputs to do a forward pass in the network.</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.framework" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">framework</span><span class="p">:</span> <span class="nb">str</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.framework" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>:str: Identifies that this is a PyTorch model.</p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.is_gradient_checkpointing" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">is_gradient_checkpointing</span><span class="p">:</span> <span class="nb">bool</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.is_gradient_checkpointing" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Whether gradient checkpointing is activated for this model or not.</p>
<p>Note that in other frameworks this feature can be referred to as "activation checkpointing" or "checkpoint
activations".</p>
    </div>

</div>



<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.add_model_tags" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">add_model_tags</span><span class="p">(</span><span class="n">tags</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.add_model_tags" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Add custom tags into the model that gets pushed to the Hugging Face Hub. Will
not overwrite existing tags in the model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>tags</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The desired tags to inject in the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[List[str], str]`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-cased&quot;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">add_model_tags</span><span class="p">([</span><span class="s2">&quot;custom&quot;</span><span class="p">,</span> <span class="s2">&quot;custom-bert&quot;</span><span class="p">])</span>

<span class="c1"># Push the model to your namespace with the name &quot;my-custom-bert&quot;.</span>
<span class="n">model</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">&quot;my-custom-bert&quot;</span><span class="p">)</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">add_model_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tags</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add custom tags into the model that gets pushed to the Hugging Face Hub. Will</span>
<span class="sd">    not overwrite existing tags in the model.</span>

<span class="sd">    Args:</span>
<span class="sd">        tags (`Union[List[str], str]`):</span>
<span class="sd">            The desired tags to inject in the model</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    from transformers import AutoModel</span>

<span class="sd">    model = AutoModel.from_pretrained(&quot;google-bert/bert-base-cased&quot;)</span>

<span class="sd">    model.add_model_tags([&quot;custom&quot;, &quot;custom-bert&quot;])</span>

<span class="sd">    # Push the model to your namespace with the name &quot;my-custom-bert&quot;.</span>
<span class="sd">    model.push_to_hub(&quot;my-custom-bert&quot;)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tags</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">tags</span> <span class="o">=</span> <span class="p">[</span><span class="n">tags</span><span class="p">]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_tags</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_tags</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">tag</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_tags</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_tags</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.can_generate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">can_generate</span><span class="p">()</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.can_generate" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns whether this model can generate sequences with <code>.generate()</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code>bool</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>bool</code>: Whether this model can generate sequences with <code>.generate()</code>.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">can_generate</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns whether this model can generate sequences with `.generate()`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `bool`: Whether this model can generate sequences with `.generate()`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Detects whether `prepare_inputs_for_generation` has been overwritten, which is a requirement for generation.</span>
    <span class="c1"># Alternativelly, the model can also have a custom `generate` function.</span>
    <span class="k">if</span> <span class="s2">&quot;GenerationMixin&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;GenerationMixin&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">generate</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.dequantize" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">dequantize</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.dequantize" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Potentially dequantize the model in case it has been quantized by a quantization method that support
dequantization.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">dequantize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Potentially dequantize the model in case it has been quantized by a quantization method that support</span>
<span class="sd">    dequantization.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hf_quantizer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;hf_quantizer&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">hf_quantizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You need to first quantize your model in order to dequantize it&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">hf_quantizer</span><span class="o">.</span><span class="n">dequantize</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.disable_input_require_grads" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">disable_input_require_grads</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.disable_input_require_grads" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Removes the <code>_require_grads_hook</code>.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">disable_input_require_grads</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Removes the `_require_grads_hook`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_require_grads_hook</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.enable_input_require_grads" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">enable_input_require_grads</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.enable_input_require_grads" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while keeping
the model weights fixed.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">enable_input_require_grads</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while keeping</span>
<span class="sd">    the model weights fixed.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">make_inputs_require_grads</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="n">output</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_require_grads_hook</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">make_inputs_require_grads</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.from_pretrained" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">*</span><span class="n">model_args</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_mismatched_sizes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">force_download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">local_files_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="s1">&#39;main&#39;</span><span class="p">,</span> <span class="n">use_safetensors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.from_pretrained" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Instantiate a pretrained pytorch model from a pre-trained model configuration.</p>
<p>The model is set in evaluation mode by default using <code>model.eval()</code> (Dropout modules are deactivated). To train
the model, you should first set it back in training mode with <code>model.train()</code>.</p>
<p>The warning <em>Weights from XXX not initialized from pretrained model</em> means that the weights of XXX do not come
pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
task.</p>
<p>The warning <em>Weights from XXX not used in YYY</em> means that the layer XXX is not used by YYY, therefore those
weights are discarded.</p>
<p>If model weights are the same precision as the base model (and is a supported model), weights will be lazily loaded
in using the <code>meta</code> device and brought into memory once an input is passed through that layer regardless of
<code>low_cpu_mem_usage</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
- A path to a *directory* containing model weights saved using
  [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.
- A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In
  this case, `from_tf` should be set to `True` and a configuration object should be provided as
  `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a
  PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.
- A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,
  `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to
  `True`.
- `None` if you are both providing the configuration and state dictionary (resp. with keyword
  arguments `config` and `state_dict`).
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>model_args</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>All remaining positional arguments will be passed to the underlying model's <code>__init__</code> method.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>sequence of positional arguments, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>()</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- an instance of a class derived from [`PretrainedConfig`],
- a string or path valid as input to [`~PretrainedConfig.from_pretrained`].
</code></pre></div>
<p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<div class="highlight"><pre><span></span><code>- The model is a model provided by the library (loaded with the *model id* string of a pretrained
  model).
- The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the
  save directory.
- The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a
  configuration JSON file named *config.json* is found in the directory.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[PretrainedConfig, str, os.PathLike]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using [<code>~PreTrainedModel.save_pretrained</code>] and
[<code>~PreTrainedModel.from_pretrained</code>] is not a simpler option.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, mindspore.Tensor]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_dir</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>from_tf</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>from_flax</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Load the model weights from a Flax checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ignore_mismatched_sizes</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to raise an error if some of the weights from the checkpoint do not have the same size
as the weights of the model (if for instance, you are instantiating a model with 10 labels from a
checkpoint with 3 labels).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_download</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resume_download</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proxies</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_loading_info(`bool`,</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>*optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>local_files_only(`bool`,</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to only look at local files (i.e., do not try to download the model).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>*optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, or not specified, will use
the token generated when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>revision</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.</p>
<p><Tip></p>
<p>To test a pull request you made on the Hub, you can pass `revision="refs/pr/<pr_number>".</p>
<p></Tip></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;main&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mirror</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Mirror source to accelerate downloads in China. If you are from China and have an accessibility
problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.
Please refer to the mirror site for more information.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>_fast_init(`bool`,</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to disable fast initialization.</p>
<p><Tip warning={true}></p>
<p>One should only disable <em>_fast_init</em> to ensure backwards compatibility with <code>transformers.__version__ &lt;
4.6.0</code> for seeded model initialization. This argument will be removed at the next major version. See
<a href="https://github.com/huggingface/transformers/pull/11471">pull request 11471</a> for more information.</p>
<p></Tip></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>*optional*, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_implementation</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The attention implementation to use in the model (if relevant). The default is otherwise the manual <code>"eager"</code> implementation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>low_cpu_mem_usage(`bool`,</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.
Generally should be combined with a <code>device_map</code> (such as <code>"auto"</code>) for best results.
This is an experimental feature and a subject to change at any moment.
</Tip>
    If the model weights are in the same precision as the model loaded in, <code>low_cpu_mem_usage</code> (without
    <code>device_map</code>) is redundant and will not provide any benefit in regards to CPU memory usage. However,
    this should still be enabled if you are passing in a <code>device_map</code>.
</Tip></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>*optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ms_dtype</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Override the default <code>mindspore.dtype.TensorType</code> and load the model under a specific <code>dtype</code>. The different options
are:</p>
<ol>
<li>
<p><code>torch.float16</code> or <code>torch.bfloat16</code> or <code>torch.float</code>: load in a specified
  <code>dtype</code>, ignoring the model's <code>config.ms_dtype</code> if one exists. If not specified
  - the model will get loaded in <code>torch.float</code> (fp32).</p>
</li>
<li>
<p><code>"auto"</code> - A <code>ms_dtype</code> entry in the <code>config.json</code> file of the model will be
  attempted to be used. If this entry isn't found then next check the <code>dtype</code> of the first weight in
  the checkpoint that's of a floating point type and use that as <code>dtype</code>. This will load the model
  using the <code>dtype</code> it was saved in at the end of the training. It can't be used as an indicator of how
  the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.</p>
</li>
<li>
<p>A string that is a valid <code>mindspore.dtype.TensorType</code>. E.g. "float32" loads the model in <code>mindspore.float32</code>, "float16" loads in <code>torch.float16</code> etc.</p>
</li>
</ol>
<p><Tip></p>
<p>For some models the <code>dtype</code> they were trained in is unknown - you may try to check the model's paper or
reach out to the authors and ask them to add this information to the model's card and to insert the
<code>ms_dtype</code> entry in <code>config.json</code> on the hub.</p>
<p></Tip></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `mindspore.dtype.TensorType`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device_map</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A map that specifies where each submodule should go. It doesn't need to be refined to each
parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
same device. If we only pass the device (<em>e.g.</em>, <code>"cpu"</code>, <code>"cuda:1"</code>, <code>"mps"</code>, or a GPU ordinal rank
like <code>1</code>) on which the model will be allocated, the device map will map the entire model to this
device. Passing <code>device_map = 0</code> means put the whole model on GPU 0.</p>
<p>To have Accelerate compute the most optimized <code>device_map</code> automatically, set <code>device_map="auto"</code>. For
more information about each option see <a href="https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map">designing a device
map</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `Dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_memory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary device identifier to maximum memory. Will default to the maximum memory available for each
GPU and the available CPU RAM if unset.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>offload_folder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If the <code>device_map</code> contains any value <code>"disk"</code>, the folder where we will offload weights.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>offload_state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If <code>True</code>, will temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU
RAM if the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to
<code>True</code> when there is some disk offload.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>offload_buffers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to offload the buffers with the model parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>quantization_config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g
bitsandbytes, gptq). There may be other quantization-related kwargs, including <code>load_in_4bit</code> and
<code>load_in_8bit</code>, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes
quantizations and not preferred. consider inserting all such arguments into quantization_config
instead.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[QuantizationConfigMixin,Dict]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>subfolder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
specify the folder name here.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>variant</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If specified load weights from <code>variant</code> filename, <em>e.g.</em> pytorch_model.<variant>.bin. <code>variant</code> is
ignored when using <code>from_tf</code> or <code>from_flax</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_safetensors</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to use <code>safetensors</code> checkpoints. Defaults to <code>None</code>. If not specified and <code>safetensors</code>
is not installed, it will be set to <code>False</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `None`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<div class="highlight"><pre><span></span><code>- If a configuration is provided with `config`, `**kwargs` will be directly passed to the
  underlying model&#39;s `__init__` method (we assume all relevant updates to the configuration have
  already been done)
- If a configuration is not provided, `kwargs` will be first passed to the configuration class
  initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that
  corresponds to a configuration attribute will be used to override said attribute with the
  supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute
  will be passed to the underlying model&#39;s `__init__` function.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>remaining dictionary of keyword arguments, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <p><Tip></p>
<p>Activate the special <a href="https://huggingface.co/transformers/installation.html#offline-mode">"offline-mode"</a> to
use this method in a firewalled environment.</p>
<p></Tip></p>
<p>Examples:</p>
<div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertConfig</span><span class="p">,</span> <span class="n">BertModel</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-uncased&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Model was saved using *save_pretrained(&#39;./test/saved_model/&#39;)* (for example purposes, not runnable).</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./test/saved_model/&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Update configuration during loading.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-uncased&quot;</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span> <span class="o">==</span> <span class="kc">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s2">&quot;./tf_model/my_tf_model_config.json&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./tf_model/my_tf_checkpoint.ckpt.index&quot;</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Loading from a Flax checkpoint file instead of a PyTorch model (slower)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-uncased&quot;</span><span class="p">,</span> <span class="n">from_flax</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<ul>
<li><code>low_cpu_mem_usage</code> algorithm:</li>
</ul>
<p>This is an experimental function that loads the model using ~1x model size CPU memory</p>
<p>Here is how it works:</p>
<ol>
<li>save which state_dict keys we have</li>
<li>drop state_dict before the model is created, since the latter takes 1x model size CPU memory</li>
<li>after the model has been instantiated switch to the meta device all params/buffers that
are going to be replaced from the loaded state_dict</li>
<li>load state_dict 2<sup>nd</sup> time</li>
<li>replace the params/buffers from the state_dict</li>
</ol>
<p>Currently, it can't handle deepspeed ZeRO stage 3 and ignores loading errors</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2180</span>
<span class="normal">2181</span>
<span class="normal">2182</span>
<span class="normal">2183</span>
<span class="normal">2184</span>
<span class="normal">2185</span>
<span class="normal">2186</span>
<span class="normal">2187</span>
<span class="normal">2188</span>
<span class="normal">2189</span>
<span class="normal">2190</span>
<span class="normal">2191</span>
<span class="normal">2192</span>
<span class="normal">2193</span>
<span class="normal">2194</span>
<span class="normal">2195</span>
<span class="normal">2196</span>
<span class="normal">2197</span>
<span class="normal">2198</span>
<span class="normal">2199</span>
<span class="normal">2200</span>
<span class="normal">2201</span>
<span class="normal">2202</span>
<span class="normal">2203</span>
<span class="normal">2204</span>
<span class="normal">2205</span>
<span class="normal">2206</span>
<span class="normal">2207</span>
<span class="normal">2208</span>
<span class="normal">2209</span>
<span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span>
<span class="normal">2219</span>
<span class="normal">2220</span>
<span class="normal">2221</span>
<span class="normal">2222</span>
<span class="normal">2223</span>
<span class="normal">2224</span>
<span class="normal">2225</span>
<span class="normal">2226</span>
<span class="normal">2227</span>
<span class="normal">2228</span>
<span class="normal">2229</span>
<span class="normal">2230</span>
<span class="normal">2231</span>
<span class="normal">2232</span>
<span class="normal">2233</span>
<span class="normal">2234</span>
<span class="normal">2235</span>
<span class="normal">2236</span>
<span class="normal">2237</span>
<span class="normal">2238</span>
<span class="normal">2239</span>
<span class="normal">2240</span>
<span class="normal">2241</span>
<span class="normal">2242</span>
<span class="normal">2243</span>
<span class="normal">2244</span>
<span class="normal">2245</span>
<span class="normal">2246</span>
<span class="normal">2247</span>
<span class="normal">2248</span>
<span class="normal">2249</span>
<span class="normal">2250</span>
<span class="normal">2251</span>
<span class="normal">2252</span>
<span class="normal">2253</span>
<span class="normal">2254</span>
<span class="normal">2255</span>
<span class="normal">2256</span>
<span class="normal">2257</span>
<span class="normal">2258</span>
<span class="normal">2259</span>
<span class="normal">2260</span>
<span class="normal">2261</span>
<span class="normal">2262</span>
<span class="normal">2263</span>
<span class="normal">2264</span>
<span class="normal">2265</span>
<span class="normal">2266</span>
<span class="normal">2267</span>
<span class="normal">2268</span>
<span class="normal">2269</span>
<span class="normal">2270</span>
<span class="normal">2271</span>
<span class="normal">2272</span>
<span class="normal">2273</span>
<span class="normal">2274</span>
<span class="normal">2275</span>
<span class="normal">2276</span>
<span class="normal">2277</span>
<span class="normal">2278</span>
<span class="normal">2279</span>
<span class="normal">2280</span>
<span class="normal">2281</span>
<span class="normal">2282</span>
<span class="normal">2283</span>
<span class="normal">2284</span>
<span class="normal">2285</span>
<span class="normal">2286</span>
<span class="normal">2287</span>
<span class="normal">2288</span>
<span class="normal">2289</span>
<span class="normal">2290</span>
<span class="normal">2291</span>
<span class="normal">2292</span>
<span class="normal">2293</span>
<span class="normal">2294</span>
<span class="normal">2295</span>
<span class="normal">2296</span>
<span class="normal">2297</span>
<span class="normal">2298</span>
<span class="normal">2299</span>
<span class="normal">2300</span>
<span class="normal">2301</span>
<span class="normal">2302</span>
<span class="normal">2303</span>
<span class="normal">2304</span>
<span class="normal">2305</span>
<span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span>
<span class="normal">2313</span>
<span class="normal">2314</span>
<span class="normal">2315</span>
<span class="normal">2316</span>
<span class="normal">2317</span>
<span class="normal">2318</span>
<span class="normal">2319</span>
<span class="normal">2320</span>
<span class="normal">2321</span>
<span class="normal">2322</span>
<span class="normal">2323</span>
<span class="normal">2324</span>
<span class="normal">2325</span>
<span class="normal">2326</span>
<span class="normal">2327</span>
<span class="normal">2328</span>
<span class="normal">2329</span>
<span class="normal">2330</span>
<span class="normal">2331</span>
<span class="normal">2332</span>
<span class="normal">2333</span>
<span class="normal">2334</span>
<span class="normal">2335</span>
<span class="normal">2336</span>
<span class="normal">2337</span>
<span class="normal">2338</span>
<span class="normal">2339</span>
<span class="normal">2340</span>
<span class="normal">2341</span>
<span class="normal">2342</span>
<span class="normal">2343</span>
<span class="normal">2344</span>
<span class="normal">2345</span>
<span class="normal">2346</span>
<span class="normal">2347</span>
<span class="normal">2348</span>
<span class="normal">2349</span>
<span class="normal">2350</span>
<span class="normal">2351</span>
<span class="normal">2352</span>
<span class="normal">2353</span>
<span class="normal">2354</span>
<span class="normal">2355</span>
<span class="normal">2356</span>
<span class="normal">2357</span>
<span class="normal">2358</span>
<span class="normal">2359</span>
<span class="normal">2360</span>
<span class="normal">2361</span>
<span class="normal">2362</span>
<span class="normal">2363</span>
<span class="normal">2364</span>
<span class="normal">2365</span>
<span class="normal">2366</span>
<span class="normal">2367</span>
<span class="normal">2368</span>
<span class="normal">2369</span>
<span class="normal">2370</span>
<span class="normal">2371</span>
<span class="normal">2372</span>
<span class="normal">2373</span>
<span class="normal">2374</span>
<span class="normal">2375</span>
<span class="normal">2376</span>
<span class="normal">2377</span>
<span class="normal">2378</span>
<span class="normal">2379</span>
<span class="normal">2380</span>
<span class="normal">2381</span>
<span class="normal">2382</span>
<span class="normal">2383</span>
<span class="normal">2384</span>
<span class="normal">2385</span>
<span class="normal">2386</span>
<span class="normal">2387</span>
<span class="normal">2388</span>
<span class="normal">2389</span>
<span class="normal">2390</span>
<span class="normal">2391</span>
<span class="normal">2392</span>
<span class="normal">2393</span>
<span class="normal">2394</span>
<span class="normal">2395</span>
<span class="normal">2396</span>
<span class="normal">2397</span>
<span class="normal">2398</span>
<span class="normal">2399</span>
<span class="normal">2400</span>
<span class="normal">2401</span>
<span class="normal">2402</span>
<span class="normal">2403</span>
<span class="normal">2404</span>
<span class="normal">2405</span>
<span class="normal">2406</span>
<span class="normal">2407</span>
<span class="normal">2408</span>
<span class="normal">2409</span>
<span class="normal">2410</span>
<span class="normal">2411</span>
<span class="normal">2412</span>
<span class="normal">2413</span>
<span class="normal">2414</span>
<span class="normal">2415</span>
<span class="normal">2416</span>
<span class="normal">2417</span>
<span class="normal">2418</span>
<span class="normal">2419</span>
<span class="normal">2420</span>
<span class="normal">2421</span>
<span class="normal">2422</span>
<span class="normal">2423</span>
<span class="normal">2424</span>
<span class="normal">2425</span>
<span class="normal">2426</span>
<span class="normal">2427</span>
<span class="normal">2428</span>
<span class="normal">2429</span>
<span class="normal">2430</span>
<span class="normal">2431</span>
<span class="normal">2432</span>
<span class="normal">2433</span>
<span class="normal">2434</span>
<span class="normal">2435</span>
<span class="normal">2436</span>
<span class="normal">2437</span>
<span class="normal">2438</span>
<span class="normal">2439</span>
<span class="normal">2440</span>
<span class="normal">2441</span>
<span class="normal">2442</span>
<span class="normal">2443</span>
<span class="normal">2444</span>
<span class="normal">2445</span>
<span class="normal">2446</span>
<span class="normal">2447</span>
<span class="normal">2448</span>
<span class="normal">2449</span>
<span class="normal">2450</span>
<span class="normal">2451</span>
<span class="normal">2452</span>
<span class="normal">2453</span>
<span class="normal">2454</span>
<span class="normal">2455</span>
<span class="normal">2456</span>
<span class="normal">2457</span>
<span class="normal">2458</span>
<span class="normal">2459</span>
<span class="normal">2460</span>
<span class="normal">2461</span>
<span class="normal">2462</span>
<span class="normal">2463</span>
<span class="normal">2464</span>
<span class="normal">2465</span>
<span class="normal">2466</span>
<span class="normal">2467</span>
<span class="normal">2468</span>
<span class="normal">2469</span>
<span class="normal">2470</span>
<span class="normal">2471</span>
<span class="normal">2472</span>
<span class="normal">2473</span>
<span class="normal">2474</span>
<span class="normal">2475</span>
<span class="normal">2476</span>
<span class="normal">2477</span>
<span class="normal">2478</span>
<span class="normal">2479</span>
<span class="normal">2480</span>
<span class="normal">2481</span>
<span class="normal">2482</span>
<span class="normal">2483</span>
<span class="normal">2484</span>
<span class="normal">2485</span>
<span class="normal">2486</span>
<span class="normal">2487</span>
<span class="normal">2488</span>
<span class="normal">2489</span>
<span class="normal">2490</span>
<span class="normal">2491</span>
<span class="normal">2492</span>
<span class="normal">2493</span>
<span class="normal">2494</span>
<span class="normal">2495</span>
<span class="normal">2496</span>
<span class="normal">2497</span>
<span class="normal">2498</span>
<span class="normal">2499</span>
<span class="normal">2500</span>
<span class="normal">2501</span>
<span class="normal">2502</span>
<span class="normal">2503</span>
<span class="normal">2504</span>
<span class="normal">2505</span>
<span class="normal">2506</span>
<span class="normal">2507</span>
<span class="normal">2508</span>
<span class="normal">2509</span>
<span class="normal">2510</span>
<span class="normal">2511</span>
<span class="normal">2512</span>
<span class="normal">2513</span>
<span class="normal">2514</span>
<span class="normal">2515</span>
<span class="normal">2516</span>
<span class="normal">2517</span>
<span class="normal">2518</span>
<span class="normal">2519</span>
<span class="normal">2520</span>
<span class="normal">2521</span>
<span class="normal">2522</span>
<span class="normal">2523</span>
<span class="normal">2524</span>
<span class="normal">2525</span>
<span class="normal">2526</span>
<span class="normal">2527</span>
<span class="normal">2528</span>
<span class="normal">2529</span>
<span class="normal">2530</span>
<span class="normal">2531</span>
<span class="normal">2532</span>
<span class="normal">2533</span>
<span class="normal">2534</span>
<span class="normal">2535</span>
<span class="normal">2536</span>
<span class="normal">2537</span>
<span class="normal">2538</span>
<span class="normal">2539</span>
<span class="normal">2540</span>
<span class="normal">2541</span>
<span class="normal">2542</span>
<span class="normal">2543</span>
<span class="normal">2544</span>
<span class="normal">2545</span>
<span class="normal">2546</span>
<span class="normal">2547</span>
<span class="normal">2548</span>
<span class="normal">2549</span>
<span class="normal">2550</span>
<span class="normal">2551</span>
<span class="normal">2552</span>
<span class="normal">2553</span>
<span class="normal">2554</span>
<span class="normal">2555</span>
<span class="normal">2556</span>
<span class="normal">2557</span>
<span class="normal">2558</span>
<span class="normal">2559</span>
<span class="normal">2560</span>
<span class="normal">2561</span>
<span class="normal">2562</span>
<span class="normal">2563</span>
<span class="normal">2564</span>
<span class="normal">2565</span>
<span class="normal">2566</span>
<span class="normal">2567</span>
<span class="normal">2568</span>
<span class="normal">2569</span>
<span class="normal">2570</span>
<span class="normal">2571</span>
<span class="normal">2572</span>
<span class="normal">2573</span>
<span class="normal">2574</span>
<span class="normal">2575</span>
<span class="normal">2576</span>
<span class="normal">2577</span>
<span class="normal">2578</span>
<span class="normal">2579</span>
<span class="normal">2580</span>
<span class="normal">2581</span>
<span class="normal">2582</span>
<span class="normal">2583</span>
<span class="normal">2584</span>
<span class="normal">2585</span>
<span class="normal">2586</span>
<span class="normal">2587</span>
<span class="normal">2588</span>
<span class="normal">2589</span>
<span class="normal">2590</span>
<span class="normal">2591</span>
<span class="normal">2592</span>
<span class="normal">2593</span>
<span class="normal">2594</span>
<span class="normal">2595</span>
<span class="normal">2596</span>
<span class="normal">2597</span>
<span class="normal">2598</span>
<span class="normal">2599</span>
<span class="normal">2600</span>
<span class="normal">2601</span>
<span class="normal">2602</span>
<span class="normal">2603</span>
<span class="normal">2604</span>
<span class="normal">2605</span>
<span class="normal">2606</span>
<span class="normal">2607</span>
<span class="normal">2608</span>
<span class="normal">2609</span>
<span class="normal">2610</span>
<span class="normal">2611</span>
<span class="normal">2612</span>
<span class="normal">2613</span>
<span class="normal">2614</span>
<span class="normal">2615</span>
<span class="normal">2616</span>
<span class="normal">2617</span>
<span class="normal">2618</span>
<span class="normal">2619</span>
<span class="normal">2620</span>
<span class="normal">2621</span>
<span class="normal">2622</span>
<span class="normal">2623</span>
<span class="normal">2624</span>
<span class="normal">2625</span>
<span class="normal">2626</span>
<span class="normal">2627</span>
<span class="normal">2628</span>
<span class="normal">2629</span>
<span class="normal">2630</span>
<span class="normal">2631</span>
<span class="normal">2632</span>
<span class="normal">2633</span>
<span class="normal">2634</span>
<span class="normal">2635</span>
<span class="normal">2636</span>
<span class="normal">2637</span>
<span class="normal">2638</span>
<span class="normal">2639</span>
<span class="normal">2640</span>
<span class="normal">2641</span>
<span class="normal">2642</span>
<span class="normal">2643</span>
<span class="normal">2644</span>
<span class="normal">2645</span>
<span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span>
<span class="normal">2652</span>
<span class="normal">2653</span>
<span class="normal">2654</span>
<span class="normal">2655</span>
<span class="normal">2656</span>
<span class="normal">2657</span>
<span class="normal">2658</span>
<span class="normal">2659</span>
<span class="normal">2660</span>
<span class="normal">2661</span>
<span class="normal">2662</span>
<span class="normal">2663</span>
<span class="normal">2664</span>
<span class="normal">2665</span>
<span class="normal">2666</span>
<span class="normal">2667</span>
<span class="normal">2668</span>
<span class="normal">2669</span>
<span class="normal">2670</span>
<span class="normal">2671</span>
<span class="normal">2672</span>
<span class="normal">2673</span>
<span class="normal">2674</span>
<span class="normal">2675</span>
<span class="normal">2676</span>
<span class="normal">2677</span>
<span class="normal">2678</span>
<span class="normal">2679</span>
<span class="normal">2680</span>
<span class="normal">2681</span>
<span class="normal">2682</span>
<span class="normal">2683</span>
<span class="normal">2684</span>
<span class="normal">2685</span>
<span class="normal">2686</span>
<span class="normal">2687</span>
<span class="normal">2688</span>
<span class="normal">2689</span>
<span class="normal">2690</span>
<span class="normal">2691</span>
<span class="normal">2692</span>
<span class="normal">2693</span>
<span class="normal">2694</span>
<span class="normal">2695</span>
<span class="normal">2696</span>
<span class="normal">2697</span>
<span class="normal">2698</span>
<span class="normal">2699</span>
<span class="normal">2700</span>
<span class="normal">2701</span>
<span class="normal">2702</span>
<span class="normal">2703</span>
<span class="normal">2704</span>
<span class="normal">2705</span>
<span class="normal">2706</span>
<span class="normal">2707</span>
<span class="normal">2708</span>
<span class="normal">2709</span>
<span class="normal">2710</span>
<span class="normal">2711</span>
<span class="normal">2712</span>
<span class="normal">2713</span>
<span class="normal">2714</span>
<span class="normal">2715</span>
<span class="normal">2716</span>
<span class="normal">2717</span>
<span class="normal">2718</span>
<span class="normal">2719</span>
<span class="normal">2720</span>
<span class="normal">2721</span>
<span class="normal">2722</span>
<span class="normal">2723</span>
<span class="normal">2724</span>
<span class="normal">2725</span>
<span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span>
<span class="normal">2733</span>
<span class="normal">2734</span>
<span class="normal">2735</span>
<span class="normal">2736</span>
<span class="normal">2737</span>
<span class="normal">2738</span>
<span class="normal">2739</span>
<span class="normal">2740</span>
<span class="normal">2741</span>
<span class="normal">2742</span>
<span class="normal">2743</span>
<span class="normal">2744</span>
<span class="normal">2745</span>
<span class="normal">2746</span>
<span class="normal">2747</span>
<span class="normal">2748</span>
<span class="normal">2749</span>
<span class="normal">2750</span>
<span class="normal">2751</span>
<span class="normal">2752</span>
<span class="normal">2753</span>
<span class="normal">2754</span>
<span class="normal">2755</span>
<span class="normal">2756</span>
<span class="normal">2757</span>
<span class="normal">2758</span>
<span class="normal">2759</span>
<span class="normal">2760</span>
<span class="normal">2761</span>
<span class="normal">2762</span>
<span class="normal">2763</span>
<span class="normal">2764</span>
<span class="normal">2765</span>
<span class="normal">2766</span>
<span class="normal">2767</span>
<span class="normal">2768</span>
<span class="normal">2769</span>
<span class="normal">2770</span>
<span class="normal">2771</span>
<span class="normal">2772</span>
<span class="normal">2773</span>
<span class="normal">2774</span>
<span class="normal">2775</span>
<span class="normal">2776</span>
<span class="normal">2777</span>
<span class="normal">2778</span>
<span class="normal">2779</span>
<span class="normal">2780</span>
<span class="normal">2781</span>
<span class="normal">2782</span>
<span class="normal">2783</span>
<span class="normal">2784</span>
<span class="normal">2785</span>
<span class="normal">2786</span>
<span class="normal">2787</span>
<span class="normal">2788</span>
<span class="normal">2789</span>
<span class="normal">2790</span>
<span class="normal">2791</span>
<span class="normal">2792</span>
<span class="normal">2793</span>
<span class="normal">2794</span>
<span class="normal">2795</span>
<span class="normal">2796</span>
<span class="normal">2797</span>
<span class="normal">2798</span>
<span class="normal">2799</span>
<span class="normal">2800</span>
<span class="normal">2801</span>
<span class="normal">2802</span>
<span class="normal">2803</span>
<span class="normal">2804</span>
<span class="normal">2805</span>
<span class="normal">2806</span>
<span class="normal">2807</span>
<span class="normal">2808</span>
<span class="normal">2809</span>
<span class="normal">2810</span>
<span class="normal">2811</span>
<span class="normal">2812</span>
<span class="normal">2813</span>
<span class="normal">2814</span>
<span class="normal">2815</span>
<span class="normal">2816</span>
<span class="normal">2817</span>
<span class="normal">2818</span>
<span class="normal">2819</span>
<span class="normal">2820</span>
<span class="normal">2821</span>
<span class="normal">2822</span>
<span class="normal">2823</span>
<span class="normal">2824</span>
<span class="normal">2825</span>
<span class="normal">2826</span>
<span class="normal">2827</span>
<span class="normal">2828</span>
<span class="normal">2829</span>
<span class="normal">2830</span>
<span class="normal">2831</span>
<span class="normal">2832</span>
<span class="normal">2833</span>
<span class="normal">2834</span>
<span class="normal">2835</span>
<span class="normal">2836</span>
<span class="normal">2837</span>
<span class="normal">2838</span>
<span class="normal">2839</span>
<span class="normal">2840</span>
<span class="normal">2841</span>
<span class="normal">2842</span>
<span class="normal">2843</span>
<span class="normal">2844</span>
<span class="normal">2845</span>
<span class="normal">2846</span>
<span class="normal">2847</span>
<span class="normal">2848</span>
<span class="normal">2849</span>
<span class="normal">2850</span>
<span class="normal">2851</span>
<span class="normal">2852</span>
<span class="normal">2853</span>
<span class="normal">2854</span>
<span class="normal">2855</span>
<span class="normal">2856</span>
<span class="normal">2857</span>
<span class="normal">2858</span>
<span class="normal">2859</span>
<span class="normal">2860</span>
<span class="normal">2861</span>
<span class="normal">2862</span>
<span class="normal">2863</span>
<span class="normal">2864</span>
<span class="normal">2865</span>
<span class="normal">2866</span>
<span class="normal">2867</span>
<span class="normal">2868</span>
<span class="normal">2869</span>
<span class="normal">2870</span>
<span class="normal">2871</span>
<span class="normal">2872</span>
<span class="normal">2873</span>
<span class="normal">2874</span>
<span class="normal">2875</span>
<span class="normal">2876</span>
<span class="normal">2877</span>
<span class="normal">2878</span>
<span class="normal">2879</span>
<span class="normal">2880</span>
<span class="normal">2881</span>
<span class="normal">2882</span>
<span class="normal">2883</span>
<span class="normal">2884</span>
<span class="normal">2885</span>
<span class="normal">2886</span>
<span class="normal">2887</span>
<span class="normal">2888</span>
<span class="normal">2889</span>
<span class="normal">2890</span>
<span class="normal">2891</span>
<span class="normal">2892</span>
<span class="normal">2893</span>
<span class="normal">2894</span>
<span class="normal">2895</span>
<span class="normal">2896</span>
<span class="normal">2897</span>
<span class="normal">2898</span>
<span class="normal">2899</span>
<span class="normal">2900</span>
<span class="normal">2901</span>
<span class="normal">2902</span>
<span class="normal">2903</span>
<span class="normal">2904</span>
<span class="normal">2905</span>
<span class="normal">2906</span>
<span class="normal">2907</span>
<span class="normal">2908</span>
<span class="normal">2909</span>
<span class="normal">2910</span>
<span class="normal">2911</span>
<span class="normal">2912</span>
<span class="normal">2913</span>
<span class="normal">2914</span>
<span class="normal">2915</span>
<span class="normal">2916</span>
<span class="normal">2917</span>
<span class="normal">2918</span>
<span class="normal">2919</span>
<span class="normal">2920</span>
<span class="normal">2921</span>
<span class="normal">2922</span>
<span class="normal">2923</span>
<span class="normal">2924</span>
<span class="normal">2925</span>
<span class="normal">2926</span>
<span class="normal">2927</span>
<span class="normal">2928</span>
<span class="normal">2929</span>
<span class="normal">2930</span>
<span class="normal">2931</span>
<span class="normal">2932</span>
<span class="normal">2933</span>
<span class="normal">2934</span>
<span class="normal">2935</span>
<span class="normal">2936</span>
<span class="normal">2937</span>
<span class="normal">2938</span>
<span class="normal">2939</span>
<span class="normal">2940</span>
<span class="normal">2941</span>
<span class="normal">2942</span>
<span class="normal">2943</span>
<span class="normal">2944</span>
<span class="normal">2945</span>
<span class="normal">2946</span>
<span class="normal">2947</span>
<span class="normal">2948</span>
<span class="normal">2949</span>
<span class="normal">2950</span>
<span class="normal">2951</span>
<span class="normal">2952</span>
<span class="normal">2953</span>
<span class="normal">2954</span>
<span class="normal">2955</span>
<span class="normal">2956</span>
<span class="normal">2957</span>
<span class="normal">2958</span>
<span class="normal">2959</span>
<span class="normal">2960</span>
<span class="normal">2961</span>
<span class="normal">2962</span>
<span class="normal">2963</span>
<span class="normal">2964</span>
<span class="normal">2965</span>
<span class="normal">2966</span>
<span class="normal">2967</span>
<span class="normal">2968</span>
<span class="normal">2969</span>
<span class="normal">2970</span>
<span class="normal">2971</span>
<span class="normal">2972</span>
<span class="normal">2973</span>
<span class="normal">2974</span>
<span class="normal">2975</span>
<span class="normal">2976</span>
<span class="normal">2977</span>
<span class="normal">2978</span>
<span class="normal">2979</span>
<span class="normal">2980</span>
<span class="normal">2981</span>
<span class="normal">2982</span>
<span class="normal">2983</span>
<span class="normal">2984</span>
<span class="normal">2985</span>
<span class="normal">2986</span>
<span class="normal">2987</span>
<span class="normal">2988</span>
<span class="normal">2989</span>
<span class="normal">2990</span>
<span class="normal">2991</span>
<span class="normal">2992</span>
<span class="normal">2993</span>
<span class="normal">2994</span>
<span class="normal">2995</span>
<span class="normal">2996</span>
<span class="normal">2997</span>
<span class="normal">2998</span>
<span class="normal">2999</span>
<span class="normal">3000</span>
<span class="normal">3001</span>
<span class="normal">3002</span>
<span class="normal">3003</span>
<span class="normal">3004</span>
<span class="normal">3005</span>
<span class="normal">3006</span>
<span class="normal">3007</span>
<span class="normal">3008</span>
<span class="normal">3009</span>
<span class="normal">3010</span>
<span class="normal">3011</span>
<span class="normal">3012</span>
<span class="normal">3013</span>
<span class="normal">3014</span>
<span class="normal">3015</span>
<span class="normal">3016</span>
<span class="normal">3017</span>
<span class="normal">3018</span>
<span class="normal">3019</span>
<span class="normal">3020</span>
<span class="normal">3021</span>
<span class="normal">3022</span>
<span class="normal">3023</span>
<span class="normal">3024</span>
<span class="normal">3025</span>
<span class="normal">3026</span>
<span class="normal">3027</span>
<span class="normal">3028</span>
<span class="normal">3029</span>
<span class="normal">3030</span>
<span class="normal">3031</span>
<span class="normal">3032</span>
<span class="normal">3033</span>
<span class="normal">3034</span>
<span class="normal">3035</span>
<span class="normal">3036</span>
<span class="normal">3037</span>
<span class="normal">3038</span>
<span class="normal">3039</span>
<span class="normal">3040</span>
<span class="normal">3041</span>
<span class="normal">3042</span>
<span class="normal">3043</span>
<span class="normal">3044</span>
<span class="normal">3045</span>
<span class="normal">3046</span>
<span class="normal">3047</span>
<span class="normal">3048</span>
<span class="normal">3049</span>
<span class="normal">3050</span>
<span class="normal">3051</span>
<span class="normal">3052</span>
<span class="normal">3053</span>
<span class="normal">3054</span>
<span class="normal">3055</span>
<span class="normal">3056</span>
<span class="normal">3057</span>
<span class="normal">3058</span>
<span class="normal">3059</span>
<span class="normal">3060</span>
<span class="normal">3061</span>
<span class="normal">3062</span>
<span class="normal">3063</span>
<span class="normal">3064</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]],</span>
    <span class="o">*</span><span class="n">model_args</span><span class="p">,</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">PretrainedConfig</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cache_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ignore_mismatched_sizes</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">force_download</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">local_files_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">token</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">revision</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;main&quot;</span><span class="p">,</span>
    <span class="n">use_safetensors</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;PreTrainedModel&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Instantiate a pretrained pytorch model from a pre-trained model configuration.</span>

<span class="sd">    The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train</span>
<span class="sd">    the model, you should first set it back in training mode with `model.train()`.</span>

<span class="sd">    The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come</span>
<span class="sd">    pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning</span>
<span class="sd">    task.</span>

<span class="sd">    The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those</span>
<span class="sd">    weights are discarded.</span>

<span class="sd">    If model weights are the same precision as the base model (and is a supported model), weights will be lazily loaded</span>
<span class="sd">    in using the `meta` device and brought into memory once an input is passed through that layer regardless of</span>
<span class="sd">    `low_cpu_mem_usage`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.</span>
<span class="sd">                - A path to a *directory* containing model weights saved using</span>
<span class="sd">                  [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.</span>
<span class="sd">                - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In</span>
<span class="sd">                  this case, `from_tf` should be set to `True` and a configuration object should be provided as</span>
<span class="sd">                  `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a</span>
<span class="sd">                  PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</span>
<span class="sd">                - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,</span>
<span class="sd">                  `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to</span>
<span class="sd">                  `True`.</span>
<span class="sd">                - `None` if you are both providing the configuration and state dictionary (resp. with keyword</span>
<span class="sd">                  arguments `config` and `state_dict`).</span>
<span class="sd">        model_args (sequence of positional arguments, *optional*):</span>
<span class="sd">            All remaining positional arguments will be passed to the underlying model&#39;s `__init__` method.</span>
<span class="sd">        config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - an instance of a class derived from [`PretrainedConfig`],</span>
<span class="sd">                - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].</span>

<span class="sd">            Configuration for the model to use instead of an automatically loaded configuration. Configuration can</span>
<span class="sd">            be automatically loaded when:</span>

<span class="sd">                - The model is a model provided by the library (loaded with the *model id* string of a pretrained</span>
<span class="sd">                  model).</span>
<span class="sd">                - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the</span>
<span class="sd">                  save directory.</span>
<span class="sd">                - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a</span>
<span class="sd">                  configuration JSON file named *config.json* is found in the directory.</span>
<span class="sd">        state_dict (`Dict[str, mindspore.Tensor]`, *optional*):</span>
<span class="sd">            A state dictionary to use instead of a state dictionary loaded from saved weights file.</span>

<span class="sd">            This option can be used if you want to create a model from a pretrained configuration but load your own</span>
<span class="sd">            weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and</span>
<span class="sd">            [`~PreTrainedModel.from_pretrained`] is not a simpler option.</span>
<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory in which a downloaded pretrained model configuration should be cached if the</span>
<span class="sd">            standard cache should not be used.</span>
<span class="sd">        from_tf (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Load the model weights from a TensorFlow checkpoint save file (see docstring of</span>
<span class="sd">            `pretrained_model_name_or_path` argument).</span>
<span class="sd">        from_flax (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Load the model weights from a Flax checkpoint save file (see docstring of</span>
<span class="sd">            `pretrained_model_name_or_path` argument).</span>
<span class="sd">        ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to raise an error if some of the weights from the checkpoint do not have the same size</span>
<span class="sd">            as the weights of the model (if for instance, you are instantiating a model with 10 labels from a</span>
<span class="sd">            checkpoint with 3 labels).</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>
<span class="sd">        resume_download:</span>
<span class="sd">            Deprecated and ignored. All downloads are now resumed by default when possible.</span>
<span class="sd">            Will be removed in v5 of Transformers.</span>
<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        output_loading_info(`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</span>
<span class="sd">        local_files_only(`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to only look at local files (i.e., do not try to download the model).</span>
<span class="sd">        token (`str` or `bool`, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use</span>
<span class="sd">            the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a</span>
<span class="sd">            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any</span>
<span class="sd">            identifier allowed by git.</span>

<span class="sd">            &lt;Tip&gt;</span>

<span class="sd">            To test a pull request you made on the Hub, you can pass `revision=&quot;refs/pr/&lt;pr_number&gt;&quot;.</span>

<span class="sd">            &lt;/Tip&gt;</span>

<span class="sd">        mirror (`str`, *optional*):</span>
<span class="sd">            Mirror source to accelerate downloads in China. If you are from China and have an accessibility</span>
<span class="sd">            problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.</span>
<span class="sd">            Please refer to the mirror site for more information.</span>
<span class="sd">        _fast_init(`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether or not to disable fast initialization.</span>

<span class="sd">            &lt;Tip warning={true}&gt;</span>

<span class="sd">            One should only disable *_fast_init* to ensure backwards compatibility with `transformers.__version__ &lt;</span>
<span class="sd">            4.6.0` for seeded model initialization. This argument will be removed at the next major version. See</span>
<span class="sd">            [pull request 11471](https://github.com/huggingface/transformers/pull/11471) for more information.</span>

<span class="sd">            &lt;/Tip&gt;</span>
<span class="sd">        attn_implementation (`str`, *optional*):</span>
<span class="sd">            The attention implementation to use in the model (if relevant). The default is otherwise the manual `&quot;eager&quot;` implementation.</span>

<span class="sd">        &gt; Parameters for big model inference</span>

<span class="sd">        low_cpu_mem_usage(`bool`, *optional*):</span>
<span class="sd">            Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.</span>
<span class="sd">            Generally should be combined with a `device_map` (such as `&quot;auto&quot;`) for best results.</span>
<span class="sd">            This is an experimental feature and a subject to change at any moment.</span>
<span class="sd">            &lt;/Tip&gt;</span>
<span class="sd">                If the model weights are in the same precision as the model loaded in, `low_cpu_mem_usage` (without</span>
<span class="sd">                `device_map`) is redundant and will not provide any benefit in regards to CPU memory usage. However,</span>
<span class="sd">                this should still be enabled if you are passing in a `device_map`.</span>
<span class="sd">            &lt;/Tip&gt;</span>
<span class="sd">        ms_dtype (`str` or `mindspore.dtype.TensorType`, *optional*):</span>
<span class="sd">            Override the default `mindspore.dtype.TensorType` and load the model under a specific `dtype`. The different options</span>
<span class="sd">            are:</span>

<span class="sd">            1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified</span>
<span class="sd">              `dtype`, ignoring the model&#39;s `config.ms_dtype` if one exists. If not specified</span>
<span class="sd">              - the model will get loaded in `torch.float` (fp32).</span>

<span class="sd">            2. `&quot;auto&quot;` - A `ms_dtype` entry in the `config.json` file of the model will be</span>
<span class="sd">              attempted to be used. If this entry isn&#39;t found then next check the `dtype` of the first weight in</span>
<span class="sd">              the checkpoint that&#39;s of a floating point type and use that as `dtype`. This will load the model</span>
<span class="sd">              using the `dtype` it was saved in at the end of the training. It can&#39;t be used as an indicator of how</span>
<span class="sd">              the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.</span>

<span class="sd">            3. A string that is a valid `mindspore.dtype.TensorType`. E.g. &quot;float32&quot; loads the model in `mindspore.float32`, &quot;float16&quot; loads in `torch.float16` etc.</span>

<span class="sd">            &lt;Tip&gt;</span>

<span class="sd">            For some models the `dtype` they were trained in is unknown - you may try to check the model&#39;s paper or</span>
<span class="sd">            reach out to the authors and ask them to add this information to the model&#39;s card and to insert the</span>
<span class="sd">            `ms_dtype` entry in `config.json` on the hub.</span>

<span class="sd">            &lt;/Tip&gt;</span>

<span class="sd">        device_map (`str` or `Dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):</span>
<span class="sd">            A map that specifies where each submodule should go. It doesn&#39;t need to be refined to each</span>
<span class="sd">            parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the</span>
<span class="sd">            same device. If we only pass the device (*e.g.*, `&quot;cpu&quot;`, `&quot;cuda:1&quot;`, `&quot;mps&quot;`, or a GPU ordinal rank</span>
<span class="sd">            like `1`) on which the model will be allocated, the device map will map the entire model to this</span>
<span class="sd">            device. Passing `device_map = 0` means put the whole model on GPU 0.</span>

<span class="sd">            To have Accelerate compute the most optimized `device_map` automatically, set `device_map=&quot;auto&quot;`. For</span>
<span class="sd">            more information about each option see [designing a device</span>
<span class="sd">            map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).</span>
<span class="sd">        max_memory (`Dict`, *optional*):</span>
<span class="sd">            A dictionary device identifier to maximum memory. Will default to the maximum memory available for each</span>
<span class="sd">            GPU and the available CPU RAM if unset.</span>
<span class="sd">        offload_folder (`str` or `os.PathLike`, *optional*):</span>
<span class="sd">            If the `device_map` contains any value `&quot;disk&quot;`, the folder where we will offload weights.</span>
<span class="sd">        offload_state_dict (`bool`, *optional*):</span>
<span class="sd">            If `True`, will temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU</span>
<span class="sd">            RAM if the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to</span>
<span class="sd">            `True` when there is some disk offload.</span>
<span class="sd">        offload_buffers (`bool`, *optional*):</span>
<span class="sd">            Whether or not to offload the buffers with the model parameters.</span>
<span class="sd">        quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):</span>
<span class="sd">            A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g</span>
<span class="sd">            bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and</span>
<span class="sd">            `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes</span>
<span class="sd">            quantizations and not preferred. consider inserting all such arguments into quantization_config</span>
<span class="sd">            instead.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can</span>
<span class="sd">            specify the folder name here.</span>
<span class="sd">        variant (`str`, *optional*):</span>
<span class="sd">            If specified load weights from `variant` filename, *e.g.* pytorch_model.&lt;variant&gt;.bin. `variant` is</span>
<span class="sd">            ignored when using `from_tf` or `from_flax`.</span>
<span class="sd">        use_safetensors (`bool`, *optional*, defaults to `None`):</span>
<span class="sd">            Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`</span>
<span class="sd">            is not installed, it will be set to `False`.</span>

<span class="sd">        kwargs (remaining dictionary of keyword arguments, *optional*):</span>
<span class="sd">            Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,</span>
<span class="sd">            `output_attentions=True`). Behaves differently depending on whether a `config` is provided or</span>
<span class="sd">            automatically loaded:</span>

<span class="sd">                - If a configuration is provided with `config`, `**kwargs` will be directly passed to the</span>
<span class="sd">                  underlying model&#39;s `__init__` method (we assume all relevant updates to the configuration have</span>
<span class="sd">                  already been done)</span>
<span class="sd">                - If a configuration is not provided, `kwargs` will be first passed to the configuration class</span>
<span class="sd">                  initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that</span>
<span class="sd">                  corresponds to a configuration attribute will be used to override said attribute with the</span>
<span class="sd">                  supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute</span>
<span class="sd">                  will be passed to the underlying model&#39;s `__init__` function.</span>

<span class="sd">    &lt;Tip&gt;</span>

<span class="sd">    Activate the special [&quot;offline-mode&quot;](https://huggingface.co/transformers/installation.html#offline-mode) to</span>
<span class="sd">    use this method in a firewalled environment.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; from transformers import BertConfig, BertModel</span>

<span class="sd">    &gt;&gt;&gt; # Download model and configuration from huggingface.co and cache.</span>
<span class="sd">    &gt;&gt;&gt; model = BertModel.from_pretrained(&quot;google-bert/bert-base-uncased&quot;)</span>
<span class="sd">    &gt;&gt;&gt; # Model was saved using *save_pretrained(&#39;./test/saved_model/&#39;)* (for example purposes, not runnable).</span>
<span class="sd">    &gt;&gt;&gt; model = BertModel.from_pretrained(&quot;./test/saved_model/&quot;)</span>
<span class="sd">    &gt;&gt;&gt; # Update configuration during loading.</span>
<span class="sd">    &gt;&gt;&gt; model = BertModel.from_pretrained(&quot;google-bert/bert-base-uncased&quot;, output_attentions=True)</span>
<span class="sd">    &gt;&gt;&gt; assert model.config.output_attentions == True</span>
<span class="sd">    &gt;&gt;&gt; # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).</span>
<span class="sd">    &gt;&gt;&gt; config = BertConfig.from_json_file(&quot;./tf_model/my_tf_model_config.json&quot;)</span>
<span class="sd">    &gt;&gt;&gt; model = BertModel.from_pretrained(&quot;./tf_model/my_tf_checkpoint.ckpt.index&quot;, from_tf=True, config=config)</span>
<span class="sd">    &gt;&gt;&gt; # Loading from a Flax checkpoint file instead of a PyTorch model (slower)</span>
<span class="sd">    &gt;&gt;&gt; model = BertModel.from_pretrained(&quot;google-bert/bert-base-uncased&quot;, from_flax=True)</span>
<span class="sd">    ```</span>

<span class="sd">    * `low_cpu_mem_usage` algorithm:</span>

<span class="sd">    This is an experimental function that loads the model using ~1x model size CPU memory</span>

<span class="sd">    Here is how it works:</span>

<span class="sd">    1. save which state_dict keys we have</span>
<span class="sd">    2. drop state_dict before the model is created, since the latter takes 1x model size CPU memory</span>
<span class="sd">    3. after the model has been instantiated switch to the meta device all params/buffers that</span>
<span class="sd">    are going to be replaced from the loaded state_dict</span>
<span class="sd">    4. load state_dict 2nd time</span>
<span class="sd">    5. replace the params/buffers from the state_dict</span>

<span class="sd">    Currently, it can&#39;t handle deepspeed ZeRO stage 3 and ignores loading errors</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;state_dict&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">from_pt</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;from_pt&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">resume_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;resume_download&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">output_loading_info</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;output_loading_info&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">from_pipeline</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_from_pipeline&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">from_auto_class</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_from_auto&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">_fast_init</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_fast_init&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">ms_dtype</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;ms_dtype&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">low_cpu_mem_usage</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;low_cpu_mem_usage&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">device_map</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;device_map&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">max_memory</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;max_memory&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">offload_folder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;offload_folder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">offload_state_dict</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;offload_state_dict&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">offload_buffers</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;offload_buffers&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">load_in_8bit</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;load_in_8bit&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">load_in_4bit</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;load_in_4bit&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">quantization_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;quantization_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">mirror</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;mirror&#39;</span><span class="p">,</span> <span class="s1">&#39;huggingface&#39;</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="n">commit_hash</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_commit_hash&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">variant</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;variant&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">adapter_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;adapter_kwargs&quot;</span><span class="p">,</span> <span class="p">{})</span>
    <span class="n">adapter_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;adapter_name&quot;</span><span class="p">,</span> <span class="s2">&quot;default&quot;</span><span class="p">)</span>
    <span class="n">use_flash_attention_2</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_flash_attention_2&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">gguf_file</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;gguf_file&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">adapter_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;token&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">adapter_kwargs</span><span class="p">:</span>
        <span class="n">adapter_kwargs</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">token</span>

    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_safetensors_available</span><span class="p">():</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">gguf_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;MindNLP not support GGUF file.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">commit_hash</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PretrainedConfig</span><span class="p">):</span>
            <span class="c1"># We make a call to the config file first (which may be absent) to get the commit hash as soon as possible</span>
            <span class="n">resolved_config_file</span> <span class="o">=</span> <span class="n">cached_file</span><span class="p">(</span>
                <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                <span class="n">CONFIG_NAME</span><span class="p">,</span>
                <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
                <span class="n">_raise_exceptions_for_gated_repo</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">_raise_exceptions_for_missing_entries</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">_raise_exceptions_for_connection_errors</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">commit_hash</span> <span class="o">=</span> <span class="n">extract_commit_hash</span><span class="p">(</span><span class="n">resolved_config_file</span><span class="p">,</span> <span class="n">commit_hash</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">commit_hash</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;_commit_hash&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="n">_adapter_model_path</span> <span class="o">=</span> <span class="n">adapter_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_adapter_model_path&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_adapter_model_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_adapter_model_path</span> <span class="o">=</span> <span class="n">find_adapter_config_file</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">_commit_hash</span><span class="o">=</span><span class="n">commit_hash</span><span class="p">,</span>
            <span class="n">mirror</span><span class="o">=</span><span class="n">mirror</span><span class="p">,</span>
            <span class="o">**</span><span class="n">adapter_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">_adapter_model_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">_adapter_model_path</span><span class="p">):</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">_adapter_model_path</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">_adapter_model_path</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>
            <span class="n">pretrained_model_name_or_path</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)[</span><span class="s2">&quot;base_model_name_or_path&quot;</span><span class="p">]</span>

    <span class="c1"># change device_map into a map if we passed an int, a str or a torch.device</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device_map</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">device_map</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="s2">&quot;balanced&quot;</span><span class="p">,</span> <span class="s2">&quot;balanced_low_0&quot;</span><span class="p">,</span> <span class="s2">&quot;sequential&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;When passing device_map as a string, the value needs to be &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;&#39;auto&#39;, &#39;balanced&#39;, &#39;balanced_low_0&#39;, &#39;sequential&#39; but found </span><span class="si">{</span><span class="n">device_map</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device_map</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">device_map</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You can&#39;t pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = &#39;cpu&#39; &quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">device_map</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;&quot;</span><span class="p">:</span> <span class="n">device_map</span><span class="p">}</span>

    <span class="k">if</span> <span class="n">device_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">low_cpu_mem_usage</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">low_cpu_mem_usage</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="n">low_cpu_mem_usage</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Passing along a `device_map` requires `low_cpu_mem_usage=True`&quot;</span><span class="p">)</span>

    <span class="c1"># handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.</span>
    <span class="k">if</span> <span class="n">load_in_4bit</span> <span class="ow">or</span> <span class="n">load_in_8bit</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;MindNLP do not support 4bit or 8bit for now.&quot;</span><span class="p">)</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">,</span> <span class="s2">&quot;from_auto_class&quot;</span><span class="p">:</span> <span class="n">from_auto_class</span><span class="p">}</span>
    <span class="k">if</span> <span class="n">from_pipeline</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">user_agent</span><span class="p">[</span><span class="s2">&quot;using_pipeline&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">from_pipeline</span>

    <span class="k">if</span> <span class="n">is_offline_mode</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">local_files_only</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Offline mode: forcing local_files_only=True&quot;</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Load config if we don&#39;t provide a configuration</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PretrainedConfig</span><span class="p">):</span>
        <span class="n">config_path</span> <span class="o">=</span> <span class="n">config</span> <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">pretrained_model_name_or_path</span>
        <span class="n">config</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">config_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
            <span class="n">config_path</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">return_unused_kwargs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="n">mirror</span><span class="o">=</span><span class="n">mirror</span><span class="p">,</span>
            <span class="n">_from_auto</span><span class="o">=</span><span class="n">from_auto_class</span><span class="p">,</span>
            <span class="n">_from_pipeline</span><span class="o">=</span><span class="n">from_pipeline</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># In case one passes a config to `from_pretrained` + &quot;attn_implementation&quot;</span>
        <span class="c1"># override the `_attn_implementation` attribute to `attn_implementation` of the kwargs</span>
        <span class="c1"># Please see: https://github.com/huggingface/transformers/issues/28038</span>

        <span class="c1"># Overwrite `config._attn_implementation` by the one from the kwargs --&gt; in auto-factory</span>
        <span class="c1"># we pop attn_implementation from the kwargs but this handles the case where users</span>
        <span class="c1"># passes manually the config to `from_pretrained`.</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="n">kwarg_attn_imp</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">kwarg_attn_imp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">=</span> <span class="n">kwarg_attn_imp</span>

        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

    <span class="c1"># This variable will flag if we&#39;re loading a sharded checkpoint. In this case the archive file is just the</span>
    <span class="c1"># index of the files.</span>
    <span class="n">is_sharded</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">sharded_metadata</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Load model</span>
    <span class="n">loading_info</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Keep in fp32 modules</span>
    <span class="n">keep_in_fp32_modules</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">use_keep_in_fp32_modules</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">gguf_file</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pretrained_model_name_or_path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span>
        <span class="n">is_local</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_local</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">False</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span>
                <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">))</span>
            <span class="p">):</span>
                <span class="c1"># Load from a safetensors checkpoint</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">False</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span>
                <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="p">):</span>
                <span class="c1"># Load from a sharded safetensors checkpoint</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">is_sharded</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">use_safetensors</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span>
                <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">))</span>
            <span class="p">):</span>
                <span class="c1"># Load from a MindSpore checkpoint</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">use_safetensors</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span>
                <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">))</span>
            <span class="p">):</span>
                <span class="c1"># Load from a sharded MindSpore checkpoint</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">is_sharded</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">use_safetensors</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span>
                <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">))</span>
            <span class="p">):</span>
                <span class="c1"># Load from a PyTorch checkpoint</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">use_safetensors</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span>
                <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">))</span>
            <span class="p">):</span>
                <span class="c1"># Load from a sharded PyTorch checkpoint</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">is_sharded</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">elif</span> <span class="n">use_safetensors</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Error no file named </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2"> found in directory&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Error no file named </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2">,&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; found in directory </span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
        <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">subfolder</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">)):</span>
            <span class="n">archive_file</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>
            <span class="n">is_local</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="n">is_remote_url</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">):</span>
            <span class="n">filename</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>
            <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">download_url</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># set correct filename</span>
            <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">False</span><span class="p">:</span>
                <span class="n">filename</span> <span class="o">=</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">from_pt</span><span class="p">:</span>
                <span class="n">filename</span> <span class="o">=</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">filename</span> <span class="o">=</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>

            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Load from URL or cache if already cached</span>
                <span class="n">cached_file_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">&quot;cache_dir&quot;</span><span class="p">:</span> <span class="n">cache_dir</span><span class="p">,</span>
                    <span class="s2">&quot;force_download&quot;</span><span class="p">:</span> <span class="n">force_download</span><span class="p">,</span>
                    <span class="s2">&quot;proxies&quot;</span><span class="p">:</span> <span class="n">proxies</span><span class="p">,</span>
                    <span class="s2">&quot;resume_download&quot;</span><span class="p">:</span> <span class="n">resume_download</span><span class="p">,</span>
                    <span class="s2">&quot;local_files_only&quot;</span><span class="p">:</span> <span class="n">local_files_only</span><span class="p">,</span>
                    <span class="s2">&quot;token&quot;</span><span class="p">:</span> <span class="n">token</span><span class="p">,</span>
                    <span class="s2">&quot;user_agent&quot;</span><span class="p">:</span> <span class="n">user_agent</span><span class="p">,</span>
                    <span class="s2">&quot;revision&quot;</span><span class="p">:</span> <span class="n">revision</span><span class="p">,</span>
                    <span class="s2">&quot;subfolder&quot;</span><span class="p">:</span> <span class="n">subfolder</span><span class="p">,</span>
                    <span class="s2">&quot;_raise_exceptions_for_gated_repo&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="s2">&quot;_raise_exceptions_for_missing_entries&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="c1"># &quot;_commit_hash&quot;: commit_hash,</span>
                    <span class="s2">&quot;mirror&quot;</span><span class="p">:</span> <span class="n">mirror</span>
                <span class="p">}</span>
                <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">cached_file</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">cached_file_kwargs</span><span class="p">)</span>

                <span class="c1"># Since we set _raise_exceptions_for_missing_entries=False, we don&#39;t get an exception but a None</span>
                <span class="c1"># result when internet is up, the repo and revision exist, but the file does not.</span>
                <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">filename</span> <span class="o">==</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">):</span>
                    <span class="c1"># Maybe the checkpoint is sharded, we try to grab the index name in this case.</span>
                    <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">cached_file</span><span class="p">(</span>
                        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                        <span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">),</span>
                        <span class="o">**</span><span class="n">cached_file_kwargs</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">is_sharded</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="k">elif</span> <span class="n">use_safetensors</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">revision</span> <span class="o">==</span> <span class="s2">&quot;main&quot;</span><span class="p">:</span>
                            <span class="n">resolved_archive_file</span><span class="p">,</span> <span class="n">revision</span><span class="p">,</span> <span class="n">is_sharded</span> <span class="o">=</span> <span class="n">auto_conversion</span><span class="p">(</span>
                                <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">**</span><span class="n">cached_file_kwargs</span>
                            <span class="p">)</span>
                        <span class="n">cached_file_kwargs</span><span class="p">[</span><span class="s2">&quot;revision&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">revision</span>
                        <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2"> does not appear to have a file named&quot;</span>
                                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2"> or </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_INDEX_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2"> &quot;</span>
                                <span class="s2">&quot;and thus cannot be loaded with `safetensors`. Please make sure that the model has &quot;</span>
                                <span class="s2">&quot;been saved with `safe_serialization=True` or do not set `use_safetensors=True`.&quot;</span>
                            <span class="p">)</span>
                    <span class="k">elif</span> <span class="n">from_pt</span><span class="p">:</span>
                        <span class="n">filename</span> <span class="o">=</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                        <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">cached_file</span><span class="p">(</span>
                            <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">cached_file_kwargs</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># This repo has no safetensors file of any kind, we switch to PyTorch.</span>
                        <span class="n">filename</span> <span class="o">=</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
                        <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">cached_file</span><span class="p">(</span>
                            <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">cached_file_kwargs</span>
                        <span class="p">)</span>

                <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">filename</span> <span class="o">==</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">):</span>
                    <span class="c1"># Maybe the checkpoint is sharded, we try to grab the index name in this case.</span>
                    <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">cached_file</span><span class="p">(</span>
                        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                        <span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">),</span>
                        <span class="o">**</span><span class="n">cached_file_kwargs</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">is_sharded</span> <span class="o">=</span> <span class="kc">True</span>

                <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">filename</span> <span class="o">==</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">):</span>
                    <span class="c1"># Maybe the checkpoint is sharded, we try to grab the index name in this case.</span>
                    <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">cached_file</span><span class="p">(</span>
                        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                        <span class="n">_add_variant</span><span class="p">(</span><span class="n">PT_WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">variant</span><span class="p">),</span>
                        <span class="o">**</span><span class="n">cached_file_kwargs</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">is_sharded</span> <span class="o">=</span> <span class="kc">True</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="n">local_files_only</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_offline_mode</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">filename</span> <span class="ow">in</span> <span class="p">[</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">WEIGHTS_INDEX_NAME</span><span class="p">,</span> <span class="n">PT_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">PT_WEIGHTS_INDEX_NAME</span><span class="p">]:</span>
                            <span class="c1"># If the PyTorch file was found, check if there is a safetensors file on the repository</span>
                            <span class="c1"># If there is no safetensors file on the repositories, start an auto conversion</span>
                            <span class="n">safe_weights_name</span> <span class="o">=</span> <span class="n">SAFE_WEIGHTS_INDEX_NAME</span> <span class="k">if</span> <span class="n">is_sharded</span> <span class="k">else</span> <span class="n">SAFE_WEIGHTS_NAME</span>
                            <span class="n">has_file_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                                <span class="s2">&quot;revision&quot;</span><span class="p">:</span> <span class="n">revision</span><span class="p">,</span>
                                <span class="s2">&quot;proxies&quot;</span><span class="p">:</span> <span class="n">proxies</span><span class="p">,</span>
                                <span class="s2">&quot;token&quot;</span><span class="p">:</span> <span class="n">token</span><span class="p">,</span>
                                <span class="s2">&quot;cache_dir&quot;</span><span class="p">:</span> <span class="n">cache_dir</span><span class="p">,</span>
                                <span class="s2">&quot;local_files_only&quot;</span><span class="p">:</span> <span class="n">local_files_only</span><span class="p">,</span>
                                <span class="s2">&quot;mirror&quot;</span><span class="p">:</span> <span class="n">mirror</span><span class="p">,</span>
                            <span class="p">}</span>
                            <span class="n">cached_file_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                                <span class="s2">&quot;cache_dir&quot;</span><span class="p">:</span> <span class="n">cache_dir</span><span class="p">,</span>
                                <span class="s2">&quot;force_download&quot;</span><span class="p">:</span> <span class="n">force_download</span><span class="p">,</span>
                                <span class="s2">&quot;resume_download&quot;</span><span class="p">:</span> <span class="n">resume_download</span><span class="p">,</span>
                                <span class="s2">&quot;local_files_only&quot;</span><span class="p">:</span> <span class="n">local_files_only</span><span class="p">,</span>
                                <span class="s2">&quot;user_agent&quot;</span><span class="p">:</span> <span class="n">user_agent</span><span class="p">,</span>
                                <span class="s2">&quot;subfolder&quot;</span><span class="p">:</span> <span class="n">subfolder</span><span class="p">,</span>
                                <span class="s2">&quot;_raise_exceptions_for_gated_repo&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                                <span class="s2">&quot;_raise_exceptions_for_missing_entries&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                                <span class="c1"># &quot;_commit_hash&quot;: commit_hash,</span>
                                <span class="o">**</span><span class="n">has_file_kwargs</span><span class="p">,</span>
                            <span class="p">}</span>

                            <span class="k">if</span> <span class="ow">not</span> <span class="n">has_file</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">safe_weights_name</span><span class="p">,</span> <span class="o">**</span><span class="n">has_file_kwargs</span><span class="p">):</span>
                                <span class="n">Thread</span><span class="p">(</span>
                                    <span class="n">target</span><span class="o">=</span><span class="n">auto_conversion</span><span class="p">,</span>
                                    <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,),</span>
                                    <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;ignore_errors_during_conversion&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">cached_file_kwargs</span><span class="p">},</span>
                                    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Thread-autoconversion&quot;</span><span class="p">,</span>
                                <span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Otherwise, no PyTorch file was found, maybe there is a TF or Flax model file.</span>
                        <span class="c1"># We try those to give a helpful error message.</span>
                        <span class="n">has_file_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                            <span class="s2">&quot;mirror&quot;</span><span class="p">:</span> <span class="n">mirror</span><span class="p">,</span>
                            <span class="s2">&quot;revision&quot;</span><span class="p">:</span> <span class="n">revision</span><span class="p">,</span>
                            <span class="s2">&quot;proxies&quot;</span><span class="p">:</span> <span class="n">proxies</span><span class="p">,</span>
                            <span class="s2">&quot;token&quot;</span><span class="p">:</span> <span class="n">token</span><span class="p">,</span>
                            <span class="s2">&quot;cache_dir&quot;</span><span class="p">:</span> <span class="n">cache_dir</span><span class="p">,</span>
                            <span class="s2">&quot;local_files_only&quot;</span><span class="p">:</span> <span class="n">local_files_only</span><span class="p">,</span>
                        <span class="p">}</span>
                        <span class="k">if</span> <span class="n">variant</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">has_file</span><span class="p">(</span>
                            <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="o">**</span><span class="n">has_file_kwargs</span>
                        <span class="p">):</span>
                            <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2"> does not appear to have a file named&quot;</span>
                                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2"> but there is a file without the variant&quot;</span>
                                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">variant</span><span class="si">}</span><span class="s2">. Use `variant=None` to load this model from those weights.&quot;</span>
                            <span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2"> does not appear to have a file named&quot;</span>
                                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">SAFE_WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                            <span class="p">)</span>

            <span class="k">except</span> <span class="ne">EnvironmentError</span><span class="p">:</span>
                <span class="c1"># Raise any environment error raise by `cached_file`. It will have a helpful error message adapted</span>
                <span class="c1"># to the original exception.</span>
                <span class="k">raise</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="c1"># For any other exception, we throw a generic error.</span>
                <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Can&#39;t load the model for &#39;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">&#39;. If you were trying to load it&quot;</span>
                    <span class="s2">&quot; from &#39;https://huggingface.co/models&#39;, make sure you don&#39;t have a local directory with the&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; same name. Otherwise, make sure &#39;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">&#39; is the correct path to a&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; directory containing a file named </span><span class="si">{</span><span class="n">_add_variant</span><span class="p">(</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">variant</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

        <span class="k">if</span> <span class="n">is_local</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;loading weights file </span><span class="si">{</span><span class="n">archive_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">archive_file</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;loading weights file </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2"> from cache at </span><span class="si">{</span><span class="n">resolved_archive_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># We&#39;ll need to download and cache each checkpoint shard if the checkpoint is sharded.</span>
    <span class="k">if</span> <span class="n">is_sharded</span><span class="p">:</span>
        <span class="c1"># resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.</span>
        <span class="n">resolved_archive_file</span><span class="p">,</span> <span class="n">sharded_metadata</span> <span class="o">=</span> <span class="n">get_checkpoint_shard_files</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
            <span class="n">resolved_archive_file</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="c1"># _commit_hash=commit_hash,</span>
            <span class="n">mirror</span><span class="o">=</span><span class="n">mirror</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">is_safetensors_available</span><span class="p">()</span>
        <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="ow">and</span> <span class="n">resolved_archive_file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.safetensors&quot;</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="k">with</span> <span class="n">safe_open</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">,</span> <span class="n">framework</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">metadata</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">metadata</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;format&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;pt&#39;</span><span class="p">,</span> <span class="s1">&#39;tf&#39;</span><span class="p">,</span> <span class="s1">&#39;flax&#39;</span><span class="p">,</span> <span class="s1">&#39;mlx&#39;</span><span class="p">,</span> <span class="s1">&#39;np&#39;</span><span class="p">]:</span>
            <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Incompatible safetensors file. File metadata is not [&#39;np&#39;, &#39;pt&#39;, &#39;tf&#39;, &#39;flax&#39;, &#39;mlx&#39;] but </span><span class="si">{</span><span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;format&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="c1"># load pt weights early so that we know which dtype to init the model under</span>
    <span class="k">if</span> <span class="n">from_pt</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_sharded</span> <span class="ow">and</span> <span class="n">state_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Time to load the checkpoint</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">load_state_dict</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">)</span>

        <span class="c1"># set dtype to instantiate the model under:</span>
        <span class="c1"># 1. If ms_dtype is not None, we use that dtype</span>
        <span class="c1"># 2. If ms_dtype is &quot;auto&quot;, we auto-detect dtype from the loaded state_dict, by checking its first</span>
        <span class="c1">#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype</span>
        <span class="c1"># we also may have config.ms_dtype available, but we won&#39;t rely on it till v5</span>
        <span class="n">dtype_orig</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">ms_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ms_dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">ms_dtype</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;ms_dtype&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">ms_dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Will use ms_dtype=</span><span class="si">{</span><span class="n">ms_dtype</span><span class="si">}</span><span class="s2"> as defined in model&#39;s config object&quot;</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">is_sharded</span> <span class="ow">and</span> <span class="s2">&quot;dtype&quot;</span> <span class="ow">in</span> <span class="n">sharded_metadata</span><span class="p">:</span>
                            <span class="n">ms_dtype</span> <span class="o">=</span> <span class="n">sharded_metadata</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">]</span>
                        <span class="k">elif</span> <span class="ow">not</span> <span class="n">is_sharded</span><span class="p">:</span>
                            <span class="n">ms_dtype</span> <span class="o">=</span> <span class="n">get_state_dict_dtype</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">one_state_dict</span> <span class="o">=</span> <span class="n">load_state_dict</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                            <span class="n">ms_dtype</span> <span class="o">=</span> <span class="n">get_state_dict_dtype</span><span class="p">(</span><span class="n">one_state_dict</span><span class="p">)</span>
                            <span class="k">del</span> <span class="n">one_state_dict</span>  <span class="c1"># free CPU memory</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                            <span class="s2">&quot;Since the `ms_dtype` attribute can&#39;t be found in model&#39;s config object, &quot;</span>
                            <span class="s2">&quot;will use ms_dtype=</span><span class="si">{ms_dtype}</span><span class="s2"> as derived from model&#39;s weights&quot;</span>
                        <span class="p">)</span>
                <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mindspore</span><span class="p">,</span> <span class="n">ms_dtype</span><span class="p">):</span>
                    <span class="n">ms_dtype</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mindspore</span><span class="p">,</span> <span class="n">ms_dtype</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s1">&#39;`ms_dtype` can be one of: `mindspore.dtype.TensorType`, `&quot;auto&quot;` or a string of a valid `mindspore.dtype.TensorType`, but received </span><span class="si">{</span><span class="n">ms_dtype</span><span class="si">}</span><span class="s1">&#39;</span>
                    <span class="p">)</span>
            <span class="n">dtype_orig</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_set_default_ms_dtype</span><span class="p">(</span><span class="n">ms_dtype</span><span class="p">)</span>

        <span class="c1"># Check if `_keep_in_fp32_modules` is not None</span>
        <span class="n">use_keep_in_fp32_modules</span> <span class="o">=</span> <span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">_keep_in_fp32_modules</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">ms_dtype</span> <span class="o">==</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">is_sharded</span><span class="p">:</span>
            <span class="n">loaded_state_dict_keys</span> <span class="o">=</span> <span class="n">sharded_metadata</span><span class="p">[</span><span class="s2">&quot;all_checkpoint_keys&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loaded_state_dict_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="n">config</span><span class="o">.</span><span class="n">name_or_path</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>

    <span class="c1"># Instantiate model.</span>
    <span class="n">init_contexts</span> <span class="o">=</span> <span class="p">[</span><span class="n">no_init_weights</span><span class="p">(</span><span class="n">_enable</span><span class="o">=</span><span class="n">_fast_init</span><span class="p">)]</span>

    <span class="n">config</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># We do not want to modify the config inplace in from_pretrained.</span>
    <span class="n">config</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_autoset_attn_implementation</span><span class="p">(</span>
        <span class="n">config</span><span class="p">,</span> <span class="n">use_flash_attention_2</span><span class="o">=</span><span class="n">use_flash_attention_2</span><span class="p">,</span> <span class="n">ms_dtype</span><span class="o">=</span><span class="n">ms_dtype</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="n">device_map</span>
    <span class="p">)</span>

    <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;mirror&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">ContextManagers</span><span class="p">(</span><span class="n">init_contexts</span><span class="p">):</span>
        <span class="c1"># Let&#39;s make sure we don&#39;t run the init function of buffer modules</span>
        <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">model_args</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>
    <span class="c1"># make sure we use the model&#39;s config since the __init__ call might have copied it</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>

    <span class="c1"># Check first if we are `from_pt`</span>
    <span class="k">if</span> <span class="n">use_keep_in_fp32_modules</span><span class="p">:</span>
        <span class="n">keep_in_fp32_modules</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_keep_in_fp32_modules</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">keep_in_fp32_modules</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device_map</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">special_dtypes</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="n">special_dtypes</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="n">name</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span>
                <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">m</span> <span class="ow">in</span> <span class="n">name</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">keep_in_fp32_modules</span><span class="p">)</span>
            <span class="p">}</span>
        <span class="p">)</span>

        <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">ms_dtype</span>

        <span class="n">no_split_modules</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_get_no_split_modules</span><span class="p">(</span><span class="n">device_map</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">device_map</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="s2">&quot;balanced&quot;</span><span class="p">,</span> <span class="s2">&quot;balanced_low_0&quot;</span><span class="p">,</span> <span class="s2">&quot;sequential&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;If passing a string for `device_map`, please choose &#39;auto&#39;, &#39;balanced&#39;, &#39;balanced_low_0&#39; or &quot;</span>
                <span class="s2">&quot;&#39;sequential&#39;.&quot;</span>
            <span class="p">)</span>

        <span class="n">device_map_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;no_split_module_classes&quot;</span><span class="p">:</span> <span class="n">no_split_modules</span><span class="p">}</span>
        <span class="k">if</span> <span class="s2">&quot;special_dtypes&quot;</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">infer_auto_device_map</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">device_map_kwargs</span><span class="p">[</span><span class="s2">&quot;special_dtypes&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">special_dtypes</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">special_dtypes</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;This model has some weights that should be kept in higher precision, you need to upgrade &quot;</span>
                <span class="s2">&quot;`accelerate` to properly deal with them (`pip install --upgrade accelerate`).&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">device_map</span> <span class="o">!=</span> <span class="s2">&quot;sequential&quot;</span><span class="p">:</span>
            <span class="n">max_memory</span> <span class="o">=</span> <span class="n">get_balanced_memory</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">target_dtype</span><span class="p">,</span>
                <span class="n">low_zero</span><span class="o">=</span><span class="p">(</span><span class="n">device_map</span> <span class="o">==</span> <span class="s2">&quot;balanced_low_0&quot;</span><span class="p">),</span>
                <span class="n">max_memory</span><span class="o">=</span><span class="n">max_memory</span><span class="p">,</span>
                <span class="o">**</span><span class="n">device_map_kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">max_memory</span> <span class="o">=</span> <span class="n">get_max_memory</span><span class="p">(</span><span class="n">max_memory</span><span class="p">)</span>

        <span class="n">device_map_kwargs</span><span class="p">[</span><span class="s2">&quot;max_memory&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_memory</span>

        <span class="c1"># Make sure tied weights are tied before creating the device map.</span>
        <span class="n">model</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>
        <span class="n">device_map</span> <span class="o">=</span> <span class="n">infer_auto_device_map</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">target_dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">device_map_kwargs</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">device_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>
        <span class="n">tied_params</span> <span class="o">=</span> <span class="n">find_tied_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="c1"># check if we don&#39;t have tied param in different devices</span>
        <span class="n">check_tied_parameters_on_same_device</span><span class="p">(</span><span class="n">tied_params</span><span class="p">,</span> <span class="n">device_map</span><span class="p">)</span>

    <span class="c1"># replace unnessasery modules to nn.Identity and insert send/recv for pipeline inference.</span>
    <span class="k">if</span> <span class="n">device_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">group_size</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">modify_model_for_pp_infer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_map</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">_no_split_modules</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">from_pt</span><span class="p">:</span>
        <span class="c1"># restore default dtype</span>
        <span class="k">if</span> <span class="n">dtype_orig</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">set_default_dtype</span><span class="p">(</span><span class="n">dtype_orig</span><span class="p">)</span>

        <span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">missing_keys</span><span class="p">,</span>
            <span class="n">unexpected_keys</span><span class="p">,</span>
            <span class="n">mismatched_keys</span><span class="p">,</span>
            <span class="n">offload_index</span><span class="p">,</span>
            <span class="n">error_msgs</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_load_pretrained_model</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">loaded_state_dict_keys</span><span class="p">,</span>
            <span class="n">resolved_archive_file</span><span class="p">,</span>
            <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
            <span class="n">ignore_mismatched_sizes</span><span class="o">=</span><span class="n">ignore_mismatched_sizes</span><span class="p">,</span>
            <span class="n">sharded_metadata</span><span class="o">=</span><span class="n">sharded_metadata</span><span class="p">,</span>
            <span class="n">_fast_init</span><span class="o">=</span><span class="n">_fast_init</span><span class="p">,</span>
            <span class="n">low_cpu_mem_usage</span><span class="o">=</span><span class="n">low_cpu_mem_usage</span><span class="p">,</span>
            <span class="n">device_map</span><span class="o">=</span><span class="n">device_map</span><span class="p">,</span>
            <span class="n">offload_folder</span><span class="o">=</span><span class="n">offload_folder</span><span class="p">,</span>
            <span class="n">offload_state_dict</span><span class="o">=</span><span class="n">offload_state_dict</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">ms_dtype</span><span class="p">,</span>
            <span class="n">keep_in_fp32_modules</span><span class="o">=</span><span class="n">keep_in_fp32_modules</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># make sure token embedding weights are still tied if needed</span>
    <span class="n">model</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>

    <span class="c1"># Set model in evaluation mode to deactivate DropOut modules by default</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="c1"># If it is a model with generation capabilities, attempt to load the generation config</span>
    <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">can_generate</span><span class="p">()</span> <span class="ow">and</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
                <span class="n">mirror</span><span class="o">=</span><span class="n">mirror</span><span class="p">,</span>
                <span class="n">_from_auto</span><span class="o">=</span><span class="n">from_auto_class</span><span class="p">,</span>
                <span class="n">_from_pipeline</span><span class="o">=</span><span class="n">from_pipeline</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Generation config file not found, using a generation config created from the model config.&quot;</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">_adapter_model_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span>
            <span class="n">_adapter_model_path</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">adapter_kwargs</span><span class="o">=</span><span class="n">adapter_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">output_loading_info</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">loading_info</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loading_info</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;missing_keys&quot;</span><span class="p">:</span> <span class="n">missing_keys</span><span class="p">,</span>
                <span class="s2">&quot;unexpected_keys&quot;</span><span class="p">:</span> <span class="n">unexpected_keys</span><span class="p">,</span>
                <span class="s2">&quot;mismatched_keys&quot;</span><span class="p">:</span> <span class="n">mismatched_keys</span><span class="p">,</span>
                <span class="s2">&quot;error_msgs&quot;</span><span class="p">:</span> <span class="n">error_msgs</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">loading_info</span>

    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.get_input_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.get_input_embeddings" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the model's input embeddings.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="mindnlp.core.nn.Module">Module</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>nn.Module</code>: A torch module mapping vocabulary to hidden states.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the model&#39;s input embeddings.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `nn.Module`: A torch module mapping vocabulary to hidden states.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">base_model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">base_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">base_model</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.get_memory_footprint" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">(</span><span class="n">return_buffers</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.get_memory_footprint" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Get the memory footprint of a model. This will return the memory footprint of the current model in bytes.
Useful to benchmark the memory footprint of the current model and design some tests. Solution inspired from the
PyTorch discussions: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822/2</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>return_buffers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to return the size of the buffer tensors in the computation of the memory footprint. Buffers
are tensors that do not require gradients and not registered as parameters. E.g. mean and std in batch
norm layers. Please see: https://discuss.pytorch.org/t/what-pytorch-means-by-buffers/120266/2</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2142</span>
<span class="normal">2143</span>
<span class="normal">2144</span>
<span class="normal">2145</span>
<span class="normal">2146</span>
<span class="normal">2147</span>
<span class="normal">2148</span>
<span class="normal">2149</span>
<span class="normal">2150</span>
<span class="normal">2151</span>
<span class="normal">2152</span>
<span class="normal">2153</span>
<span class="normal">2154</span>
<span class="normal">2155</span>
<span class="normal">2156</span>
<span class="normal">2157</span>
<span class="normal">2158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_memory_footprint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">return_buffers</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the memory footprint of a model. This will return the memory footprint of the current model in bytes.</span>
<span class="sd">    Useful to benchmark the memory footprint of the current model and design some tests. Solution inspired from the</span>
<span class="sd">    PyTorch discussions: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822/2</span>

<span class="sd">    Arguments:</span>
<span class="sd">        return_buffers (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to return the size of the buffer tensors in the computation of the memory footprint. Buffers</span>
<span class="sd">            are tensors that do not require gradients and not registered as parameters. E.g. mean and std in batch</span>
<span class="sd">            norm layers. Please see: https://discuss.pytorch.org/t/what-pytorch-means-by-buffers/120266/2</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mem</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="o">*</span> <span class="n">param</span><span class="o">.</span><span class="n">itemsize</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">return_buffers</span><span class="p">:</span>
        <span class="n">mem_bufs</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="o">*</span> <span class="n">buf</span><span class="o">.</span><span class="n">itemsize</span> <span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffers</span><span class="p">())</span>
        <span class="n">mem</span> <span class="o">=</span> <span class="n">mem</span> <span class="o">+</span> <span class="n">mem_bufs</span>
    <span class="k">return</span> <span class="n">mem</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.get_output_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.get_output_embeddings" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the model's output embeddings.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="mindnlp.core.nn.Module">Module</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>nn.Module</code>: A torch module mapping hidden states to vocabulary.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the model&#39;s output embeddings.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `nn.Module`: A torch module mapping hidden states to vocabulary.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Overwrite for models with output embeddings</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.gradient_checkpointing_disable" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">gradient_checkpointing_disable</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.gradient_checkpointing_disable" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Deactivates gradient checkpointing for the current model.</p>
<p>Note that in other frameworks this feature can be referred to as "activation checkpointing" or "checkpoint
activations".</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">gradient_checkpointing_disable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Deactivates gradient checkpointing for the current model.</span>

<span class="sd">    Note that in other frameworks this feature can be referred to as &quot;activation checkpointing&quot; or &quot;checkpoint</span>
<span class="sd">    activations&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">supports_gradient_checkpointing</span><span class="p">:</span>
        <span class="c1"># For old GC format (transformers &lt; 4.35.0) for models that live on the Hub</span>
        <span class="c1"># we will fall back to the overwritten `_set_gradient_checkpointing` methid</span>
        <span class="n">_is_using_old_format</span> <span class="o">=</span> <span class="s2">&quot;value&quot;</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_set_gradient_checkpointing</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_using_old_format</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_gradient_checkpointing</span><span class="p">(</span><span class="n">enable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).&quot;</span>
                <span class="s2">&quot;Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_set_gradient_checkpointing</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_hf_peft_config_loaded&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">disable_input_require_grads</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.gradient_checkpointing_enable" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">(</span><span class="n">gradient_checkpointing_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.gradient_checkpointing_enable" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Activates gradient checkpointing for the current model.</p>
<p>Note that in other frameworks this feature can be referred to as "activation checkpointing" or "checkpoint
activations".</p>
<p>We pass the <code>__call__</code> method of the modules instead of <code>forward</code> because <code>__call__</code> attaches all the hooks of
the module. https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>gradient_checkpointing_kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Additional keyword arguments passed along to the <code>torch.utils.checkpoint.checkpoint</code> function.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>dict, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">gradient_checkpointing_enable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient_checkpointing_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Activates gradient checkpointing for the current model.</span>

<span class="sd">    Note that in other frameworks this feature can be referred to as &quot;activation checkpointing&quot; or &quot;checkpoint</span>
<span class="sd">    activations&quot;.</span>

<span class="sd">    We pass the `__call__` method of the modules instead of `forward` because `__call__` attaches all the hooks of</span>
<span class="sd">    the module. https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2</span>

<span class="sd">    Args:</span>
<span class="sd">        gradient_checkpointing_kwargs (dict, *optional*):</span>
<span class="sd">            Additional keyword arguments passed along to the `torch.utils.checkpoint.checkpoint` function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">supports_gradient_checkpointing</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> does not support gradient checkpointing.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">gradient_checkpointing_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">gradient_checkpointing_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;use_reentrant&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>

    <span class="k">if</span> <span class="n">GENERATOR_SEED</span><span class="p">:</span>
        <span class="n">gradient_checkpointing_func</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">recompute</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">function</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">gradient_checkpointing_func</span> <span class="o">=</span> <span class="n">function</span>

    <span class="c1"># For old GC format (transformers &lt; 4.35.0) for models that live on the Hub</span>
    <span class="c1"># we will fall back to the overwritten `_set_gradient_checkpointing` method</span>
    <span class="n">_is_using_old_format</span> <span class="o">=</span> <span class="s2">&quot;value&quot;</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_set_gradient_checkpointing</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_using_old_format</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_gradient_checkpointing</span><span class="p">(</span><span class="n">enable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">gradient_checkpointing_func</span><span class="o">=</span><span class="n">gradient_checkpointing_func</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_set_gradient_checkpointing</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).&quot;</span>
            <span class="s2">&quot;Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_hf_peft_config_loaded&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="c1"># When using PEFT + gradient checkpointing + Trainer we need to make sure the input has requires_grad=True</span>
        <span class="c1"># we do it also on PEFT: https://github.com/huggingface/peft/blob/85013987aa82aa1af3da1236b6902556ce3e483e/src/peft/peft_model.py#L334</span>
        <span class="c1"># When training with PEFT, only LoRA layers will have requires grad set to True, but the output of frozen layers need to propagate</span>
        <span class="c1"># the gradients to make sure the gradient flows.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enable_input_require_grads</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.init_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.init_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>If needed prunes and maybe initializes weights. If using a custom <code>PreTrainedModel</code>, you need to implement any
initialization logic in <code>_init_weights</code>.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    If needed prunes and maybe initializes weights. If using a custom `PreTrainedModel`, you need to implement any</span>
<span class="sd">    initialization logic in `_init_weights`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Prune heads if needed</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_init_weights</span><span class="p">:</span>
        <span class="c1"># Initialize weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">)</span>

        <span class="c1"># Tie weights should be skipped when not initializing all weights</span>
        <span class="c1"># since from_pretrained(...) calls tie weights anyways</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.post_init" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.post_init" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>A method executed at the end of each Transformer model initialization, to execute code that needs the model's
modules properly initialized (such as weight initialization).</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">post_init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A method executed at the end of each Transformer model initialization, to execute code that needs the model&#39;s</span>
<span class="sd">    modules properly initialized (such as weight initialization).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_compatibility_gradient_checkpointing</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.prune_heads" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">prune_heads</span><span class="p">(</span><span class="n">heads_to_prune</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.prune_heads" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Prunes heads of the base model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>heads_to_prune</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dictionary with keys being selected layer indices (<code>int</code>) and associated values being the list of heads
to prune in said layer (list of <code>int</code>). For instance {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on
layer 1 and heads 2 and 3 on layer 2.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[int, List[int]]`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">heads_to_prune</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prunes heads of the base model.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        heads_to_prune (`Dict[int, List[int]]`):</span>
<span class="sd">            Dictionary with keys being selected layer indices (`int`) and associated values being the list of heads</span>
<span class="sd">            to prune in said layer (list of `int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on</span>
<span class="sd">            layer 1 and heads 2 and 3 on layer 2.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># save new sets of pruned heads as union of previously stored pruned heads and newly pruned heads</span>
    <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">heads</span> <span class="ow">in</span> <span class="n">heads_to_prune</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">union_heads</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pruned_heads</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="p">[]))</span> <span class="o">|</span> <span class="nb">set</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">union_heads</span><span class="p">)</span>  <span class="c1"># Unfortunately we have to store it as list for JSON</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">_prune_heads</span><span class="p">(</span><span class="n">heads_to_prune</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.register_for_auto_class" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">register_for_auto_class</span><span class="p">(</span><span class="n">auto_class</span><span class="o">=</span><span class="s1">&#39;AutoModel&#39;</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.register_for_auto_class" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Register this class with a given auto class. This should only be used for custom models as the ones in the
library are already mapped with an auto class.</p>
<p><Tip warning={true}></p>
<p>This API is experimental and may have some slight breaking changes in the next releases.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>auto_class</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The auto class to register this new model with.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `type`, *optional*, defaults to `&#34;AutoModel&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;AutoModel&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3457</span>
<span class="normal">3458</span>
<span class="normal">3459</span>
<span class="normal">3460</span>
<span class="normal">3461</span>
<span class="normal">3462</span>
<span class="normal">3463</span>
<span class="normal">3464</span>
<span class="normal">3465</span>
<span class="normal">3466</span>
<span class="normal">3467</span>
<span class="normal">3468</span>
<span class="normal">3469</span>
<span class="normal">3470</span>
<span class="normal">3471</span>
<span class="normal">3472</span>
<span class="normal">3473</span>
<span class="normal">3474</span>
<span class="normal">3475</span>
<span class="normal">3476</span>
<span class="normal">3477</span>
<span class="normal">3478</span>
<span class="normal">3479</span>
<span class="normal">3480</span>
<span class="normal">3481</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">register_for_auto_class</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">auto_class</span><span class="o">=</span><span class="s2">&quot;AutoModel&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Register this class with a given auto class. This should only be used for custom models as the ones in the</span>
<span class="sd">    library are already mapped with an auto class.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This API is experimental and may have some slight breaking changes in the next releases.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        auto_class (`str` or `type`, *optional*, defaults to `&quot;AutoModel&quot;`):</span>
<span class="sd">            The auto class to register this new model with.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">auto_class</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">auto_class</span> <span class="o">=</span> <span class="n">auto_class</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="kn">import</span> <span class="nn">mindnlp.transformers.models.auto</span> <span class="k">as</span> <span class="nn">auto_module</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">auto_module</span><span class="p">,</span> <span class="n">auto_class</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">auto_class</span><span class="si">}</span><span class="s2"> is not a valid auto class.&quot;</span><span class="p">)</span>

    <span class="bp">cls</span><span class="o">.</span><span class="n">_auto_class</span> <span class="o">=</span> <span class="n">auto_class</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.resize_token_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="n">new_num_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.resize_token_embeddings" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Resizes input token embeddings matrix of the model if <code>new_num_tokens != config.vocab_size</code>.</p>
<p>Takes care of tying weights embeddings afterwards if the model class has a <code>tie_weights()</code> method.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>new_num_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The new number of tokens in the embedding matrix. Increasing the size will add newly initialized
vectors at the end. Reducing the size will remove vectors from the end. If not provided or <code>None</code>, just
returns a pointer to the input tokens <code>nn.Embedding</code> module of the model without doing anything.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_to_multiple_of</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If set will pad the embedding matrix to a multiple of the provided value.If <code>new_num_tokens</code> is set to
<code>None</code> will just pad the embedding to a multiple of <code>pad_to_multiple_of</code>.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
<code>&gt;= 7.5</code> (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128. For more
details about this, or help on choosing the correct value for resizing, refer to this guide:
https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="return" open>
  <summary>Return</summary>
  <p><code>nn.Embedding</code>: Pointer to the input tokens Embeddings Module of the model.</p>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">resize_token_embeddings</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.</span>

<span class="sd">    Takes care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        new_num_tokens (`int`, *optional*):</span>
<span class="sd">            The new number of tokens in the embedding matrix. Increasing the size will add newly initialized</span>
<span class="sd">            vectors at the end. Reducing the size will remove vectors from the end. If not provided or `None`, just</span>
<span class="sd">            returns a pointer to the input tokens `nn.Embedding` module of the model without doing anything.</span>
<span class="sd">        pad_to_multiple_of (`int`, *optional*):</span>
<span class="sd">            If set will pad the embedding matrix to a multiple of the provided value.If `new_num_tokens` is set to</span>
<span class="sd">            `None` will just pad the embedding to a multiple of `pad_to_multiple_of`.</span>

<span class="sd">            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</span>
<span class="sd">            `&gt;= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128. For more</span>
<span class="sd">            details about this, or help on choosing the correct value for resizing, refer to this guide:</span>
<span class="sd">            https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc</span>

<span class="sd">    Return:</span>
<span class="sd">        `nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_resize_token_embeddings</span><span class="p">(</span><span class="n">new_num_tokens</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">new_num_tokens</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pad_to_multiple_of</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">model_embeds</span>

    <span class="c1"># Since we are basically resuing the same old embeddings with new weight values, gathering is required</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">model_embeds</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Update base model and current model config</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;text_config&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">text_config</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>

    <span class="c1"># Tie weights again if needed</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">model_embeds</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.save_pretrained" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="n">save_checkpoint</span><span class="p">,</span> <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_shard_size</span><span class="o">=</span><span class="s1">&#39;5GB&#39;</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">variant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_peft_format</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.save_pretrained" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save a model and its configuration file to a directory, so that it can be re-loaded using the
[<code>~PreTrainedModel.from_pretrained</code>] class method.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to which to save. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_main_process</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful when in distributed training like
TPUs and need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on
the main process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The state dictionary of the model to save. Will default to <code>self.state_dict()</code>, but can be used to only
save parts of the model or if special precautions need to be taken when recovering the state dictionary
of a model (like when using model parallelism).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>nested dictionary of `mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful on distributed training like TPUs when one
need to replace <code>torch.save</code> by another method.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code><span title="mindnlp.core.serialization.save_checkpoint">save_checkpoint</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>push_to_hub</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_shard_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size
lower than this size. If expressed as a string, needs to be digits followed by a unit (like <code>"5MB"</code>).
We default it to 5GB in order for models to be able to run easily on free-tier google colab instances
without CPU OOM issues.</p>
<p><Tip warning={true}></p>
<p>If a single weight of the model is bigger than <code>max_shard_size</code>, it will be in its own checkpoint shard
which will be bigger than <code>max_shard_size</code>.</p>
<p></Tip></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int` or `str`, *optional*, defaults to `&#34;5GB&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;5GB&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_serialization</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional PyTorch way (that uses <code>pickle</code>).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>variant</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If specified, weights are saved in the format pytorch_model.<variant>.bin.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, or not specified, will use
the token generated when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_peft_format</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>For backward compatibility with PEFT library, in case adapter weights are attached to the model, all
keys of the state dict of adapters needs to be pre-pended with <code>base_model.model</code>. Advanced users can
disable this behaviours by setting <code>save_peft_format</code> to <code>False</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Additional key word arguments passed along to the [<code>~utils.PushToHubMixin.push_to_hub</code>] method.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, Any]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span>
<span class="normal">2105</span>
<span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span>
<span class="normal">2109</span>
<span class="normal">2110</span>
<span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">save_pretrained</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">save_checkpoint</span><span class="p">,</span>
    <span class="n">push_to_hub</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">max_shard_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;5GB&quot;</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">variant</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">token</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_peft_format</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save a model and its configuration file to a directory, so that it can be re-loaded using the</span>
<span class="sd">    [`~PreTrainedModel.from_pretrained`] class method.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to which to save. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful when in distributed training like</span>
<span class="sd">            TPUs and need to call this function on all processes. In this case, set `is_main_process=True` only on</span>
<span class="sd">            the main process to avoid race conditions.</span>
<span class="sd">        state_dict (nested dictionary of `mindspore.Tensor`):</span>
<span class="sd">            The state dictionary of the model to save. Will default to `self.state_dict()`, but can be used to only</span>
<span class="sd">            save parts of the model or if special precautions need to be taken when recovering the state dictionary</span>
<span class="sd">            of a model (like when using model parallelism).</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful on distributed training like TPUs when one</span>
<span class="sd">            need to replace `torch.save` by another method.</span>
<span class="sd">        push_to_hub (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the</span>
<span class="sd">            repository you want to push to with `repo_id` (will default to the name of `save_directory` in your</span>
<span class="sd">            namespace).</span>
<span class="sd">        max_shard_size (`int` or `str`, *optional*, defaults to `&quot;5GB&quot;`):</span>
<span class="sd">            The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size</span>
<span class="sd">            lower than this size. If expressed as a string, needs to be digits followed by a unit (like `&quot;5MB&quot;`).</span>
<span class="sd">            We default it to 5GB in order for models to be able to run easily on free-tier google colab instances</span>
<span class="sd">            without CPU OOM issues.</span>

<span class="sd">            &lt;Tip warning={true}&gt;</span>

<span class="sd">            If a single weight of the model is bigger than `max_shard_size`, it will be in its own checkpoint shard</span>
<span class="sd">            which will be bigger than `max_shard_size`.</span>

<span class="sd">            &lt;/Tip&gt;</span>

<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).</span>
<span class="sd">        variant (`str`, *optional*):</span>
<span class="sd">            If specified, weights are saved in the format pytorch_model.&lt;variant&gt;.bin.</span>
<span class="sd">        token (`str` or `bool`, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use</span>
<span class="sd">            the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).</span>
<span class="sd">        save_peft_format (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            For backward compatibility with PEFT library, in case adapter weights are attached to the model, all</span>
<span class="sd">            keys of the state dict of adapters needs to be pre-pended with `base_model.model`. Advanced users can</span>
<span class="sd">            disable this behaviours by setting `save_peft_format` to `False`.</span>
<span class="sd">        kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">            Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">use_auth_token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_auth_token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">ignore_metadata_errors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;ignore_metadata_errors&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">use_auth_token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;The `use_auth_token` argument is deprecated. Please use `token` instead.&quot;</span><span class="p">,</span>
            <span class="ne">FutureWarning</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`token` and `use_auth_token` are both specified. Please set only the argument `token`.&quot;</span>
            <span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">use_auth_token</span>

    <span class="k">if</span> <span class="n">token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">token</span>

    <span class="n">_hf_peft_config_loaded</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_hf_peft_config_loaded&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;save_config&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;`save_config` is deprecated. Use `is_main_process` instead.&quot;</span>
        <span class="p">)</span>
        <span class="n">is_main_process</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;save_config&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">safe_serialization</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_safetensors_available</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;`safe_serialization` requires the `safetensors library: `pip install safetensors`.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Provided path (</span><span class="si">{</span><span class="n">save_directory</span><span class="si">}</span><span class="s2">) should be a directory, not a file&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">push_to_hub</span><span class="p">:</span>
        <span class="n">commit_message</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;commit_message&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">repo_id</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;repo_id&quot;</span><span class="p">,</span> <span class="n">save_directory</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">sep</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">repo_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_repo</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">files_timestamps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_files_timestamps</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>

    <span class="c1"># Only save the model itself if we are using distributed training</span>
    <span class="n">model_to_save</span> <span class="o">=</span> <span class="n">unwrap_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="c1"># save the string version of dtype to the config, e.g. convert mindspore.float32 =&gt; &quot;float32&quot;</span>
    <span class="c1"># we currently don&#39;t use this setting automatically, but may start to use with v5</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">get_parameter_dtype</span><span class="p">(</span><span class="n">model_to_save</span><span class="p">)</span>
    <span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

    <span class="c1"># Attach architecture to the config</span>
    <span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">architectures</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_to_save</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">]</span>

    <span class="c1"># Save the config</span>
    <span class="k">if</span> <span class="n">is_main_process</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_hf_peft_config_loaded</span><span class="p">:</span>
            <span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">can_generate</span><span class="p">():</span>
            <span class="c1"># generation config built from the model config + the model config holds generation kwargs -&gt; generate</span>
            <span class="c1"># may revert to legacy behavior if the two don&#39;t match</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">model_to_save</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">_from_model_config</span>
                <span class="ow">and</span> <span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_has_non_default_generation_parameters</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="n">new_generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_model_config</span><span class="p">(</span><span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">new_generation_config</span> <span class="o">!=</span> <span class="n">model_to_save</span><span class="o">.</span><span class="n">generation_config</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;Your generation config was originally created from the model config, but the model &quot;</span>
                        <span class="s2">&quot;config has changed since then. Unless you pass the `generation_config` argument to this &quot;</span>
                        <span class="s2">&quot;model&#39;s `generate` calls, they will revert to the legacy behavior where the base &quot;</span>
                        <span class="s2">&quot;`generate` parameterization is loaded from the model config instead. &quot;</span>
                        <span class="s2">&quot;To avoid this behavior and this warning, we recommend you to overwrite the generation &quot;</span>
                        <span class="s2">&quot;config model attribute before calling the model&#39;s `save_pretrained`, preferably also &quot;</span>
                        <span class="s2">&quot;removing any generation kwargs from the model config. This warning will be raised to an &quot;</span>
                        <span class="s2">&quot;exception in v4.41.&quot;</span>
                    <span class="p">)</span>
            <span class="n">model_to_save</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">_hf_peft_config_loaded</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Detected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.&quot;</span>
            <span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">model_to_save</span><span class="o">.</span><span class="n">get_adapter_state_dict</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">save_peft_format</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="s2">&quot;To match the expected format of the PEFT library, all keys of the state dict of adapters will be pre-pended with `base_model.model`.&quot;</span>
                <span class="p">)</span>
                <span class="n">peft_state_dict</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">peft_state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;base_model.model.</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">peft_state_dict</span>

            <span class="n">active_adapter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_adapters</span><span class="p">()</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">active_adapter</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Multiple active adapters detected, saving multiple active adapters is not supported yet. You can save adapters separately one by one &quot;</span>
                    <span class="s2">&quot;by iteratively calling `model.set_adapter(adapter_name)` then `model.save_pretrained(...)`&quot;</span>
                <span class="p">)</span>
            <span class="n">active_adapter</span> <span class="o">=</span> <span class="n">active_adapter</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">current_peft_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">peft_config</span><span class="p">[</span><span class="n">active_adapter</span><span class="p">]</span>
            <span class="n">current_peft_config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>

    <span class="c1"># for offloaded modules</span>
    <span class="n">module_map</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Save the model</span>
    <span class="k">if</span> <span class="n">state_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># if any model parameters are offloaded, make module map</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;hf_device_map&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_device_map</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span> <span class="o">&gt;</span> <span class="mi">1</span>
            <span class="ow">and</span> <span class="p">(</span><span class="s2">&quot;cpu&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_device_map</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="ow">or</span> <span class="s2">&quot;disk&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_device_map</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)&quot;</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model_to_save</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">module_state_dict</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

                <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">module_state_dict</span><span class="p">:</span>
                    <span class="n">module_map</span><span class="p">[</span><span class="n">name</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;.</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">model_to_save</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

    <span class="c1"># Handle the case where some state_dict keys shouldn&#39;t be saved</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_keys_to_ignore_on_save</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">ignore_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_keys_to_ignore_on_save</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">ignore_key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="k">del</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">ignore_key</span><span class="p">]</span>

    <span class="c1"># Shard the model if it is too big.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_hf_peft_config_loaded</span><span class="p">:</span>
        <span class="n">weights_name</span> <span class="o">=</span> <span class="n">SAFE_WEIGHTS_NAME</span> <span class="k">if</span> <span class="n">safe_serialization</span> <span class="k">else</span> <span class="n">WEIGHTS_NAME</span>
        <span class="n">weights_name</span> <span class="o">=</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">weights_name</span><span class="p">,</span> <span class="n">variant</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">weights_name</span> <span class="o">=</span> <span class="n">ADAPTER_SAFE_WEIGHTS_NAME</span> <span class="k">if</span> <span class="n">safe_serialization</span> <span class="k">else</span> <span class="n">ADAPTER_WEIGHTS_NAME</span>

    <span class="n">filename_pattern</span> <span class="o">=</span> <span class="n">weights_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.bin&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{suffix}</span><span class="s2">.bin&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.safetensors&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{suffix}</span><span class="s2">.safetensors&quot;</span><span class="p">)</span>
    <span class="n">state_dict_split</span> <span class="o">=</span> <span class="n">split_state_dict_into_shards</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">filename_pattern</span><span class="o">=</span><span class="n">filename_pattern</span><span class="p">,</span> <span class="n">max_shard_size</span><span class="o">=</span><span class="n">max_shard_size</span>
    <span class="p">)</span>
    <span class="c1"># Save index if sharded</span>
    <span class="n">index</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">state_dict_split</span><span class="o">.</span><span class="n">is_sharded</span><span class="p">:</span>
        <span class="n">index</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;metadata&quot;</span><span class="p">:</span> <span class="n">state_dict_split</span><span class="o">.</span><span class="n">metadata</span><span class="p">,</span>
            <span class="s2">&quot;weight_map&quot;</span><span class="p">:</span> <span class="n">state_dict_split</span><span class="o">.</span><span class="n">tensor_to_filename</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="c1"># Clean the folder from a previous save</span>
    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
        <span class="n">full_filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
        <span class="c1"># If we have a shard file that is not going to be replaced, we delete it, but only from the main process</span>
        <span class="c1"># in distributed settings to avoid race conditions.</span>
        <span class="n">weights_no_suffix</span> <span class="o">=</span> <span class="n">weights_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.bin&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.safetensors&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>

        <span class="c1"># make sure that file to be deleted matches format of sharded file, e.g. pytorch_model-00001-of-00005</span>
        <span class="n">filename_no_suffix</span> <span class="o">=</span> <span class="n">filename</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.bin&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.safetensors&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="n">reg</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;(.*?)-\d</span><span class="si">{5}</span><span class="s2">-of-\d</span><span class="si">{5}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">filename</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">weights_no_suffix</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">full_filename</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">filename</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state_dict_split</span><span class="o">.</span><span class="n">filename_to_tensors</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="ow">and</span> <span class="n">is_main_process</span>
            <span class="ow">and</span> <span class="n">reg</span><span class="o">.</span><span class="n">fullmatch</span><span class="p">(</span><span class="n">filename_no_suffix</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">full_filename</span><span class="p">)</span>
    <span class="c1"># Save the model</span>
    <span class="n">filename_to_tensors</span> <span class="o">=</span> <span class="n">state_dict_split</span><span class="o">.</span><span class="n">filename_to_tensors</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">module_map</span><span class="p">:</span>
        <span class="n">filename_to_tensors</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">filename_to_tensors</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Saving checkpoint shards&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">shard_file</span><span class="p">,</span> <span class="n">tensors</span> <span class="ow">in</span> <span class="n">filename_to_tensors</span><span class="p">:</span>
        <span class="n">shard</span> <span class="o">=</span> <span class="p">{</span><span class="n">tensor</span><span class="p">:</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">tensor</span><span class="p">]</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">}</span>
        <span class="c1"># remake shard with onloaded parameters if necessary</span>
        <span class="k">if</span> <span class="n">module_map</span><span class="p">:</span>
            <span class="c1"># init state_dict for this shard</span>
            <span class="n">shard_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="s2">&quot;&quot;</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">shard</span><span class="p">}</span>
            <span class="k">for</span> <span class="n">module_name</span> <span class="ow">in</span> <span class="n">shard</span><span class="p">:</span>
                <span class="n">module</span> <span class="o">=</span> <span class="n">module_map</span><span class="p">[</span><span class="n">module_name</span><span class="p">]</span>
                <span class="c1"># update state dict with onloaded parameters</span>
                <span class="c1"># shard_state_dict = get_state_dict_from_offload(module, module_name, shard_state_dict)</span>

            <span class="c1"># assign shard to be the completed state dict</span>
            <span class="n">shard</span> <span class="o">=</span> <span class="n">shard_state_dict</span>
            <span class="k">del</span> <span class="n">shard_state_dict</span>
            <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">safe_serialization</span><span class="p">:</span>
            <span class="c1"># At some point we will need to deal better with save_function (used for TPU and other distributed</span>
            <span class="c1"># joyfulness), but for now this enough.</span>
            <span class="n">safe_save_file</span><span class="p">(</span><span class="n">shard</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">shard_file</span><span class="p">),</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;np&quot;</span><span class="p">})</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">save_function</span><span class="p">(</span><span class="n">shard</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">shard_file</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">path_to_weights</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">weights_name</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model weights saved in </span><span class="si">{</span><span class="n">path_to_weights</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">save_index_file</span> <span class="o">=</span> <span class="n">SAFE_WEIGHTS_INDEX_NAME</span> <span class="k">if</span> <span class="n">safe_serialization</span> <span class="k">else</span> <span class="n">WEIGHTS_INDEX_NAME</span>
        <span class="n">save_index_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">_add_variant</span><span class="p">(</span><span class="n">save_index_file</span><span class="p">,</span> <span class="n">variant</span><span class="p">))</span>
        <span class="c1"># Save the index as well</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">save_index_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">content</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The model is bigger than the maximum size per checkpoint (</span><span class="si">{</span><span class="n">max_shard_size</span><span class="si">}</span><span class="s2">) and is going to be &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;split in </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">state_dict_split</span><span class="o">.</span><span class="n">filename_to_tensors</span><span class="p">)</span><span class="si">}</span><span class="s2"> checkpoint shards. You can find where each parameters has been saved in the &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;index located at </span><span class="si">{</span><span class="n">save_index_file</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.set_input_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">set_input_embeddings</span><span class="p">(</span><span class="n">value</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.set_input_embeddings" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Set model's input embeddings.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>value</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A module mapping vocabulary to hidden states.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`nn.Module`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set model&#39;s input embeddings.</span>

<span class="sd">    Args:</span>
<span class="sd">        value (`nn.Module`): A module mapping vocabulary to hidden states.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">base_model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">base_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">:</span>
        <span class="n">base_model</span><span class="o">.</span><span class="n">set_input_embeddings</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.tie_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.tie_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Tie the weights between the input embeddings and the output embeddings.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">tie_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tie the weights between the input embeddings and the output embeddings.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;tie_word_embeddings&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
        <span class="n">output_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">output_embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="n">output_embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">())</span>

    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;is_encoder_decoder&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;tie_encoder_decoder&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">):</span>
            <span class="bp">self</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span> <span class="c1"># pylint: disable=self-cls-assignment</span>
        <span class="n">tied_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tie_encoder_decoder_weights</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="s2">&quot;encoder&quot;</span>
        <span class="p">)</span>
        <span class="c1"># Setting a dynamic variable instead of `_tied_weights_keys` because it&#39;s a class</span>
        <span class="c1"># attributed not an instance member, therefore modifying it will modify the entire class</span>
        <span class="c1"># Leading to issues on subsequent calls by different tests or subsequent calls.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_tied_weights_keys</span> <span class="o">=</span> <span class="n">tied_weights</span>

    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;_tie_weights&quot;</span><span class="p">):</span>
            <span class="n">module</span><span class="o">.</span><span class="n">_tie_weights</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.modeling_utils.PreTrainedModel.warn_if_padding_and_no_attention_mask" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">modeling_utils</span><span class="o">.</span><span class="n">PreTrainedModel</span><span class="o">.</span><span class="n">warn_if_padding_and_no_attention_mask</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.modeling_utils.PreTrainedModel.warn_if_padding_and_no_attention_mask" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Shows a one-time warning if the input_ids appear to contain padding and no attention mask was given.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\modeling_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3483</span>
<span class="normal">3484</span>
<span class="normal">3485</span>
<span class="normal">3486</span>
<span class="normal">3487</span>
<span class="normal">3488</span>
<span class="normal">3489</span>
<span class="normal">3490</span>
<span class="normal">3491</span>
<span class="normal">3492</span>
<span class="normal">3493</span>
<span class="normal">3494</span>
<span class="normal">3495</span>
<span class="normal">3496</span>
<span class="normal">3497</span>
<span class="normal">3498</span>
<span class="normal">3499</span>
<span class="normal">3500</span>
<span class="normal">3501</span>
<span class="normal">3502</span>
<span class="normal">3503</span>
<span class="normal">3504</span>
<span class="normal">3505</span>
<span class="normal">3506</span>
<span class="normal">3507</span>
<span class="normal">3508</span>
<span class="normal">3509</span>
<span class="normal">3510</span>
<span class="normal">3511</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">warn_if_padding_and_no_attention_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Shows a one-time warning if the input_ids appear to contain padding and no attention mask was given.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="n">ON_ORANGE_PI</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Check only the first and last input IDs to reduce overhead.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()[:,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]:</span>
        <span class="n">warn_string</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See &quot;</span>
            <span class="s2">&quot;https://huggingface.co/docs/transformers/troubleshooting&quot;</span>
            <span class="s2">&quot;#incorrect-output-when-padding-tokens-arent-masked.&quot;</span>
        <span class="p">)</span>

        <span class="c1"># If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an</span>
        <span class="c1"># attention_mask or not. In this case, we should still show a warning because this is a rare case.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bos_token_id</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
            <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
            <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sep_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sep_token_id</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">warn_string</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">You may ignore this warning if your `pad_token_id` (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="si">}</span><span class="s2">) is identical &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;to the `bos_token_id` (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bos_token_id</span><span class="si">}</span><span class="s2">), `eos_token_id` (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="si">}</span><span class="s2">), &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;or the `sep_token_id` (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sep_token_id</span><span class="si">}</span><span class="s2">), and your input is not padded.&quot;</span>
            <span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span><span class="n">warn_string</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="页脚" >
        
          
          <a href="../configuration_utils/" class="md-footer__link md-footer__link--prev" aria-label="上一页: configuration_utils">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                上一页
              </span>
              <div class="md-ellipsis">
                configuration_utils
              </div>
            </div>
          </a>
        
        
          
          <a href="../tokenization_utils/" class="md-footer__link md-footer__link--next" aria-label="下一页: tokenization_utils">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                下一页
              </span>
              <div class="md-ellipsis">
                tokenization_utils
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 - 2024 MindSpore Lab and CQU NLP Team.
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:lvyufeng@cqu.edu.cn" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindnlp" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/lu-yu-feng-46-1" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.indexes", "navigation.top", "navigation.footer", "navigation.path", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>