
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../cpmant/">
      
      
        <link rel="next" href="../ctrl/">
      
      
      <link rel="icon" href="../../../../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.44">
    
    
      
        <title>cpmbee - MindNLP Docs</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mindnlp.transformers.models.cpmbee.configuration_cpmbee" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../../../" title="MindNLP Docs" class="md-header__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindNLP Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              cpmbee
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="选择当前语言">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="../../../../../api/transformers/models/cpmbee/" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="./" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="分享" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindnlp" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../" class="md-tabs__link">
        
  
    
  
  主页

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../tutorials/quick_start/" class="md-tabs__link">
          
  
    
  
  教程

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../supported_models/" class="md-tabs__link">
        
  
    
  
  模型列表

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../contribute/" class="md-tabs__link">
        
  
    
  
  代码贡献

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../accelerate/" class="md-tabs__link">
          
  
    
  
  API文档

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../notes/changelog/" class="md-tabs__link">
          
  
    
  
  说明

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../" title="MindNLP Docs" class="md-nav__button md-logo" aria-label="MindNLP Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    MindNLP Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindnlp" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindnlp
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    主页
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    教程
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            教程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/quick_start/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/data_preprocess/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Preprocess
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../tutorials/use_mirror/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Use Mirror
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../supported_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    模型列表
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../contribute/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    代码贡献
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    API文档
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            API文档
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../accelerate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Accelerate
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../dataset/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Dataset
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            Dataset
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/load_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    load_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/BaseMapFunction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BaseMapFunction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../dataset/transforms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transforms
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" >
        
          
          <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Engine
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            Engine
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_1" >
        
          
          <label class="md-nav__link" for="__nav_5_4_1" id="__nav_5_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    train_args
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_1">
            <span class="md-nav__icon md-icon"></span>
            train_args
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/train_args/seq2seq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seq2seq
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4_2" >
        
          
          <label class="md-nav__link" for="__nav_5_4_2" id="__nav_5_4_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4_2">
            <span class="md-nav__icon md-icon"></span>
            trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/trainer/default_func/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    default_func
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/callbacks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    callbacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    export
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../engine/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../modules/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modules
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../parallel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parallel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../peft/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_7" id="__nav_5_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7">
            <span class="md-nav__icon md-icon"></span>
            PEFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_2" >
        
          
          <label class="md-nav__link" for="__nav_5_7_2" id="__nav_5_7_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    tuners
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_2">
            <span class="md-nav__icon md-icon"></span>
            tuners
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adalora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AdaLoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/adaption_prompt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adaption_Prompt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/ia3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IA3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lokr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoKr
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/lora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoRA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/tuners/prompt_tuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt tuning
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_7_3" >
        
          
          <label class="md-nav__link" for="__nav_5_7_3" id="__nav_5_7_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_7_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_7_3">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/utils/merge_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    merge_utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/mapping/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mapping
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/peft_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    peft_model
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sentence/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sentence
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Transformers
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9" id="__nav_5_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_9_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_2" >
        
          
          <label class="md-nav__link" for="__nav_5_9_2" id="__nav_5_9_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    generation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_2">
            <span class="md-nav__icon md-icon"></span>
            generation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../api/transforemrs/generation/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/beam_constraints/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_constraints
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/beam_search/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_search
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/logits_process/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    logits_process
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/stopping_criteria/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stopping_criteria
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/streamers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    streamers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../generation/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_3" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    models
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_9_3" id="__nav_5_9_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5_9_3">
            <span class="md-nav__icon md-icon"></span>
            models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../albert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    albert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../align/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    align
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../altclip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    altclip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../audio_spectrogram_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    audio_spectrogram_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../auto/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    auto
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../autoformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    autoformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../baichuan/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    baichuan
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bark/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bark
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../barthez/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    barthez
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bartpho/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bartpho
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../beit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bert_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bert_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bertweet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bertweet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bge_m3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bge_m3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../big_bird/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    big_bird
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bigbird_pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bigbird_pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../biogpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    biogpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blenderbot/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blenderbot_small/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blenderbot_small
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blip_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blip_2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bloom/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bloom
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bridgetower/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bridgetower
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../bros/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bros
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../byt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    byt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../camembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    camembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../canine/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    canine
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../chatglm3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    chatglm3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../codegen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    codegen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cogvlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cogvlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cohere/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cohere
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../convnext/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convnext
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpmant/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cpmant
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    cpmbee
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    cpmbee
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.configuration_cpmbee" class="md-nav__link">
    <span class="md-ellipsis">
      configuration_cpmbee
    </span>
  </a>
  
    <nav class="md-nav" aria-label="configuration_cpmbee">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee" class="md-nav__link">
    <span class="md-ellipsis">
      tokenization_cpmbee
    </span>
  </a>
  
    <nav class="md-nav" aria-label="tokenization_cpmbee">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeTokenizer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.bod_token_id" class="md-nav__link">
    <span class="md-ellipsis">
      bod_token_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.eod_token_id" class="md-nav__link">
    <span class="md-ellipsis">
      eod_token_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.newline_id" class="md-nav__link">
    <span class="md-ellipsis">
      newline_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.vocab_size" class="md-nav__link">
    <span class="md-ellipsis">
      vocab_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.__len__" class="md-nav__link">
    <span class="md-ellipsis">
      __len__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.check" class="md-nav__link">
    <span class="md-ellipsis">
      check
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.convert_data_to_id" class="md-nav__link">
    <span class="md-ellipsis">
      convert_data_to_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.convert_tokens_to_string" class="md-nav__link">
    <span class="md-ellipsis">
      convert_tokens_to_string
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.get_piece" class="md-nav__link">
    <span class="md-ellipsis">
      get_piece
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.get_vocab" class="md-nav__link">
    <span class="md-ellipsis">
      get_vocab
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.prepare_for_finetune" class="md-nav__link">
    <span class="md-ellipsis">
      prepare_for_finetune
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.prepare_for_model" class="md-nav__link">
    <span class="md-ellipsis">
      prepare_for_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.save_vocabulary" class="md-nav__link">
    <span class="md-ellipsis">
      save_vocabulary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.tokenize" class="md-nav__link">
    <span class="md-ellipsis">
      tokenize
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.rel_to_bucket" class="md-nav__link">
    <span class="md-ellipsis">
      rel_to_bucket
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee" class="md-nav__link">
    <span class="md-ellipsis">
      modeling_cpmbee
    </span>
  </a>
  
    <nav class="md-nav" aria-label="modeling_cpmbee">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeBeamHypotheses
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeBeamHypotheses">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses.add" class="md-nav__link">
    <span class="md-ellipsis">
      add
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeBeamSearchScorer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeBeamSearchScorer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer.apply_repetition_penalty" class="md-nav__link">
    <span class="md-ellipsis">
      apply_repetition_penalty
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer.finalize" class="md-nav__link">
    <span class="md-ellipsis">
      finalize
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer.process" class="md-nav__link">
    <span class="md-ellipsis">
      process
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeBucketPositionBias
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeBucketPositionBias">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeDenseGatedACT
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeDenseGatedACT">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeEmbeddingExt
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeEmbeddingExt">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt.projection" class="md-nav__link">
    <span class="md-ellipsis">
      projection
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeEncoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeFFNBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeFFNBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeFeedForward
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeFeedForward">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeForCausalLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeForCausalLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.adjust_logits_during_generation" class="md-nav__link">
    <span class="md-ellipsis">
      adjust_logits_during_generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.beam_search" class="md-nav__link">
    <span class="md-ellipsis">
      beam_search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.get_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.get_output_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_output_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.inference" class="md-nav__link">
    <span class="md-ellipsis">
      inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.prepare_inputs_for_generation" class="md-nav__link">
    <span class="md-ellipsis">
      prepare_inputs_for_generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.set_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.set_output_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_output_embeddings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeLayerNorm
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeLayerNorm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeLinear
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeLinear">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.get_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.inference" class="md-nav__link">
    <span class="md-ellipsis">
      inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.set_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_input_embeddings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeOutput
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeOutput">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeePreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeePreTrainedModel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeRotaryEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeRotaryEmbedding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeRotaryEmbedding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeRotaryEmbedding.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeRotaryEmbedding.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeSelfAttentionBlock" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeSelfAttentionBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeSelfAttentionBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeSelfAttentionBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeSelfAttentionBlock.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeTransformerBlock" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeTransformerBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeTransformerBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeTransformerBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeTransformerBlock.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ctrl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ctrl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cvt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cvt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../data2vec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    data2vec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deberta_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    deberta_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../decision_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    decision_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../distilbert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    distilbert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../efficientformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../efficientnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    efficientnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../electra/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    electra
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../encodec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    encodec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ernie/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ernie_m/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ernie_m
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../esm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    esm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../falcon/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    falcon
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../flava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    flava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../funnel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    funnel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gemma/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gemma
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../git/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    git
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_bigcode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_bigcode
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neox/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_neox_japanese/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_neox_japanese
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_pangu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gpt_pangu
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gptj/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    gptj
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../graphormer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    graphormer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../groupvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    groupvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hubert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    hubert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../imagegpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    imagegpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../internlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    internlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jetmoe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jetmoe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../layoutlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../layoutlmv2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    layoutlmv2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../led/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    led
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llama
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llava_next/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    llava_next
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../longformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../longt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    longt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../luke/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    luke
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mamba/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mamba
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../marian/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    marian
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../maskformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    maskformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mbart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mbart
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../megatron_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../megatron_gpt2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    megatron_gpt2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../minicpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minicpm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../minigpt4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    minigpt4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mistral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mistral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mixtral/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mixtral
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mobilebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mobilevit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mobilevit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../moss/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    moss
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mpnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mt5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mt5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../musicgen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../musicgen_melody/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    musicgen_melody
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mvp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mvp
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nezha/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nezha
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nystromformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nystromformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../olmo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    olmo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../openelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    openelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../opt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    opt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../owlvit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    owlvit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pegasus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pegasus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../phi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../phi3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phi3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../poolformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    poolformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pop2piano/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pop2piano
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../qwen2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../qwen2_moe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    qwen2_moe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    reformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../regnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    regnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rembert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rembert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../resnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    resnet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../roc_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    roc_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rwkv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    rwkv
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sam
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seamless_m4t/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seamless_m4t_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seamless_m4t_v2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../segformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    segformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seggpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    seggpt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../speech_encoder_decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_encoder_decoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../speech_to_text/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_to_text
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../squeezebert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    squeezebert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stablelm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    stablelm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../starcoder2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    starcoder2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../swiftformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    swiftformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../switch_transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    switch_transformers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../t5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    t5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../table_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    table_transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../timesformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    timesformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tinybert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tinybert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../van/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    van
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vipllava/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vipllava
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vision_text_dual_encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vision_text_dual_encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../visual_bert/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    visual_bert
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2_conformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_conformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wav2vec2_with_lm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wav2vec2_with_lm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wavlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wavlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../whisper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    whisper
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../x_clip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    x_clip
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm_roberta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlm_roberta_xl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlm_roberta_xl
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../xlnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    xlnet
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9_4" >
        
          
          <label class="md-nav__link" for="__nav_5_9_4" id="__nav_5_9_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_9_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9_4">
            <span class="md-nav__icon md-icon"></span>
            pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../api/transforemrs/pipeline/index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/automatic_speech_recognition/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    automatic_speech_recognition
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/document_question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    document_question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/fill_mask/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    fill_mask
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/question_answering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    question_answering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text2text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text2text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/text_generation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_generation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipeline/zero_shot_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    zero_shot_classification
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../configuration_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    configuration_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modeling_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    modeling_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils_base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization_utils_fast/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tokenization_utils_fast
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../trl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TRL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    说明
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            说明
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/changelog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    更新日志
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/code_of_conduct/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    行为准则
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../notes/faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    常见问题
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.configuration_cpmbee" class="md-nav__link">
    <span class="md-ellipsis">
      configuration_cpmbee
    </span>
  </a>
  
    <nav class="md-nav" aria-label="configuration_cpmbee">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee" class="md-nav__link">
    <span class="md-ellipsis">
      tokenization_cpmbee
    </span>
  </a>
  
    <nav class="md-nav" aria-label="tokenization_cpmbee">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeTokenizer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.bod_token_id" class="md-nav__link">
    <span class="md-ellipsis">
      bod_token_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.eod_token_id" class="md-nav__link">
    <span class="md-ellipsis">
      eod_token_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.newline_id" class="md-nav__link">
    <span class="md-ellipsis">
      newline_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.vocab_size" class="md-nav__link">
    <span class="md-ellipsis">
      vocab_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.__len__" class="md-nav__link">
    <span class="md-ellipsis">
      __len__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.check" class="md-nav__link">
    <span class="md-ellipsis">
      check
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.convert_data_to_id" class="md-nav__link">
    <span class="md-ellipsis">
      convert_data_to_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.convert_tokens_to_string" class="md-nav__link">
    <span class="md-ellipsis">
      convert_tokens_to_string
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.get_piece" class="md-nav__link">
    <span class="md-ellipsis">
      get_piece
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.get_vocab" class="md-nav__link">
    <span class="md-ellipsis">
      get_vocab
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.prepare_for_finetune" class="md-nav__link">
    <span class="md-ellipsis">
      prepare_for_finetune
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.prepare_for_model" class="md-nav__link">
    <span class="md-ellipsis">
      prepare_for_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.save_vocabulary" class="md-nav__link">
    <span class="md-ellipsis">
      save_vocabulary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.tokenize" class="md-nav__link">
    <span class="md-ellipsis">
      tokenize
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.rel_to_bucket" class="md-nav__link">
    <span class="md-ellipsis">
      rel_to_bucket
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee" class="md-nav__link">
    <span class="md-ellipsis">
      modeling_cpmbee
    </span>
  </a>
  
    <nav class="md-nav" aria-label="modeling_cpmbee">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeBeamHypotheses
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeBeamHypotheses">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses.add" class="md-nav__link">
    <span class="md-ellipsis">
      add
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeBeamSearchScorer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeBeamSearchScorer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer.apply_repetition_penalty" class="md-nav__link">
    <span class="md-ellipsis">
      apply_repetition_penalty
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer.finalize" class="md-nav__link">
    <span class="md-ellipsis">
      finalize
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer.process" class="md-nav__link">
    <span class="md-ellipsis">
      process
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeBucketPositionBias
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeBucketPositionBias">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeDenseGatedACT
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeDenseGatedACT">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeEmbeddingExt
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeEmbeddingExt">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt.projection" class="md-nav__link">
    <span class="md-ellipsis">
      projection
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeEncoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeFFNBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeFFNBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeFeedForward
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeFeedForward">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeForCausalLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeForCausalLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.adjust_logits_during_generation" class="md-nav__link">
    <span class="md-ellipsis">
      adjust_logits_during_generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.beam_search" class="md-nav__link">
    <span class="md-ellipsis">
      beam_search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.get_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.get_output_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_output_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.inference" class="md-nav__link">
    <span class="md-ellipsis">
      inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.prepare_inputs_for_generation" class="md-nav__link">
    <span class="md-ellipsis">
      prepare_inputs_for_generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.set_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.set_output_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_output_embeddings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeLayerNorm
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeLayerNorm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeLinear
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeLinear">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.get_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_input_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.inference" class="md-nav__link">
    <span class="md-ellipsis">
      inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.set_input_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      set_input_embeddings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeOutput
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeOutput">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeePreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeePreTrainedModel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeRotaryEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeRotaryEmbedding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeRotaryEmbedding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeRotaryEmbedding.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeRotaryEmbedding.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeSelfAttentionBlock" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeSelfAttentionBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeSelfAttentionBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeSelfAttentionBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeSelfAttentionBlock.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeTransformerBlock" class="md-nav__link">
    <span class="md-ellipsis">
      CpmBeeTransformerBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CpmBeeTransformerBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeTransformerBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeTransformerBlock.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/mindspore-lab/mindnlp/edit/master/docs/en/api/transformers/models/cpmbee.md" title="编辑此页" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindnlp/raw/master/docs/en/api/transformers/models/cpmbee.md" title="查看本页的源代码" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


  <h1>cpmbee</h1>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.cpmbee.configuration_cpmbee" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.configuration_cpmbee</code>


<a href="#mindnlp.transformers.models.cpmbee.configuration_cpmbee" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>CpmBee model configuration</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig</code>


<a href="#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.configuration_utils.PretrainedConfig" href="../../../../../api/transformers/configuration_utils/#mindnlp.transformers.configuration_utils.PretrainedConfig">PretrainedConfig</a></code></p>


        <p>This is the configuration class to store the configuration of a [<code>CpmBeeModel</code>]. It is used to instbeeiate an
CPMBee model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the CPMBee
<a href="https://hf-mirror.com/openbmb/cpm-bee-10b">openbmb/cpm-bee-10b</a> architecture.</p>
<p>Configuration objects inherit from [<code>PretrainedConfig</code>] and can be used to control the model outputs. Read the
documentation from [<code>PretrainedConfig</code>] for more information.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Vocabulary size of the CPMBee model. Defines the number of different tokens that can be represented by the
<code>input</code> passed when calling [<code>CpmBeeModel</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 30720</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>30720</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of the encoder layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 4096</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4096</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of attention heads in the Transformer encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 32</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>64</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dim_head</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of attention heads for each attention layer in the Transformer encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 128</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>64</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dim_ff</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimension of the "intermediate" (i.e., feed-forward) layer in the Transformer encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 10240</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>10240</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_hidden_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of layers of the Transformer encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 48</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>32</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dropout_p</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probabilitiy for all fully connected layers in the embeddings, encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_bias_num_buckets</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of position_bias buckets.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 512</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_bias_num_segment_buckets</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of segment buckets.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 32</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>32</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_bias_max_distance</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 2048</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2048</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eps</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The epsilon used by the layer normalization layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 1e-6</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1e-06</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>init_std</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Initialize parameters with std = init_std.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to use cache.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>distance_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Scale the rotary embedding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float` or `int`, *optional*, defaults to 16</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mask_modules</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Decides which feedforward block or attention block is pruned.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`list` or `tuple`, *optional*, defaults to None</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>half</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Decides the model parameters are half-precision or not.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">CpmBeeModel</span><span class="p">,</span> <span class="n">CpmBeeConfig</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initializing a CPMBee cpm-bee-10b style configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">configuration</span> <span class="o">=</span> <span class="n">CpmBeeConfig</span><span class="p">()</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initializing a model from the cpm-bee-10b style configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">CpmBeeModel</span><span class="p">(</span><span class="n">configuration</span><span class="p">)</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Accessing the model configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">configuration</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</code></pre></div>
</details>





              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\configuration_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeConfig</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the configuration class to store the configuration of a [`CpmBeeModel`]. It is used to instbeeiate an</span>
<span class="sd">    CPMBee model according to the specified arguments, defining the model architecture. Instantiating a configuration</span>
<span class="sd">    with the defaults will yield a similar configuration to that of the CPMBee</span>
<span class="sd">    [openbmb/cpm-bee-10b](https://hf-mirror.com/openbmb/cpm-bee-10b) architecture.</span>

<span class="sd">    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the</span>
<span class="sd">    documentation from [`PretrainedConfig`] for more information.</span>

<span class="sd">    Args:</span>
<span class="sd">        vocab_size (`int`, *optional*, defaults to 30720):</span>
<span class="sd">            Vocabulary size of the CPMBee model. Defines the number of different tokens that can be represented by the</span>
<span class="sd">            `input` passed when calling [`CpmBeeModel`].</span>
<span class="sd">        hidden_size (`int`, *optional*, defaults to 4096):</span>
<span class="sd">            Dimension of the encoder layers.</span>
<span class="sd">        num_attention_heads (`int`, *optional*, defaults to 32):</span>
<span class="sd">            Number of attention heads in the Transformer encoder.</span>
<span class="sd">        dim_head (`int`, *optional*, defaults to 128):</span>
<span class="sd">            Dimension of attention heads for each attention layer in the Transformer encoder.</span>
<span class="sd">        dim_ff (`int`, *optional*, defaults to 10240):</span>
<span class="sd">            Dimension of the &quot;intermediate&quot; (i.e., feed-forward) layer in the Transformer encoder.</span>
<span class="sd">        num_hidden_layers (`int`, *optional*, defaults to 48):</span>
<span class="sd">            Number of layers of the Transformer encoder.</span>
<span class="sd">        dropout_p (`float`, *optional*, defaults to 0.1):</span>
<span class="sd">            The dropout probabilitiy for all fully connected layers in the embeddings, encoder.</span>
<span class="sd">        position_bias_num_buckets (`int`, *optional*, defaults to 512):</span>
<span class="sd">            The number of position_bias buckets.</span>
<span class="sd">        position_bias_num_segment_buckets (`int`, *optional*, defaults to 32):</span>
<span class="sd">            The number of segment buckets.</span>
<span class="sd">        position_bias_max_distance (`int`, *optional*, defaults to 2048):</span>
<span class="sd">            The maximum sequence length that this model might ever be used with. Typically set this to something large</span>
<span class="sd">            just in case (e.g., 512 or 1024 or 2048).</span>
<span class="sd">        eps (`float`, *optional*, defaults to 1e-6):</span>
<span class="sd">            The epsilon used by the layer normalization layers.</span>
<span class="sd">        init_std (`float`, *optional*, defaults to 1.0):</span>
<span class="sd">            Initialize parameters with std = init_std.</span>
<span class="sd">        use_cache (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to use cache.</span>
<span class="sd">        distance_scale (`float` or `int`, *optional*, defaults to 16):</span>
<span class="sd">            Scale the rotary embedding.</span>
<span class="sd">        mask_modules (`list` or `tuple`, *optional*, defaults to None):</span>
<span class="sd">            Decides which feedforward block or attention block is pruned.</span>
<span class="sd">        half (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Decides the model parameters are half-precision or not.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; from transformers import CpmBeeModel, CpmBeeConfig</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Initializing a CPMBee cpm-bee-10b style configuration</span>
<span class="sd">        &gt;&gt;&gt; configuration = CpmBeeConfig()</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Initializing a model from the cpm-bee-10b style configuration</span>
<span class="sd">        &gt;&gt;&gt; model = CpmBeeModel(configuration)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # Accessing the model configuration</span>
<span class="sd">        &gt;&gt;&gt; configuration = model.config</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;cpmbee&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">30720</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
        <span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">dim_ff</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10240</span><span class="p">,</span>
        <span class="n">num_hidden_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">position_bias_num_buckets</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">position_bias_num_segment_buckets</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">position_bias_max_distance</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
        <span class="n">init_std</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">distance_scale</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
        <span class="n">mask_modules</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">half</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__</span>

<span class="sd">        Initializes a CpmBeeConfig instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            vocab_size (int): The size of the vocabulary. Defaults to 30720.</span>
<span class="sd">            hidden_size (int): The size of the hidden layers. Defaults to 4096.</span>
<span class="sd">            num_attention_heads (int): The number of attention heads. Defaults to 64.</span>
<span class="sd">            dim_head (int): The dimension of each attention head. Defaults to 64.</span>
<span class="sd">            dim_ff (int): The dimension of the feed forward network. Defaults to 10240.</span>
<span class="sd">            num_hidden_layers (int): The number of hidden layers. Defaults to 32.</span>
<span class="sd">            dropout_p (int): The dropout probability. Defaults to 0.0.</span>
<span class="sd">            position_bias_num_buckets (int): The number of buckets for position bias. Defaults to 256.</span>
<span class="sd">            position_bias_num_segment_buckets (int): The number of segment buckets for position bias. Defaults to 32.</span>
<span class="sd">            position_bias_max_distance (int): The maximum distance for position bias. Defaults to 2048.</span>
<span class="sd">            eps (int): A small value to avoid division by zero. Defaults to 1e-06.</span>
<span class="sd">            init_std (float): The standard deviation for weight initialization. Defaults to 1.0.</span>
<span class="sd">            use_cache (bool): Flag to indicate whether to use cache. Defaults to True.</span>
<span class="sd">            distance_scale (Union[int, float]): The scale factor for distance. Defaults to 16.</span>
<span class="sd">            mask_modules (Optional[Union[List, Tuple]]): List or Tuple of modules to be masked. Defaults to None.</span>
<span class="sd">            half (bool): Flag to indicate whether to use half precision. Defaults to False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_bias_num_segment_buckets</span> <span class="o">=</span> <span class="n">position_bias_num_segment_buckets</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim_ff</span> <span class="o">=</span> <span class="n">dim_ff</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">num_hidden_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_bias_num_buckets</span> <span class="o">=</span> <span class="n">position_bias_num_buckets</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_bias_max_distance</span> <span class="o">=</span> <span class="n">position_bias_max_distance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_p</span> <span class="o">=</span> <span class="n">dropout_p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_std</span> <span class="o">=</span> <span class="n">init_std</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distance_scale</span> <span class="o">=</span> <span class="n">distance_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">half</span> <span class="o">=</span> <span class="n">half</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask_modules</span> <span class="o">=</span> <span class="n">mask_modules</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">configuration_cpmbee</span><span class="o">.</span><span class="n">CpmBeeConfig</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">30720</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">dim_head</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">dim_ff</span><span class="o">=</span><span class="mi">10240</span><span class="p">,</span> <span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">position_bias_num_buckets</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">position_bias_num_segment_buckets</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">position_bias_max_distance</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">init_std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">distance_scale</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">mask_modules</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">half</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p><strong>init</strong></p>
<p>Initializes a CpmBeeConfig instance.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the vocabulary. Defaults to 30720.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>30720</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the hidden layers. Defaults to 4096.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4096</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads. Defaults to 64.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>64</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dim_head</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of each attention head. Defaults to 64.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>64</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dim_ff</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the feed forward network. Defaults to 10240.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>10240</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_hidden_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of hidden layers. Defaults to 32.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>32</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dropout_p</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability. Defaults to 0.0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_bias_num_buckets</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of buckets for position bias. Defaults to 256.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>256</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_bias_num_segment_buckets</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of segment buckets for position bias. Defaults to 32.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>32</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_bias_max_distance</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum distance for position bias. Defaults to 2048.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2048</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eps</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A small value to avoid division by zero. Defaults to 1e-06.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1e-06</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>init_std</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The standard deviation for weight initialization. Defaults to 1.0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Flag to indicate whether to use cache. Defaults to True.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>distance_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The scale factor for distance. Defaults to 16.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Union">Union</span>[int, float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>16</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mask_modules</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List or Tuple of modules to be masked. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="typing.List">List</span>, <span title="typing.Tuple">Tuple</span>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>half</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Flag to indicate whether to use half precision. Defaults to False.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\configuration_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">30720</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
    <span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">dim_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">dim_ff</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10240</span><span class="p">,</span>
    <span class="n">num_hidden_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">position_bias_num_buckets</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="n">position_bias_num_segment_buckets</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">position_bias_max_distance</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
    <span class="n">init_std</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">distance_scale</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">mask_modules</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">half</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    __init__</span>

<span class="sd">    Initializes a CpmBeeConfig instance.</span>

<span class="sd">    Args:</span>
<span class="sd">        vocab_size (int): The size of the vocabulary. Defaults to 30720.</span>
<span class="sd">        hidden_size (int): The size of the hidden layers. Defaults to 4096.</span>
<span class="sd">        num_attention_heads (int): The number of attention heads. Defaults to 64.</span>
<span class="sd">        dim_head (int): The dimension of each attention head. Defaults to 64.</span>
<span class="sd">        dim_ff (int): The dimension of the feed forward network. Defaults to 10240.</span>
<span class="sd">        num_hidden_layers (int): The number of hidden layers. Defaults to 32.</span>
<span class="sd">        dropout_p (int): The dropout probability. Defaults to 0.0.</span>
<span class="sd">        position_bias_num_buckets (int): The number of buckets for position bias. Defaults to 256.</span>
<span class="sd">        position_bias_num_segment_buckets (int): The number of segment buckets for position bias. Defaults to 32.</span>
<span class="sd">        position_bias_max_distance (int): The maximum distance for position bias. Defaults to 2048.</span>
<span class="sd">        eps (int): A small value to avoid division by zero. Defaults to 1e-06.</span>
<span class="sd">        init_std (float): The standard deviation for weight initialization. Defaults to 1.0.</span>
<span class="sd">        use_cache (bool): Flag to indicate whether to use cache. Defaults to True.</span>
<span class="sd">        distance_scale (Union[int, float]): The scale factor for distance. Defaults to 16.</span>
<span class="sd">        mask_modules (Optional[Union[List, Tuple]]): List or Tuple of modules to be masked. Defaults to None.</span>
<span class="sd">        half (bool): Flag to indicate whether to use half precision. Defaults to False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">position_bias_num_segment_buckets</span> <span class="o">=</span> <span class="n">position_bias_num_segment_buckets</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">dim_head</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dim_ff</span> <span class="o">=</span> <span class="n">dim_ff</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">num_hidden_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">position_bias_num_buckets</span> <span class="o">=</span> <span class="n">position_bias_num_buckets</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">position_bias_max_distance</span> <span class="o">=</span> <span class="n">position_bias_max_distance</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout_p</span> <span class="o">=</span> <span class="n">dropout_p</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">init_std</span> <span class="o">=</span> <span class="n">init_std</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">distance_scale</span> <span class="o">=</span> <span class="n">distance_scale</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">half</span> <span class="o">=</span> <span class="n">half</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mask_modules</span> <span class="o">=</span> <span class="n">mask_modules</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.tokenization_cpmbee</code>


<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>Tokenization classes for CpmBee.</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer</code>


<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.tokenization_utils.PreTrainedTokenizer" href="../../../../../api/transformers/tokenization_utils/#mindnlp.transformers.tokenization_utils.PreTrainedTokenizer">PreTrainedTokenizer</a></code></p>


        <p>Construct a CPMBee tokenizer.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vocab_file</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to the vocabulary file.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bos_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The beginning of sequence token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;s&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The end of sequence token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;/s&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;/s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>line_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The line token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;\n&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;\n&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>space_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The space token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34; &#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39; &#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unk_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The unknown token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;unk&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;unk&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mask_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The mask token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;mask&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;mask&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token used for padding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&lt;pad&gt;&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;pad&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_side</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The padding side. CPM-Bee will use left padding by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;left&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;left&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\tokenization_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 176</span>
<span class="normal"> 177</span>
<span class="normal"> 178</span>
<span class="normal"> 179</span>
<span class="normal"> 180</span>
<span class="normal"> 181</span>
<span class="normal"> 182</span>
<span class="normal"> 183</span>
<span class="normal"> 184</span>
<span class="normal"> 185</span>
<span class="normal"> 186</span>
<span class="normal"> 187</span>
<span class="normal"> 188</span>
<span class="normal"> 189</span>
<span class="normal"> 190</span>
<span class="normal"> 191</span>
<span class="normal"> 192</span>
<span class="normal"> 193</span>
<span class="normal"> 194</span>
<span class="normal"> 195</span>
<span class="normal"> 196</span>
<span class="normal"> 197</span>
<span class="normal"> 198</span>
<span class="normal"> 199</span>
<span class="normal"> 200</span>
<span class="normal"> 201</span>
<span class="normal"> 202</span>
<span class="normal"> 203</span>
<span class="normal"> 204</span>
<span class="normal"> 205</span>
<span class="normal"> 206</span>
<span class="normal"> 207</span>
<span class="normal"> 208</span>
<span class="normal"> 209</span>
<span class="normal"> 210</span>
<span class="normal"> 211</span>
<span class="normal"> 212</span>
<span class="normal"> 213</span>
<span class="normal"> 214</span>
<span class="normal"> 215</span>
<span class="normal"> 216</span>
<span class="normal"> 217</span>
<span class="normal"> 218</span>
<span class="normal"> 219</span>
<span class="normal"> 220</span>
<span class="normal"> 221</span>
<span class="normal"> 222</span>
<span class="normal"> 223</span>
<span class="normal"> 224</span>
<span class="normal"> 225</span>
<span class="normal"> 226</span>
<span class="normal"> 227</span>
<span class="normal"> 228</span>
<span class="normal"> 229</span>
<span class="normal"> 230</span>
<span class="normal"> 231</span>
<span class="normal"> 232</span>
<span class="normal"> 233</span>
<span class="normal"> 234</span>
<span class="normal"> 235</span>
<span class="normal"> 236</span>
<span class="normal"> 237</span>
<span class="normal"> 238</span>
<span class="normal"> 239</span>
<span class="normal"> 240</span>
<span class="normal"> 241</span>
<span class="normal"> 242</span>
<span class="normal"> 243</span>
<span class="normal"> 244</span>
<span class="normal"> 245</span>
<span class="normal"> 246</span>
<span class="normal"> 247</span>
<span class="normal"> 248</span>
<span class="normal"> 249</span>
<span class="normal"> 250</span>
<span class="normal"> 251</span>
<span class="normal"> 252</span>
<span class="normal"> 253</span>
<span class="normal"> 254</span>
<span class="normal"> 255</span>
<span class="normal"> 256</span>
<span class="normal"> 257</span>
<span class="normal"> 258</span>
<span class="normal"> 259</span>
<span class="normal"> 260</span>
<span class="normal"> 261</span>
<span class="normal"> 262</span>
<span class="normal"> 263</span>
<span class="normal"> 264</span>
<span class="normal"> 265</span>
<span class="normal"> 266</span>
<span class="normal"> 267</span>
<span class="normal"> 268</span>
<span class="normal"> 269</span>
<span class="normal"> 270</span>
<span class="normal"> 271</span>
<span class="normal"> 272</span>
<span class="normal"> 273</span>
<span class="normal"> 274</span>
<span class="normal"> 275</span>
<span class="normal"> 276</span>
<span class="normal"> 277</span>
<span class="normal"> 278</span>
<span class="normal"> 279</span>
<span class="normal"> 280</span>
<span class="normal"> 281</span>
<span class="normal"> 282</span>
<span class="normal"> 283</span>
<span class="normal"> 284</span>
<span class="normal"> 285</span>
<span class="normal"> 286</span>
<span class="normal"> 287</span>
<span class="normal"> 288</span>
<span class="normal"> 289</span>
<span class="normal"> 290</span>
<span class="normal"> 291</span>
<span class="normal"> 292</span>
<span class="normal"> 293</span>
<span class="normal"> 294</span>
<span class="normal"> 295</span>
<span class="normal"> 296</span>
<span class="normal"> 297</span>
<span class="normal"> 298</span>
<span class="normal"> 299</span>
<span class="normal"> 300</span>
<span class="normal"> 301</span>
<span class="normal"> 302</span>
<span class="normal"> 303</span>
<span class="normal"> 304</span>
<span class="normal"> 305</span>
<span class="normal"> 306</span>
<span class="normal"> 307</span>
<span class="normal"> 308</span>
<span class="normal"> 309</span>
<span class="normal"> 310</span>
<span class="normal"> 311</span>
<span class="normal"> 312</span>
<span class="normal"> 313</span>
<span class="normal"> 314</span>
<span class="normal"> 315</span>
<span class="normal"> 316</span>
<span class="normal"> 317</span>
<span class="normal"> 318</span>
<span class="normal"> 319</span>
<span class="normal"> 320</span>
<span class="normal"> 321</span>
<span class="normal"> 322</span>
<span class="normal"> 323</span>
<span class="normal"> 324</span>
<span class="normal"> 325</span>
<span class="normal"> 326</span>
<span class="normal"> 327</span>
<span class="normal"> 328</span>
<span class="normal"> 329</span>
<span class="normal"> 330</span>
<span class="normal"> 331</span>
<span class="normal"> 332</span>
<span class="normal"> 333</span>
<span class="normal"> 334</span>
<span class="normal"> 335</span>
<span class="normal"> 336</span>
<span class="normal"> 337</span>
<span class="normal"> 338</span>
<span class="normal"> 339</span>
<span class="normal"> 340</span>
<span class="normal"> 341</span>
<span class="normal"> 342</span>
<span class="normal"> 343</span>
<span class="normal"> 344</span>
<span class="normal"> 345</span>
<span class="normal"> 346</span>
<span class="normal"> 347</span>
<span class="normal"> 348</span>
<span class="normal"> 349</span>
<span class="normal"> 350</span>
<span class="normal"> 351</span>
<span class="normal"> 352</span>
<span class="normal"> 353</span>
<span class="normal"> 354</span>
<span class="normal"> 355</span>
<span class="normal"> 356</span>
<span class="normal"> 357</span>
<span class="normal"> 358</span>
<span class="normal"> 359</span>
<span class="normal"> 360</span>
<span class="normal"> 361</span>
<span class="normal"> 362</span>
<span class="normal"> 363</span>
<span class="normal"> 364</span>
<span class="normal"> 365</span>
<span class="normal"> 366</span>
<span class="normal"> 367</span>
<span class="normal"> 368</span>
<span class="normal"> 369</span>
<span class="normal"> 370</span>
<span class="normal"> 371</span>
<span class="normal"> 372</span>
<span class="normal"> 373</span>
<span class="normal"> 374</span>
<span class="normal"> 375</span>
<span class="normal"> 376</span>
<span class="normal"> 377</span>
<span class="normal"> 378</span>
<span class="normal"> 379</span>
<span class="normal"> 380</span>
<span class="normal"> 381</span>
<span class="normal"> 382</span>
<span class="normal"> 383</span>
<span class="normal"> 384</span>
<span class="normal"> 385</span>
<span class="normal"> 386</span>
<span class="normal"> 387</span>
<span class="normal"> 388</span>
<span class="normal"> 389</span>
<span class="normal"> 390</span>
<span class="normal"> 391</span>
<span class="normal"> 392</span>
<span class="normal"> 393</span>
<span class="normal"> 394</span>
<span class="normal"> 395</span>
<span class="normal"> 396</span>
<span class="normal"> 397</span>
<span class="normal"> 398</span>
<span class="normal"> 399</span>
<span class="normal"> 400</span>
<span class="normal"> 401</span>
<span class="normal"> 402</span>
<span class="normal"> 403</span>
<span class="normal"> 404</span>
<span class="normal"> 405</span>
<span class="normal"> 406</span>
<span class="normal"> 407</span>
<span class="normal"> 408</span>
<span class="normal"> 409</span>
<span class="normal"> 410</span>
<span class="normal"> 411</span>
<span class="normal"> 412</span>
<span class="normal"> 413</span>
<span class="normal"> 414</span>
<span class="normal"> 415</span>
<span class="normal"> 416</span>
<span class="normal"> 417</span>
<span class="normal"> 418</span>
<span class="normal"> 419</span>
<span class="normal"> 420</span>
<span class="normal"> 421</span>
<span class="normal"> 422</span>
<span class="normal"> 423</span>
<span class="normal"> 424</span>
<span class="normal"> 425</span>
<span class="normal"> 426</span>
<span class="normal"> 427</span>
<span class="normal"> 428</span>
<span class="normal"> 429</span>
<span class="normal"> 430</span>
<span class="normal"> 431</span>
<span class="normal"> 432</span>
<span class="normal"> 433</span>
<span class="normal"> 434</span>
<span class="normal"> 435</span>
<span class="normal"> 436</span>
<span class="normal"> 437</span>
<span class="normal"> 438</span>
<span class="normal"> 439</span>
<span class="normal"> 440</span>
<span class="normal"> 441</span>
<span class="normal"> 442</span>
<span class="normal"> 443</span>
<span class="normal"> 444</span>
<span class="normal"> 445</span>
<span class="normal"> 446</span>
<span class="normal"> 447</span>
<span class="normal"> 448</span>
<span class="normal"> 449</span>
<span class="normal"> 450</span>
<span class="normal"> 451</span>
<span class="normal"> 452</span>
<span class="normal"> 453</span>
<span class="normal"> 454</span>
<span class="normal"> 455</span>
<span class="normal"> 456</span>
<span class="normal"> 457</span>
<span class="normal"> 458</span>
<span class="normal"> 459</span>
<span class="normal"> 460</span>
<span class="normal"> 461</span>
<span class="normal"> 462</span>
<span class="normal"> 463</span>
<span class="normal"> 464</span>
<span class="normal"> 465</span>
<span class="normal"> 466</span>
<span class="normal"> 467</span>
<span class="normal"> 468</span>
<span class="normal"> 469</span>
<span class="normal"> 470</span>
<span class="normal"> 471</span>
<span class="normal"> 472</span>
<span class="normal"> 473</span>
<span class="normal"> 474</span>
<span class="normal"> 475</span>
<span class="normal"> 476</span>
<span class="normal"> 477</span>
<span class="normal"> 478</span>
<span class="normal"> 479</span>
<span class="normal"> 480</span>
<span class="normal"> 481</span>
<span class="normal"> 482</span>
<span class="normal"> 483</span>
<span class="normal"> 484</span>
<span class="normal"> 485</span>
<span class="normal"> 486</span>
<span class="normal"> 487</span>
<span class="normal"> 488</span>
<span class="normal"> 489</span>
<span class="normal"> 490</span>
<span class="normal"> 491</span>
<span class="normal"> 492</span>
<span class="normal"> 493</span>
<span class="normal"> 494</span>
<span class="normal"> 495</span>
<span class="normal"> 496</span>
<span class="normal"> 497</span>
<span class="normal"> 498</span>
<span class="normal"> 499</span>
<span class="normal"> 500</span>
<span class="normal"> 501</span>
<span class="normal"> 502</span>
<span class="normal"> 503</span>
<span class="normal"> 504</span>
<span class="normal"> 505</span>
<span class="normal"> 506</span>
<span class="normal"> 507</span>
<span class="normal"> 508</span>
<span class="normal"> 509</span>
<span class="normal"> 510</span>
<span class="normal"> 511</span>
<span class="normal"> 512</span>
<span class="normal"> 513</span>
<span class="normal"> 514</span>
<span class="normal"> 515</span>
<span class="normal"> 516</span>
<span class="normal"> 517</span>
<span class="normal"> 518</span>
<span class="normal"> 519</span>
<span class="normal"> 520</span>
<span class="normal"> 521</span>
<span class="normal"> 522</span>
<span class="normal"> 523</span>
<span class="normal"> 524</span>
<span class="normal"> 525</span>
<span class="normal"> 526</span>
<span class="normal"> 527</span>
<span class="normal"> 528</span>
<span class="normal"> 529</span>
<span class="normal"> 530</span>
<span class="normal"> 531</span>
<span class="normal"> 532</span>
<span class="normal"> 533</span>
<span class="normal"> 534</span>
<span class="normal"> 535</span>
<span class="normal"> 536</span>
<span class="normal"> 537</span>
<span class="normal"> 538</span>
<span class="normal"> 539</span>
<span class="normal"> 540</span>
<span class="normal"> 541</span>
<span class="normal"> 542</span>
<span class="normal"> 543</span>
<span class="normal"> 544</span>
<span class="normal"> 545</span>
<span class="normal"> 546</span>
<span class="normal"> 547</span>
<span class="normal"> 548</span>
<span class="normal"> 549</span>
<span class="normal"> 550</span>
<span class="normal"> 551</span>
<span class="normal"> 552</span>
<span class="normal"> 553</span>
<span class="normal"> 554</span>
<span class="normal"> 555</span>
<span class="normal"> 556</span>
<span class="normal"> 557</span>
<span class="normal"> 558</span>
<span class="normal"> 559</span>
<span class="normal"> 560</span>
<span class="normal"> 561</span>
<span class="normal"> 562</span>
<span class="normal"> 563</span>
<span class="normal"> 564</span>
<span class="normal"> 565</span>
<span class="normal"> 566</span>
<span class="normal"> 567</span>
<span class="normal"> 568</span>
<span class="normal"> 569</span>
<span class="normal"> 570</span>
<span class="normal"> 571</span>
<span class="normal"> 572</span>
<span class="normal"> 573</span>
<span class="normal"> 574</span>
<span class="normal"> 575</span>
<span class="normal"> 576</span>
<span class="normal"> 577</span>
<span class="normal"> 578</span>
<span class="normal"> 579</span>
<span class="normal"> 580</span>
<span class="normal"> 581</span>
<span class="normal"> 582</span>
<span class="normal"> 583</span>
<span class="normal"> 584</span>
<span class="normal"> 585</span>
<span class="normal"> 586</span>
<span class="normal"> 587</span>
<span class="normal"> 588</span>
<span class="normal"> 589</span>
<span class="normal"> 590</span>
<span class="normal"> 591</span>
<span class="normal"> 592</span>
<span class="normal"> 593</span>
<span class="normal"> 594</span>
<span class="normal"> 595</span>
<span class="normal"> 596</span>
<span class="normal"> 597</span>
<span class="normal"> 598</span>
<span class="normal"> 599</span>
<span class="normal"> 600</span>
<span class="normal"> 601</span>
<span class="normal"> 602</span>
<span class="normal"> 603</span>
<span class="normal"> 604</span>
<span class="normal"> 605</span>
<span class="normal"> 606</span>
<span class="normal"> 607</span>
<span class="normal"> 608</span>
<span class="normal"> 609</span>
<span class="normal"> 610</span>
<span class="normal"> 611</span>
<span class="normal"> 612</span>
<span class="normal"> 613</span>
<span class="normal"> 614</span>
<span class="normal"> 615</span>
<span class="normal"> 616</span>
<span class="normal"> 617</span>
<span class="normal"> 618</span>
<span class="normal"> 619</span>
<span class="normal"> 620</span>
<span class="normal"> 621</span>
<span class="normal"> 622</span>
<span class="normal"> 623</span>
<span class="normal"> 624</span>
<span class="normal"> 625</span>
<span class="normal"> 626</span>
<span class="normal"> 627</span>
<span class="normal"> 628</span>
<span class="normal"> 629</span>
<span class="normal"> 630</span>
<span class="normal"> 631</span>
<span class="normal"> 632</span>
<span class="normal"> 633</span>
<span class="normal"> 634</span>
<span class="normal"> 635</span>
<span class="normal"> 636</span>
<span class="normal"> 637</span>
<span class="normal"> 638</span>
<span class="normal"> 639</span>
<span class="normal"> 640</span>
<span class="normal"> 641</span>
<span class="normal"> 642</span>
<span class="normal"> 643</span>
<span class="normal"> 644</span>
<span class="normal"> 645</span>
<span class="normal"> 646</span>
<span class="normal"> 647</span>
<span class="normal"> 648</span>
<span class="normal"> 649</span>
<span class="normal"> 650</span>
<span class="normal"> 651</span>
<span class="normal"> 652</span>
<span class="normal"> 653</span>
<span class="normal"> 654</span>
<span class="normal"> 655</span>
<span class="normal"> 656</span>
<span class="normal"> 657</span>
<span class="normal"> 658</span>
<span class="normal"> 659</span>
<span class="normal"> 660</span>
<span class="normal"> 661</span>
<span class="normal"> 662</span>
<span class="normal"> 663</span>
<span class="normal"> 664</span>
<span class="normal"> 665</span>
<span class="normal"> 666</span>
<span class="normal"> 667</span>
<span class="normal"> 668</span>
<span class="normal"> 669</span>
<span class="normal"> 670</span>
<span class="normal"> 671</span>
<span class="normal"> 672</span>
<span class="normal"> 673</span>
<span class="normal"> 674</span>
<span class="normal"> 675</span>
<span class="normal"> 676</span>
<span class="normal"> 677</span>
<span class="normal"> 678</span>
<span class="normal"> 679</span>
<span class="normal"> 680</span>
<span class="normal"> 681</span>
<span class="normal"> 682</span>
<span class="normal"> 683</span>
<span class="normal"> 684</span>
<span class="normal"> 685</span>
<span class="normal"> 686</span>
<span class="normal"> 687</span>
<span class="normal"> 688</span>
<span class="normal"> 689</span>
<span class="normal"> 690</span>
<span class="normal"> 691</span>
<span class="normal"> 692</span>
<span class="normal"> 693</span>
<span class="normal"> 694</span>
<span class="normal"> 695</span>
<span class="normal"> 696</span>
<span class="normal"> 697</span>
<span class="normal"> 698</span>
<span class="normal"> 699</span>
<span class="normal"> 700</span>
<span class="normal"> 701</span>
<span class="normal"> 702</span>
<span class="normal"> 703</span>
<span class="normal"> 704</span>
<span class="normal"> 705</span>
<span class="normal"> 706</span>
<span class="normal"> 707</span>
<span class="normal"> 708</span>
<span class="normal"> 709</span>
<span class="normal"> 710</span>
<span class="normal"> 711</span>
<span class="normal"> 712</span>
<span class="normal"> 713</span>
<span class="normal"> 714</span>
<span class="normal"> 715</span>
<span class="normal"> 716</span>
<span class="normal"> 717</span>
<span class="normal"> 718</span>
<span class="normal"> 719</span>
<span class="normal"> 720</span>
<span class="normal"> 721</span>
<span class="normal"> 722</span>
<span class="normal"> 723</span>
<span class="normal"> 724</span>
<span class="normal"> 725</span>
<span class="normal"> 726</span>
<span class="normal"> 727</span>
<span class="normal"> 728</span>
<span class="normal"> 729</span>
<span class="normal"> 730</span>
<span class="normal"> 731</span>
<span class="normal"> 732</span>
<span class="normal"> 733</span>
<span class="normal"> 734</span>
<span class="normal"> 735</span>
<span class="normal"> 736</span>
<span class="normal"> 737</span>
<span class="normal"> 738</span>
<span class="normal"> 739</span>
<span class="normal"> 740</span>
<span class="normal"> 741</span>
<span class="normal"> 742</span>
<span class="normal"> 743</span>
<span class="normal"> 744</span>
<span class="normal"> 745</span>
<span class="normal"> 746</span>
<span class="normal"> 747</span>
<span class="normal"> 748</span>
<span class="normal"> 749</span>
<span class="normal"> 750</span>
<span class="normal"> 751</span>
<span class="normal"> 752</span>
<span class="normal"> 753</span>
<span class="normal"> 754</span>
<span class="normal"> 755</span>
<span class="normal"> 756</span>
<span class="normal"> 757</span>
<span class="normal"> 758</span>
<span class="normal"> 759</span>
<span class="normal"> 760</span>
<span class="normal"> 761</span>
<span class="normal"> 762</span>
<span class="normal"> 763</span>
<span class="normal"> 764</span>
<span class="normal"> 765</span>
<span class="normal"> 766</span>
<span class="normal"> 767</span>
<span class="normal"> 768</span>
<span class="normal"> 769</span>
<span class="normal"> 770</span>
<span class="normal"> 771</span>
<span class="normal"> 772</span>
<span class="normal"> 773</span>
<span class="normal"> 774</span>
<span class="normal"> 775</span>
<span class="normal"> 776</span>
<span class="normal"> 777</span>
<span class="normal"> 778</span>
<span class="normal"> 779</span>
<span class="normal"> 780</span>
<span class="normal"> 781</span>
<span class="normal"> 782</span>
<span class="normal"> 783</span>
<span class="normal"> 784</span>
<span class="normal"> 785</span>
<span class="normal"> 786</span>
<span class="normal"> 787</span>
<span class="normal"> 788</span>
<span class="normal"> 789</span>
<span class="normal"> 790</span>
<span class="normal"> 791</span>
<span class="normal"> 792</span>
<span class="normal"> 793</span>
<span class="normal"> 794</span>
<span class="normal"> 795</span>
<span class="normal"> 796</span>
<span class="normal"> 797</span>
<span class="normal"> 798</span>
<span class="normal"> 799</span>
<span class="normal"> 800</span>
<span class="normal"> 801</span>
<span class="normal"> 802</span>
<span class="normal"> 803</span>
<span class="normal"> 804</span>
<span class="normal"> 805</span>
<span class="normal"> 806</span>
<span class="normal"> 807</span>
<span class="normal"> 808</span>
<span class="normal"> 809</span>
<span class="normal"> 810</span>
<span class="normal"> 811</span>
<span class="normal"> 812</span>
<span class="normal"> 813</span>
<span class="normal"> 814</span>
<span class="normal"> 815</span>
<span class="normal"> 816</span>
<span class="normal"> 817</span>
<span class="normal"> 818</span>
<span class="normal"> 819</span>
<span class="normal"> 820</span>
<span class="normal"> 821</span>
<span class="normal"> 822</span>
<span class="normal"> 823</span>
<span class="normal"> 824</span>
<span class="normal"> 825</span>
<span class="normal"> 826</span>
<span class="normal"> 827</span>
<span class="normal"> 828</span>
<span class="normal"> 829</span>
<span class="normal"> 830</span>
<span class="normal"> 831</span>
<span class="normal"> 832</span>
<span class="normal"> 833</span>
<span class="normal"> 834</span>
<span class="normal"> 835</span>
<span class="normal"> 836</span>
<span class="normal"> 837</span>
<span class="normal"> 838</span>
<span class="normal"> 839</span>
<span class="normal"> 840</span>
<span class="normal"> 841</span>
<span class="normal"> 842</span>
<span class="normal"> 843</span>
<span class="normal"> 844</span>
<span class="normal"> 845</span>
<span class="normal"> 846</span>
<span class="normal"> 847</span>
<span class="normal"> 848</span>
<span class="normal"> 849</span>
<span class="normal"> 850</span>
<span class="normal"> 851</span>
<span class="normal"> 852</span>
<span class="normal"> 853</span>
<span class="normal"> 854</span>
<span class="normal"> 855</span>
<span class="normal"> 856</span>
<span class="normal"> 857</span>
<span class="normal"> 858</span>
<span class="normal"> 859</span>
<span class="normal"> 860</span>
<span class="normal"> 861</span>
<span class="normal"> 862</span>
<span class="normal"> 863</span>
<span class="normal"> 864</span>
<span class="normal"> 865</span>
<span class="normal"> 866</span>
<span class="normal"> 867</span>
<span class="normal"> 868</span>
<span class="normal"> 869</span>
<span class="normal"> 870</span>
<span class="normal"> 871</span>
<span class="normal"> 872</span>
<span class="normal"> 873</span>
<span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeTokenizer</span><span class="p">(</span><span class="n">PreTrainedTokenizer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct a CPMBee tokenizer.</span>

<span class="sd">    Args:</span>
<span class="sd">        vocab_file (`str`):</span>
<span class="sd">            Path to the vocabulary file.</span>
<span class="sd">        bos_token (`str`, *optional*, defaults to `&quot;&lt;s&gt;&quot;`):</span>
<span class="sd">            The beginning of sequence token.</span>
<span class="sd">        eos_token (`str`, *optional*, defaults to `&quot;&lt;/s&gt;&quot;`):</span>
<span class="sd">            The end of sequence token.</span>
<span class="sd">        line_token (`str`, *optional*, defaults to `&quot;\n&quot;`):</span>
<span class="sd">            The line token.</span>
<span class="sd">        space_token (`str`, *optional*, defaults to `&quot; &quot;`):</span>
<span class="sd">            The space token.</span>
<span class="sd">        unk_token (`str`, *optional*, defaults to `&quot;&lt;unk&gt;&quot;`):</span>
<span class="sd">            The unknown token.</span>
<span class="sd">        mask_token (`str`, *optional*, defaults to `&quot;&lt;mask&gt;&quot;`):</span>
<span class="sd">            The mask token.</span>
<span class="sd">        pad_token (`str`, *optional*, defaults to `&quot;&lt;pad&gt;&quot;`):</span>
<span class="sd">            The token used for padding.</span>
<span class="sd">        padding_side (`str`, *optional*, defaults to `&quot;left&quot;`):</span>
<span class="sd">            The padding side. CPM-Bee will use left padding by default.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">vocab_files_names</span> <span class="o">=</span> <span class="n">VOCAB_FILES_NAMES</span>
    <span class="n">pretrained_vocab_files_map</span> <span class="o">=</span> <span class="n">PRETRAINED_VOCAB_FILES_MAP</span>
    <span class="n">max_model_input_sizes</span> <span class="o">=</span> <span class="n">PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES</span>
    <span class="n">model_input_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;input_ids&quot;</span><span class="p">,</span>
        <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span>
        <span class="s2">&quot;input_id_sub&quot;</span><span class="p">,</span>
        <span class="s2">&quot;position&quot;</span><span class="p">,</span>
        <span class="s2">&quot;context&quot;</span><span class="p">,</span>
        <span class="s2">&quot;sample_ids&quot;</span><span class="p">,</span>
        <span class="s2">&quot;num_segments&quot;</span><span class="p">,</span>
        <span class="s2">&quot;segment&quot;</span><span class="p">,</span>
        <span class="s2">&quot;segment_rel_offset&quot;</span><span class="p">,</span>
        <span class="s2">&quot;segment_rel&quot;</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="n">add_prefix_space</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_file</span><span class="p">,</span>
        <span class="n">bos_token</span><span class="o">=</span><span class="s2">&quot;&lt;s&gt;&quot;</span><span class="p">,</span>
        <span class="n">eos_token</span><span class="o">=</span><span class="s2">&quot;&lt;/s&gt;&quot;</span><span class="p">,</span>
        <span class="n">line_token</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">space_token</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">,</span>
        <span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">,</span>
        <span class="n">mask_token</span><span class="o">=</span><span class="s2">&quot;&lt;mask&gt;&quot;</span><span class="p">,</span>
        <span class="n">pad_token</span><span class="o">=</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">,</span>
        <span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize a CpmBeeTokenizer object.</span>

<span class="sd">        Args:</span>
<span class="sd">            vocab_file (str): The path to the file containing the vocabulary.</span>
<span class="sd">            bos_token (str, optional): The beginning of sentence token.</span>
<span class="sd">            eos_token (str, optional): The end of sentence token.</span>
<span class="sd">            line_token (str, optional): The token used to represent a new line.</span>
<span class="sd">            space_token (str, optional): The token used to represent a space.</span>
<span class="sd">            unk_token (str, optional): The token used to represent unknown words.</span>
<span class="sd">            mask_token (str, optional): The token used for masking.</span>
<span class="sd">            pad_token (str, optional): The token used for padding.</span>
<span class="sd">            padding_side (str, optional): The side to apply padding.</span>
<span class="sd">            **kwargs: Additional keyword arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            FileNotFoundError: If the vocab_file does not exist.</span>
<span class="sd">            TypeError: If any of the arguments are of incorrect type.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">bos_token</span><span class="o">=</span><span class="n">bos_token</span><span class="p">,</span>
            <span class="n">eos_token</span><span class="o">=</span><span class="n">eos_token</span><span class="p">,</span>
            <span class="n">line_token</span><span class="o">=</span><span class="n">line_token</span><span class="p">,</span>
            <span class="n">space_token</span><span class="o">=</span><span class="n">space_token</span><span class="p">,</span>
            <span class="n">unk_token</span><span class="o">=</span><span class="n">unk_token</span><span class="p">,</span>
            <span class="n">mask_token</span><span class="o">=</span><span class="n">mask_token</span><span class="p">,</span>
            <span class="n">pad_token</span><span class="o">=</span><span class="n">pad_token</span><span class="p">,</span>
            <span class="n">padding_side</span><span class="o">=</span><span class="n">padding_side</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">reader</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
                <span class="n">token</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot; &quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;&lt;/_&gt;&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;&lt;/n&gt;&quot;</span><span class="p">]</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;&lt;/_&gt;&quot;</span><span class="p">]</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;&lt;/n&gt;&quot;</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_max_word_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee_special_tokens</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;&lt;&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">k</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;&gt;&quot;</span><span class="p">)}</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ext_special_tokens</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ext_args_for_model</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">&quot;input_id_subs&quot;</span><span class="p">,</span>
            <span class="s2">&quot;input_pos&quot;</span><span class="p">,</span>
            <span class="s2">&quot;context&quot;</span><span class="p">,</span>
            <span class="s2">&quot;segment_ids&quot;</span><span class="p">,</span>
            <span class="s2">&quot;segment_rel_offset&quot;</span><span class="p">,</span>
            <span class="s2">&quot;segment_rel&quot;</span><span class="p">,</span>
            <span class="s2">&quot;sample_ids&quot;</span><span class="p">,</span>
            <span class="s2">&quot;num_segments&quot;</span><span class="p">,</span>
            <span class="s2">&quot;predict_segments&quot;</span><span class="p">,</span>
            <span class="s2">&quot;answer_placeholders&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ext_table&quot;</span><span class="p">,</span>
            <span class="s2">&quot;token_id_table&quot;</span><span class="p">,</span>
        <span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">bod_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the token ID for the beginning of document (BOD) token.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: An instance of the CpmBeeTokenizer class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: This method returns the token ID corresponding to the BOD token in the encoder dictionary.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">bod_token</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">eod_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method to retrieve the token ID corresponding to the end-of-document token in the CpmBeeTokenizer class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: An instance of the CpmBeeTokenizer class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: The method returns the token ID of the end-of-document token in the tokenizer&#39;s encoder.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eod_token</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">newline_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the ID of the newline token in the CpmBeeTokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeTokenizer): An instance of the CpmBeeTokenizer class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">line_token</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">vocab_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the size of the vocabulary used by the CpmBeeTokenizer instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            self:</span>
<span class="sd">                The CpmBeeTokenizer instance.</span>

<span class="sd">                - This parameter is of type &#39;CpmBeeTokenizer&#39;.</span>
<span class="sd">                - It represents the instance of the CpmBeeTokenizer class on which the method is called.</span>

<span class="sd">        Returns:</span>
<span class="sd">            int:</span>
<span class="sd">                An integer representing the size of the vocabulary.</span>

<span class="sd">                - The returned value represents the total number of unique tokens in the vocabulary.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>

<span class="sd">        Example:</span>
<span class="sd">            ```python</span>
<span class="sd">            &gt;&gt;&gt; tokenizer = CpmBeeTokenizer()</span>
<span class="sd">            &gt;&gt;&gt; tokenizer.vocab_size()</span>
<span class="sd">            5000</span>
<span class="sd">            ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Size of the full vocabulary with the added tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the vocabulary of the CpmBeeTokenizer instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeTokenizer): The instance of the CpmBeeTokenizer class.</span>
<span class="sd">                This parameter represents the current instance of the tokenizer.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: A dictionary containing the combined encoder and added tokens encoder.</span>
<span class="sd">                The keys represent tokens, and the values represent their corresponding IDs.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_piece</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Match with maximum length.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">len_text</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)):</span>
            <span class="n">sub</span> <span class="o">=</span> <span class="n">text</span><span class="p">[:</span> <span class="n">len_text</span> <span class="o">-</span> <span class="n">i</span><span class="p">]</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">sub</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">sub</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">sub</span>
        <span class="k">return</span> <span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="n">TextInput</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Override the `tokenize` to meet the needs of CPMBee:</span>

<span class="sd">        1. Mark the special token with `&lt;` and `&gt;`. The `&lt;&gt;` will be ignored.</span>
<span class="sd">        2. Split sentences by the marked special tokens.</span>
<span class="sd">        3. Record the marked special token by `ext_table` and `ext_table_rev`.</span>
<span class="sd">        4. Tokenize the sentence without special tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">for_cpmbee</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;for_cpmbee&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">all_special_tokens_extended</span> <span class="o">=</span> <span class="p">{</span>
            <span class="nb">str</span><span class="p">(</span><span class="n">t</span><span class="p">):</span> <span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_tokens_extended</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">)</span>
        <span class="p">}</span>

        <span class="n">sentence_split</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">]</span>
        <span class="n">is_special_token</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">is_special_token</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="s2">&quot;&lt;&quot;</span><span class="p">:</span>
                    <span class="n">tail</span> <span class="o">=</span> <span class="n">sentence_split</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">sentence_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">tail</span>
                    <span class="n">sentence_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
                    <span class="n">is_special_token</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">elif</span> <span class="n">c</span> <span class="o">==</span> <span class="s2">&quot;&gt;&quot;</span><span class="p">:</span>
                    <span class="c1"># end of special token</span>
                    <span class="n">sentence_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">c</span>
                    <span class="k">if</span> <span class="n">sentence_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;&lt;&gt;&quot;</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">is_special_token</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="n">sentence_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">sentence_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">c</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="s2">&quot;&lt;&quot;</span><span class="p">:</span>
                    <span class="n">is_special_token</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">sentence_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">sentence_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">c</span>
        <span class="k">if</span> <span class="n">is_special_token</span><span class="p">:</span>
            <span class="n">tail</span> <span class="o">=</span> <span class="n">sentence_split</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">sentence_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">tail</span>

        <span class="n">output_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">part</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sentence_split</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&amp;</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># special token</span>
                <span class="n">output_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">part</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">for_cpmbee</span> <span class="ow">and</span> <span class="p">(</span><span class="n">part</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">part</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">[</span><span class="n">part</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">[</span><span class="n">part</span><span class="p">]]</span> <span class="o">=</span> <span class="n">part</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output_tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tokenize</span><span class="p">(</span><span class="n">part</span><span class="p">,</span> <span class="n">for_cpmbee</span><span class="o">=</span><span class="n">for_cpmbee</span><span class="p">))</span>

        <span class="c1"># drop spaces</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">:</span>
                <span class="n">token</span> <span class="o">=</span> <span class="n">all_special_tokens_extended</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="n">left</span> <span class="o">=</span> <span class="n">output_tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
                <span class="n">right</span> <span class="o">=</span> <span class="n">output_tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">rstrip</span> <span class="ow">and</span> <span class="n">right</span><span class="p">:</span>
                        <span class="c1"># A bit counter-intuitive but we strip the left of the string</span>
                        <span class="c1"># since tok_extended.rstrip means the special token is eating all white spaces on its right</span>
                        <span class="n">output_tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">right</span><span class="o">.</span><span class="n">lstrip</span><span class="p">()</span>
                    <span class="c1"># Strip white spaces on the left</span>
                    <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">lstrip</span> <span class="ow">and</span> <span class="n">left</span><span class="p">:</span>
                        <span class="n">output_tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">left</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span>  <span class="c1"># Opposite here</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">right</span><span class="p">:</span>
                        <span class="n">output_tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">right</span><span class="o">.</span><span class="n">lstrip</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">left</span><span class="p">:</span>
                        <span class="n">output_tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">left</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span>

        <span class="n">skipped_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">output_tokens</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">token</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">skipped_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">skipped_tokens</span>

    <span class="k">def</span> <span class="nf">_tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a string in a sequence of tokens (string), using the tokenizer. Split in words for word-based</span>
<span class="sd">        vocabulary.</span>

<span class="sd">        Do NOT take care of added tokens. Record the unk tokens and special tokens in `ext_table` and `ext_table_rev`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">for_cpmbee</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;for_cpmbee&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">output_tokens</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">part_st</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">last_unk</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">while</span> <span class="n">part_st</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
            <span class="n">piece</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_piece</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="n">part_st</span><span class="p">:])</span>
            <span class="k">if</span> <span class="n">piece</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">last_unk</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">output_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">piece</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">for_cpmbee</span> <span class="ow">and</span> <span class="p">(</span><span class="n">last_unk</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">):</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">[</span><span class="n">last_unk</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">[</span><span class="n">last_unk</span><span class="p">]]</span> <span class="o">=</span> <span class="n">last_unk</span>
                    <span class="n">output_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">last_unk</span><span class="p">)</span>
                    <span class="n">output_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">piece</span><span class="p">)</span>
                    <span class="n">last_unk</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">last_unk</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">last_unk</span> <span class="o">=</span> <span class="n">piece</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">last_unk</span> <span class="o">+=</span> <span class="n">piece</span>
            <span class="n">part_st</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">piece</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">last_unk</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># part end with UNK</span>
            <span class="k">if</span> <span class="n">for_cpmbee</span> <span class="ow">and</span> <span class="p">(</span><span class="n">last_unk</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">[</span><span class="n">last_unk</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">[</span><span class="n">last_unk</span><span class="p">]]</span> <span class="o">=</span> <span class="n">last_unk</span>
            <span class="n">output_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">last_unk</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output_tokens</span>

    <span class="k">def</span> <span class="nf">check</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Checks if a token is present in the encoder.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeTokenizer): An instance of the CpmBeeTokenizer class.</span>
<span class="sd">            token (Any): The token to be checked in the encoder.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span>

    <span class="k">def</span> <span class="nf">convert_tokens_to_string</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a list of tokens into a single string.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeTokenizer): An instance of the CpmBeeTokenizer class.</span>
<span class="sd">            tokens (List[str]): A list of tokens to be converted into a string.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: A string representation of the tokens.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>

<span class="sd">        This method takes in two parameters, self and tokens. The self parameter is an instance of the CpmBeeTokenizer</span>
<span class="sd">        class and is used to access the class&#39;s attributes and methods. The tokens parameter is a</span>
<span class="sd">        list of strings representing individual tokens.</span>

<span class="sd">        The function returns a string that is obtained by concatenating all the tokens together using the &#39;&#39;.join() method.</span>
<span class="sd">        This method does not modify the original list of tokens.</span>

<span class="sd">        No exceptions are raised by this method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_convert_token_to_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Converts a token (str) in an id using the vocab and ext_table.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">[</span><span class="n">token</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">[</span><span class="n">token</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token_id</span>

    <span class="k">def</span> <span class="nf">_convert_id_to_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Converts an index (integer) in a token (str) using the vocab and ext_table.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">index</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">index</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_decoder</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_decoder</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">index</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">save_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the vocabulary to a file.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeTokenizer): The instance of the CpmBeeTokenizer class.</span>
<span class="sd">            save_directory (str): The directory where the vocabulary file will be saved.</span>
<span class="sd">            filename_prefix (Optional[str]): An optional prefix to prepend to the filename. Default is None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple[str]: A tuple containing the path to the saved vocabulary file.</span>

<span class="sd">        Raises:</span>
<span class="sd">            IOError: If there is an issue with reading or writing the vocabulary file.</span>
<span class="sd">            ValueError: If the provided save_directory is not a valid directory.</span>
<span class="sd">            KeyError: If any of the keys used for encoding tokens are not found in the encoder dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
            <span class="n">vocab_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="n">save_directory</span><span class="p">,</span> <span class="p">(</span><span class="n">filename_prefix</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="k">if</span> <span class="n">filename_prefix</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">VOCAB_FILES_NAMES</span><span class="p">[</span><span class="s2">&quot;vocab_file&quot;</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">vocab_file</span> <span class="o">=</span> <span class="p">(</span><span class="n">filename_prefix</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="k">if</span> <span class="n">filename_prefix</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">save_directory</span>
        <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;&lt;/n&gt;&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">]</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;&lt;/_&gt;&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot; &quot;</span><span class="p">]</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot; &quot;</span><span class="p">]</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">token_index</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="k">if</span> <span class="n">index</span> <span class="o">!=</span> <span class="n">token_index</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Saving vocabulary to </span><span class="si">{</span><span class="n">vocab_file</span><span class="si">}</span><span class="s2">: vocabulary indices are not consecutive.&quot;</span>
                        <span class="s2">&quot; Please check that the vocabulary is not corrupted!&quot;</span>
                    <span class="p">)</span>
                    <span class="n">index</span> <span class="o">=</span> <span class="n">token_index</span>
                <span class="n">writer</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">token</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">vocab_file</span><span class="p">,)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        CPMBee `call` method will use `_tokenize_cpmbee` when the input type is dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_tokenize_cpmbee</span><span class="p">([</span><span class="n">text</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_tokenize_cpmbee</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># 分词</span>
    <span class="k">def</span> <span class="nf">_tokenize_cpmbee</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TextInput</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A tokenize method to process dict data. Exclusive for CPMBee.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;CpmBeeTokenizer input data should be dict or str in dict format, but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
            <span class="p">)</span>

        <span class="c1"># 1. prepare answer placeholder</span>
        <span class="n">answer_placeholders</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">def</span> <span class="nf">_put_placeholder</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">ret</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">_put_placeholder</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">path</span> <span class="o">+</span> <span class="p">[</span><span class="n">k</span><span class="p">])</span>
                <span class="k">return</span> <span class="n">ret</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">answer_placeholders</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
                <span class="k">return</span> <span class="s2">&quot;&lt;ans_</span><span class="si">{}</span><span class="s2">&gt;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">answer_placeholders</span><span class="p">))</span>

        <span class="n">data</span><span class="p">[</span><span class="s2">&quot;&lt;ans&gt;&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_put_placeholder</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;&lt;ans&gt;&quot;</span><span class="p">])</span>

        <span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">input_id_subs</span><span class="p">,</span>
            <span class="n">context</span><span class="p">,</span>
            <span class="n">segment_ids</span><span class="p">,</span>
            <span class="n">segment_rel</span><span class="p">,</span>
            <span class="n">n_segments</span><span class="p">,</span>
            <span class="n">table_states</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_data_to_id</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">shuffle_answer</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

        <span class="c1"># &lt;ans&gt; mapping from sub to id</span>
        <span class="n">sub_ans_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">fake_id</span><span class="p">,</span> <span class="n">token_sub</span> <span class="ow">in</span> <span class="n">table_states</span><span class="p">[</span><span class="s2">&quot;token_id_table&quot;</span><span class="p">][</span><span class="s2">&quot;&lt;ans&gt;&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">token</span> <span class="o">=</span> <span class="n">table_states</span><span class="p">[</span><span class="s2">&quot;ext_table&quot;</span><span class="p">][</span><span class="n">fake_id</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;&lt;ans_&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">token</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;&gt;&quot;</span><span class="p">):</span>
                <span class="n">ans_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">token</span><span class="p">[</span><span class="mi">5</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">sub_ans_map</span><span class="p">[</span><span class="n">token_sub</span><span class="p">]</span> <span class="o">=</span> <span class="n">ans_id</span>

        <span class="n">tmp_input_ids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">tmp_input_sub</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">tmp_input_seg</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># get predict segments</span>
        <span class="n">predict_segments</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">if</span> <span class="n">context</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;&lt;ans&gt;&quot;</span><span class="p">]:</span>
                    <span class="c1"># is ans</span>
                    <span class="c1"># (segment_id, ans_id)</span>
                    <span class="n">predict_segments</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">segment_ids</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sub_ans_map</span><span class="p">[</span><span class="n">input_id_subs</span><span class="p">[</span><span class="n">i</span><span class="p">]]))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tmp_input_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">tmp_input_sub</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_id_subs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">tmp_input_seg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">predict_segments</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No answer to predict&quot;</span><span class="p">)</span>

        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tmp_input_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>  <span class="c1"># all context</span>
        <span class="n">input_id_subs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tmp_input_sub</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>  <span class="c1"># [0, 0, 0, 0, 1, 0, 0, 2, 0, ...]</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">tmp_input_ids</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>  <span class="c1"># [1, 1, 1, ...]</span>
        <span class="n">segment_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tmp_input_seg</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>  <span class="c1"># [0, 0, 0, 1, 1, 1, 2, 2, 2, 2, ...]</span>
        <span class="n">sample_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>  <span class="c1"># [0, 0, 0, 0, ...]</span>
        <span class="n">segment_rel_offset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>  <span class="c1"># [0, 0, 0, ...]</span>
        <span class="n">num_segments</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">n_segments</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>  <span class="c1"># [n_seg, n_seg, n_seg, ...]</span>
        <span class="n">input_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>  <span class="c1"># [0, 1, 2, 3, 4, ...]</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prepare_for_model</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                <span class="n">input_id_subs</span><span class="o">=</span><span class="n">input_id_subs</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                <span class="n">input_pos</span><span class="o">=</span><span class="n">input_pos</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                <span class="n">segment_ids</span><span class="o">=</span><span class="n">segment_ids</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                <span class="n">segment_rel_offset</span><span class="o">=</span><span class="n">segment_rel_offset</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                <span class="n">segment_rel</span><span class="o">=</span><span class="n">segment_rel</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                <span class="n">sample_ids</span><span class="o">=</span><span class="n">sample_ids</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                <span class="n">num_segments</span><span class="o">=</span><span class="n">num_segments</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">predict_segments</span><span class="p">,</span>
            <span class="n">answer_placeholders</span><span class="p">,</span>
            <span class="n">table_states</span><span class="p">[</span><span class="s2">&quot;ext_table&quot;</span><span class="p">],</span>
            <span class="n">table_states</span><span class="p">[</span><span class="s2">&quot;token_id_table&quot;</span><span class="p">],</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_batch_tokenize_cpmbee</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_lst</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Batched _token_cpmbee.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">return_tensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;return_tensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">batch_outputs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">segment_rel_pack</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">other_info</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">batch_ext_table_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">batch_ext_table_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">batch_ext_table_sub</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">data_lst</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">predict_segments</span><span class="p">,</span> <span class="n">answer_placeholders</span><span class="p">,</span> <span class="n">ext_table</span><span class="p">,</span> <span class="n">token_id_table</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenize_cpmbee</span><span class="p">(</span>
                <span class="n">data</span><span class="p">,</span>
                <span class="n">truncation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">return_attention_mask</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">return_tensors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">rev_ext_table</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">mp</span> <span class="ow">in</span> <span class="n">token_id_table</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="s2">&quot;&lt;ans&gt;&quot;</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="n">token</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">fake_id</span><span class="p">,</span> <span class="n">token_sub</span> <span class="ow">in</span> <span class="n">mp</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">token_sub</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="k">if</span> <span class="p">(</span><span class="n">token_id</span><span class="p">,</span> <span class="n">token_sub</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">batch_ext_table_map</span><span class="p">:</span>
                            <span class="n">batch_ext_table_map</span><span class="p">[(</span><span class="n">token_id</span><span class="p">,</span> <span class="n">token_sub</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_ext_table_ids</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span>
                            <span class="n">batch_ext_table_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token_id</span><span class="p">)</span>
                            <span class="n">batch_ext_table_sub</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token_sub</span><span class="p">)</span>
                        <span class="n">rev_ext_table</span><span class="p">[</span><span class="n">batch_ext_table_map</span><span class="p">[(</span><span class="n">token_id</span><span class="p">,</span> <span class="n">token_sub</span><span class="p">)]]</span> <span class="o">=</span> <span class="n">ext_table</span><span class="p">[</span><span class="n">fake_id</span><span class="p">]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">rev_ext_table</span><span class="p">[</span><span class="n">token_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">ext_table</span><span class="p">[</span><span class="n">fake_id</span><span class="p">]</span>

            <span class="n">segment_rel_pack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;segment_rel&quot;</span><span class="p">)))</span>
            <span class="n">other_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;predict_segments&quot;</span><span class="p">:</span> <span class="n">predict_segments</span><span class="p">,</span>
                    <span class="s2">&quot;answer_placeholders&quot;</span><span class="p">:</span> <span class="n">answer_placeholders</span><span class="p">,</span>
                    <span class="s2">&quot;ext_table&quot;</span><span class="p">:</span> <span class="n">rev_ext_table</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span>

            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">outputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">batch_outputs</span><span class="p">:</span>
                    <span class="n">batch_outputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">batch_outputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="n">max_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch_outputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_outputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch_outputs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">required_input</span> <span class="o">=</span> <span class="n">v</span>

                <span class="n">needs_to_be_padded</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">required_input</span><span class="p">)</span> <span class="o">!=</span> <span class="n">max_length</span>

                <span class="k">if</span> <span class="n">needs_to_be_padded</span><span class="p">:</span>
                    <span class="n">difference</span> <span class="o">=</span> <span class="n">max_length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">required_input</span><span class="p">)</span>
                    <span class="n">batch_outputs</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span> <span class="o">+</span> <span class="n">required_input</span>

        <span class="n">max_num_rels</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">rel</span> <span class="ow">in</span> <span class="n">segment_rel_pack</span><span class="p">:</span>
            <span class="n">max_num_rels</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_num_rels</span><span class="p">,</span> <span class="n">rel</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">padded_rels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">segment_rel_pack</span><span class="p">),</span> <span class="n">max_num_rels</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">rel</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">segment_rel_pack</span><span class="p">):</span>
            <span class="n">padded_rels</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span> <span class="n">rel</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">rel</span>
        <span class="n">batch_outputs</span><span class="p">[</span><span class="s2">&quot;segment_rel&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">padded_rels</span>
        <span class="n">batch_outputs</span><span class="p">[</span><span class="s2">&quot;batch_ext_table_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_ext_table_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">batch_outputs</span><span class="p">[</span><span class="s2">&quot;batch_ext_table_sub&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_ext_table_sub</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">batch_outputs</span> <span class="o">=</span> <span class="n">BatchEncoding</span><span class="p">(</span><span class="n">batch_outputs</span><span class="p">,</span> <span class="n">tensor_type</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">)</span>
        <span class="n">batch_outputs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">other_info</span>

        <span class="k">return</span> <span class="n">batch_outputs</span>

    <span class="k">def</span> <span class="nf">convert_data_to_id</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">prev_ext_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_PrevExtTableStates</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shuffle_answer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">max_depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parse a dict to data ids. Exclusive for CPMBee. It will</span>

<span class="sd">        1. parse the dict to segments and get segment_rel, which for calculating of position_bias.</span>
<span class="sd">        2. tokenize every segment.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">root</span><span class="p">:</span> <span class="n">_DictTree</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;&lt;root&gt;&quot;</span><span class="p">,</span>
            <span class="s2">&quot;children&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;depth&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;segment_id&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;need_predict&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">segments</span> <span class="o">=</span> <span class="p">[</span><span class="n">root</span><span class="p">]</span>

        <span class="k">def</span> <span class="nf">_build_dict_tree</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">CPMBeeInputType</span><span class="p">,</span> <span class="n">depth</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">need_predict</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">_DictTree</span><span class="p">]:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">ret_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">_DictTree</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">curr_items</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
                <span class="k">if</span> <span class="n">need_predict</span> <span class="ow">and</span> <span class="n">shuffle_answer</span><span class="p">:</span>
                    <span class="n">access_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">curr_items</span><span class="p">))</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">access_idx</span><span class="p">)</span>
                    <span class="n">curr_items</span> <span class="o">=</span> <span class="p">[</span><span class="n">curr_items</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">access_idx</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">curr_items</span><span class="p">:</span>
                    <span class="n">child_info</span><span class="p">:</span> <span class="n">_DictTree</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">k</span><span class="p">,</span>
                        <span class="s2">&quot;children&quot;</span><span class="p">:</span> <span class="p">[],</span>
                        <span class="s2">&quot;depth&quot;</span><span class="p">:</span> <span class="n">depth</span><span class="p">,</span>
                        <span class="s2">&quot;segment_id&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">segments</span><span class="p">),</span>
                        <span class="s2">&quot;need_predict&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># only leaves are contexts</span>
                    <span class="p">}</span>
                    <span class="n">segments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child_info</span><span class="p">)</span>
                    <span class="n">child_info</span><span class="p">[</span><span class="s2">&quot;children&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_build_dict_tree</span><span class="p">(</span>
                        <span class="n">v</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">need_predict</span> <span class="ow">or</span> <span class="p">(</span><span class="n">depth</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;&lt;ans&gt;&quot;</span><span class="p">)</span>
                    <span class="p">)</span>  <span class="c1"># elements in &lt;root&gt;.&lt;ans&gt;</span>

                    <span class="n">ret_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child_info</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">ret_list</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">str</span><span class="p">),</span> <span class="s2">&quot;Invalid data </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                <span class="n">ret</span><span class="p">:</span> <span class="n">_DictTree</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">data</span><span class="p">,</span>
                    <span class="s2">&quot;children&quot;</span><span class="p">:</span> <span class="p">[],</span>
                    <span class="s2">&quot;depth&quot;</span><span class="p">:</span> <span class="n">depth</span><span class="p">,</span>
                    <span class="s2">&quot;segment_id&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">segments</span><span class="p">),</span>
                    <span class="s2">&quot;need_predict&quot;</span><span class="p">:</span> <span class="n">need_predict</span><span class="p">,</span>
                <span class="p">}</span>
                <span class="n">segments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
                <span class="k">return</span> <span class="p">[</span><span class="n">ret</span><span class="p">]</span>

        <span class="n">root</span><span class="p">[</span><span class="s2">&quot;children&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_build_dict_tree</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">num_segments</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">segments</span><span class="p">)</span>
        <span class="n">segment_rel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_segments</span> <span class="o">*</span> <span class="n">num_segments</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_build_segment_rel</span><span class="p">(</span><span class="n">node</span><span class="p">:</span> <span class="n">_DictTree</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
            <span class="n">ret</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[(</span><span class="n">node</span><span class="p">[</span><span class="s2">&quot;segment_id&quot;</span><span class="p">],</span> <span class="n">node</span><span class="p">[</span><span class="s2">&quot;depth&quot;</span><span class="p">])]</span>
            <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">node</span><span class="p">[</span><span class="s2">&quot;children&quot;</span><span class="p">]:</span>
                <span class="n">sub</span> <span class="o">=</span> <span class="n">_build_segment_rel</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">seg_id_1</span><span class="p">,</span> <span class="n">depth_1</span> <span class="ow">in</span> <span class="n">sub</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">seg_id_2</span><span class="p">,</span> <span class="n">depth_2</span> <span class="ow">in</span> <span class="n">ret</span><span class="p">:</span>
                        <span class="n">n_up</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">depth_1</span> <span class="o">-</span> <span class="n">node</span><span class="p">[</span><span class="s2">&quot;depth&quot;</span><span class="p">],</span> <span class="n">max_depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                        <span class="n">n_down</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">depth_2</span> <span class="o">-</span> <span class="n">node</span><span class="p">[</span><span class="s2">&quot;depth&quot;</span><span class="p">],</span> <span class="n">max_depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                        <span class="n">segment_rel</span><span class="p">[</span><span class="n">seg_id_1</span> <span class="o">*</span> <span class="n">num_segments</span> <span class="o">+</span> <span class="n">seg_id_2</span><span class="p">]</span> <span class="o">=</span> <span class="n">rel_to_bucket</span><span class="p">(</span>
                            <span class="n">n_up</span><span class="p">,</span> <span class="n">n_down</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span>
                        <span class="p">)</span>
                        <span class="n">segment_rel</span><span class="p">[</span><span class="n">seg_id_2</span> <span class="o">*</span> <span class="n">num_segments</span> <span class="o">+</span> <span class="n">seg_id_1</span><span class="p">]</span> <span class="o">=</span> <span class="n">rel_to_bucket</span><span class="p">(</span>
                            <span class="n">n_down</span><span class="p">,</span> <span class="n">n_up</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span>
                        <span class="p">)</span>
                <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">sub</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">ret</span>

        <span class="n">_build_segment_rel</span><span class="p">(</span><span class="n">root</span><span class="p">)</span>

        <span class="n">input_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">input_id_subs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">segment_bound</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="n">prev_ext_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span> <span class="o">=</span> <span class="n">prev_ext_states</span><span class="p">[</span><span class="s2">&quot;ext_table&quot;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span> <span class="o">=</span> <span class="n">prev_ext_states</span><span class="p">[</span><span class="s2">&quot;token_id_table&quot;</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">seg</span> <span class="ow">in</span> <span class="n">segments</span><span class="p">:</span>
            <span class="c1"># tokenize</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">seg</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">],</span> <span class="n">for_cpmbee</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

            <span class="n">token_id_subs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">reid_token_ids</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span><span class="p">:</span>
                    <span class="c1"># unk or special token</span>
                    <span class="n">token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;&lt;&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">token</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;&gt;&quot;</span><span class="p">):</span>
                        <span class="c1"># special token</span>
                        <span class="k">if</span> <span class="s2">&quot;_&quot;</span> <span class="ow">in</span> <span class="n">token</span><span class="p">:</span>
                            <span class="n">token_name</span> <span class="o">=</span> <span class="n">token</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">,</span> <span class="n">maxsplit</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">token_name</span> <span class="o">=</span> <span class="n">token</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                        <span class="n">token_name</span> <span class="o">=</span> <span class="s2">&quot;&lt;</span><span class="si">{}</span><span class="s2">&gt;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">token_name</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">token_name</span> <span class="o">=</span> <span class="s2">&quot;&lt;unk&gt;&quot;</span>

                    <span class="k">if</span> <span class="n">token_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span><span class="p">[</span><span class="n">token_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
                    <span class="k">if</span> <span class="n">idx</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span><span class="p">[</span><span class="n">token_name</span><span class="p">]:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span><span class="p">[</span><span class="n">token_name</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span><span class="p">[</span><span class="n">token_name</span><span class="p">])</span>
                    <span class="k">if</span> <span class="n">token_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid token </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>
                    <span class="n">reid_token_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="n">token_name</span><span class="p">])</span>
                    <span class="n">token_id_subs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span><span class="p">[</span><span class="n">token_name</span><span class="p">][</span><span class="n">idx</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">reid_token_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
                    <span class="n">token_id_subs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">bos_token_id</span><span class="p">]</span> <span class="o">+</span> <span class="n">reid_token_ids</span>
            <span class="n">token_id_subs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">token_id_subs</span>
            <span class="c1"># eos_id 表示 no need_predict</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">seg</span><span class="p">[</span><span class="s2">&quot;need_predict&quot;</span><span class="p">]:</span>  <span class="c1"># eos</span>
                <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>
                <span class="n">token_id_subs</span> <span class="o">=</span> <span class="n">token_id_subs</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># no eos</span>
                <span class="k">pass</span>
            <span class="n">begin</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
            <span class="n">input_ids</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
            <span class="n">input_id_subs</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">token_id_subs</span><span class="p">)</span>
            <span class="n">end</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
            <span class="n">segment_bound</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">))</span>

        <span class="n">ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">id_subs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_id_subs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">segs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>  <span class="c1"># 按segment_bound对seg编号</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">segment_bound</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">segments</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;need_predict&quot;</span><span class="p">]:</span>
                <span class="n">context</span><span class="p">[</span><span class="n">begin</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">segs</span><span class="p">[</span><span class="n">begin</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>

        <span class="n">curr_ext_table_states</span><span class="p">:</span> <span class="n">_PrevExtTableStates</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;ext_table&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span><span class="p">,</span>
            <span class="s2">&quot;token_id_table&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">ids</span><span class="p">,</span> <span class="n">id_subs</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">segs</span><span class="p">,</span> <span class="n">segment_rel</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">,</span> <span class="n">curr_ext_table_states</span>

    <span class="k">def</span> <span class="nf">prepare_for_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">pair_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">prepend_batch_axis</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It</span>
<span class="sd">        adds special tokens, truncates sequences if overflowing while taking into account the special tokens and</span>
<span class="sd">        manages a moving window (with user defined stride) for overflowing tokens. Please Note, for *pair_ids*</span>
<span class="sd">        different than `None` and *truncation_strategy = longest_first* or `True`, it is not possible to return</span>
<span class="sd">        overflowing tokens. Such a combination of arguments will raise an error.</span>

<span class="sd">        Args:</span>
<span class="sd">            ids (`List[int]`):</span>
<span class="sd">                Tokenized input ids of the first sequence. Can be obtained from a string by chaining the `tokenize` and</span>
<span class="sd">                `convert_tokens_to_ids` methods.</span>
<span class="sd">            pair_ids (`List[int]`, *optional*):</span>
<span class="sd">                Tokenized input ids of the second sequence. Can be obtained from a string by chaining the `tokenize`</span>
<span class="sd">                and `convert_tokens_to_ids` methods.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Backward compatibility for &#39;truncation_strategy&#39;, &#39;pad_to_max_length&#39;</span>
        <span class="n">padding_strategy</span><span class="p">,</span> <span class="n">truncation_strategy</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_padding_truncation_strategies</span><span class="p">(</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">pair</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">pair_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">len_ids</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
        <span class="n">len_pair_ids</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">pair</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">return_token_type_ids</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">add_special_tokens</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Asking to return token_type_ids while setting add_special_tokens to False &quot;</span>
                <span class="s2">&quot;results in an undefined behavior. Please set add_special_tokens to True or &quot;</span>
                <span class="s2">&quot;set return_token_type_ids to None.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">return_overflowing_tokens</span>
            <span class="ow">and</span> <span class="n">truncation_strategy</span> <span class="o">==</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">LONGEST_FIRST</span>
            <span class="ow">and</span> <span class="n">pair_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Not possible to return overflowing tokens for pair of sequences with the &quot;</span>
                <span class="s2">&quot;`longest_first`. Please select another truncation strategy than `longest_first`, &quot;</span>
                <span class="s2">&quot;for instance `only_second` or `only_first`.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Load from model defaults</span>
        <span class="k">if</span> <span class="n">return_token_type_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">return_token_type_ids</span> <span class="o">=</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span>
        <span class="k">if</span> <span class="n">return_attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span>

        <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Compute the total size of the returned encodings</span>
        <span class="n">total_len</span> <span class="o">=</span> <span class="n">len_ids</span> <span class="o">+</span> <span class="n">len_pair_ids</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_special_tokens_to_add</span><span class="p">(</span><span class="n">pair</span><span class="o">=</span><span class="n">pair</span><span class="p">)</span> <span class="k">if</span> <span class="n">add_special_tokens</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Truncation: Handle max sequence length</span>
        <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">truncation_strategy</span> <span class="o">!=</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">DO_NOT_TRUNCATE</span> <span class="ow">and</span> <span class="n">max_length</span> <span class="ow">and</span> <span class="n">total_len</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">,</span> <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncate_sequences</span><span class="p">(</span>
                <span class="n">ids</span><span class="p">,</span>
                <span class="n">pair_ids</span><span class="o">=</span><span class="n">pair_ids</span><span class="p">,</span>
                <span class="n">num_tokens_to_remove</span><span class="o">=</span><span class="n">total_len</span> <span class="o">-</span> <span class="n">max_length</span><span class="p">,</span>
                <span class="n">truncation_strategy</span><span class="o">=</span><span class="n">truncation_strategy</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_overflowing_tokens</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;overflowing_tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">overflowing_tokens</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;num_truncated_tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_len</span> <span class="o">-</span> <span class="n">max_length</span>

        <span class="c1"># Add special tokens</span>
        <span class="k">if</span> <span class="n">add_special_tokens</span><span class="p">:</span>
            <span class="n">sequence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_inputs_with_special_tokens</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">)</span>
            <span class="n">token_type_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_token_type_ids_from_sequences</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sequence</span> <span class="o">=</span> <span class="n">ids</span> <span class="o">+</span> <span class="n">pair_ids</span> <span class="k">if</span> <span class="n">pair</span> <span class="k">else</span> <span class="n">ids</span>
            <span class="n">token_type_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">+</span> <span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">pair</span> <span class="k">else</span> <span class="p">[])</span>

        <span class="c1"># Build output dictionary</span>
        <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sequence</span>
        <span class="k">if</span> <span class="n">return_token_type_ids</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_type_ids</span>
        <span class="k">if</span> <span class="n">return_special_tokens_mask</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">add_special_tokens</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_special_tokens_mask</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>

        <span class="c1"># Check lengths</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eventual_warn_about_too_long_sequence</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

        <span class="c1"># Padding</span>
        <span class="k">if</span> <span class="n">padding_strategy</span> <span class="o">!=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span> <span class="ow">or</span> <span class="n">return_attention_mask</span><span class="p">:</span>
            <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">encoded_inputs</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="n">padding_strategy</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
                <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_length</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>

        <span class="c1"># for CPMBee, encode all the model arguments</span>
        <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ext_args_for_model</span><span class="p">:</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

        <span class="n">batch_outputs</span> <span class="o">=</span> <span class="n">BatchEncoding</span><span class="p">(</span>
            <span class="n">encoded_inputs</span><span class="p">,</span> <span class="n">tensor_type</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span> <span class="n">prepend_batch_axis</span><span class="o">=</span><span class="n">prepend_batch_axis</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">batch_outputs</span>

    <span class="k">def</span> <span class="nf">prepare_for_finetune</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares the input data for fine-tuning.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeTokenizer): The instance of the CpmBeeTokenizer class.</span>
<span class="sd">            data_list (List[Dict]): A list of dictionaries containing the input data.</span>
<span class="sd">            max_length (int, optional): The maximum length of the input data. Defaults to 2048.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">_inputs_sub</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">_context</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">_sample_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">_segments</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">_num_segments</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">_segment_rel_offset</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">_segment_rel</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">_spans</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">_raw_data</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">raw_data</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">data_list</span><span class="p">:</span>
            <span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">input_id_subs</span><span class="p">,</span>
                <span class="n">context</span><span class="p">,</span>
                <span class="n">segment_ids</span><span class="p">,</span>
                <span class="n">segment_rel</span><span class="p">,</span>
                <span class="n">n_segments</span><span class="p">,</span>
                <span class="n">_</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_data_to_id</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:</span> <span class="n">max_length</span><span class="p">]</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[:</span> <span class="n">max_length</span><span class="p">]</span>
            <span class="n">segment_ids</span> <span class="o">=</span> <span class="n">segment_ids</span><span class="p">[:</span> <span class="n">max_length</span><span class="p">]</span>
            <span class="n">raw_data</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">raw_data</span><span class="p">[</span><span class="s2">&quot;samples&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="n">sample_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="n">segment_rel_offset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="n">num_segments</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">n_segments</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

            <span class="n">_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
            <span class="n">_inputs_sub</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_id_subs</span><span class="p">)</span>
            <span class="n">_context</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
            <span class="n">_sample_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample_ids</span><span class="p">)</span>
            <span class="n">_segments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">)</span>
            <span class="n">_num_segments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">num_segments</span><span class="p">)</span>
            <span class="n">_segment_rel_offset</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">segment_rel_offset</span><span class="p">)</span>
            <span class="n">_segment_rel</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">segment_rel</span><span class="p">)</span>
            <span class="n">_spans</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
            <span class="n">_raw_data</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">raw_data</span><span class="p">])</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">_inputs</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">inputs_sub</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
        <span class="n">sample_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">segments</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">num_segments</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">segment_rel_offset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">tgt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

        <span class="n">max_rel</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">max_rel</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_rel</span><span class="p">,</span> <span class="n">_segment_rel</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">segment_rel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_rel</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">spans</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">length</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

        <span class="n">batch_ext_table_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">batch_ext_table_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">batch_ext_table_sub</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">raw_data_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">instance_length</span> <span class="o">=</span> <span class="n">_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">rel_size</span> <span class="o">=</span> <span class="n">_segment_rel</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">instance_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">inputs_sub</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">instance_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">_inputs_sub</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">context</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">instance_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">_context</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">sample_ids</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">instance_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">_sample_ids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">segments</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">instance_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">_segments</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">num_segments</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">instance_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">_num_segments</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">segment_rel_offset</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">instance_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">_segment_rel_offset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">segment_rel</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">rel_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">_segment_rel</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

            <span class="n">span_begin</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">span_id</span><span class="p">,</span> <span class="n">span_end</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">_spans</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
                <span class="n">spans</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">span_begin</span><span class="p">:</span><span class="n">span_end</span><span class="p">]</span> <span class="o">=</span> <span class="n">span_id</span>
                <span class="n">span_begin</span> <span class="o">=</span> <span class="n">span_end</span>
            <span class="n">length</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">instance_length</span>
            <span class="n">raw_data_list</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">_raw_data</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">instance_length</span><span class="p">):</span>
                <span class="n">idx</span><span class="p">,</span> <span class="n">idx_sub</span> <span class="o">=</span> <span class="n">_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="n">_inputs_sub</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
                <span class="n">tgt_idx</span> <span class="o">=</span> <span class="n">idx</span>
                <span class="k">if</span> <span class="n">idx_sub</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># need to be in ext table</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_sub</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">batch_ext_table_map</span><span class="p">:</span>
                        <span class="n">batch_ext_table_map</span><span class="p">[(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_sub</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_ext_table_map</span><span class="p">)</span>
                        <span class="n">batch_ext_table_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
                        <span class="n">batch_ext_table_sub</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx_sub</span><span class="p">)</span>
                    <span class="n">tgt_idx</span> <span class="o">=</span> <span class="n">batch_ext_table_map</span><span class="p">[(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_sub</span><span class="p">)]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span>
                <span class="k">if</span> <span class="n">j</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">context</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">idx</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bos_token_id</span><span class="p">:</span>
                        <span class="n">tgt</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">tgt_idx</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">tgt</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span>
            <span class="k">if</span> <span class="n">context</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">instance_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">tgt</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">instance_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_ext_table_map</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># placeholder</span>
            <span class="n">batch_ext_table_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">batch_ext_table_sub</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">BatchEncoding</span><span class="p">({</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">inputs</span><span class="p">,</span>
            <span class="s2">&quot;input_id_sub&quot;</span><span class="p">:</span> <span class="n">inputs_sub</span><span class="p">,</span>
            <span class="s2">&quot;length&quot;</span><span class="p">:</span> <span class="n">length</span><span class="p">,</span>
            <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">context</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;sample_ids&quot;</span><span class="p">:</span> <span class="n">sample_ids</span><span class="p">,</span>
            <span class="s2">&quot;num_segments&quot;</span><span class="p">:</span> <span class="n">num_segments</span><span class="p">,</span>
            <span class="s2">&quot;segment&quot;</span><span class="p">:</span> <span class="n">segments</span><span class="p">,</span>
            <span class="s2">&quot;segment_rel_offset&quot;</span><span class="p">:</span> <span class="n">segment_rel_offset</span><span class="p">,</span>
            <span class="s2">&quot;segment_rel&quot;</span><span class="p">:</span> <span class="n">segment_rel</span><span class="p">,</span>
            <span class="s2">&quot;span&quot;</span><span class="p">:</span> <span class="n">spans</span><span class="p">,</span>
            <span class="s2">&quot;labels&quot;</span><span class="p">:</span> <span class="n">tgt</span><span class="p">,</span>
            <span class="s2">&quot;ext_table_ids&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_ext_table_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
            <span class="s2">&quot;ext_table_sub&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_ext_table_sub</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="p">},</span> <span class="n">tensor_type</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.bod_token_id" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTokenizer</span><span class="o">.</span><span class="n">bod_token_id</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.bod_token_id" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the token ID for the beginning of document (BOD) token.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeTokenizer class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>This method returns the token ID corresponding to the BOD token in the encoder dictionary.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.eod_token_id" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTokenizer</span><span class="o">.</span><span class="n">eod_token_id</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.eod_token_id" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Method to retrieve the token ID corresponding to the end-of-document token in the CpmBeeTokenizer class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeTokenizer class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The method returns the token ID of the end-of-document token in the tokenizer's encoder.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.newline_id" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTokenizer</span><span class="o">.</span><span class="n">newline_id</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.newline_id" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the ID of the newline token in the CpmBeeTokenizer.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeTokenizer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer">CpmBeeTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.vocab_size" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.vocab_size" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the size of the vocabulary used by the CpmBeeTokenizer instance.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The CpmBeeTokenizer instance.</p>
<ul>
<li>This parameter is of type 'CpmBeeTokenizer'.</li>
<li>It represents the instance of the CpmBeeTokenizer class on which the method is called.</li>
</ul>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>int</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>An integer representing the size of the vocabulary.</p>
<ul>
<li>The returned value represents the total number of unique tokens in the vocabulary.</li>
</ul>
              </div>
                <p>
                  <span class="doc-returns-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CpmBeeTokenizer</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">()</span>
<span class="mi">5000</span>
</code></pre></div>
</details>    </div>

</div>



<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.__call__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTokenizer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.__call__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>CPMBee <code>call</code> method will use <code>_tokenize_cpmbee</code> when the input type is dict.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\tokenization_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    CPMBee `call` method will use `_tokenize_cpmbee` when the input type is dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_tokenize_cpmbee</span><span class="p">([</span><span class="n">text</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_tokenize_cpmbee</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTokenizer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">bos_token</span><span class="o">=</span><span class="s1">&#39;&lt;s&gt;&#39;</span><span class="p">,</span> <span class="n">eos_token</span><span class="o">=</span><span class="s1">&#39;&lt;/s&gt;&#39;</span><span class="p">,</span> <span class="n">line_token</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">space_token</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">unk_token</span><span class="o">=</span><span class="s1">&#39;&lt;unk&gt;&#39;</span><span class="p">,</span> <span class="n">mask_token</span><span class="o">=</span><span class="s1">&#39;&lt;mask&gt;&#39;</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s1">&#39;&lt;pad&gt;&#39;</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initialize a CpmBeeTokenizer object.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vocab_file</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The path to the file containing the vocabulary.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bos_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The beginning of sentence token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The end of sentence token.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;/s&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>line_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token used to represent a new line.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;\n&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>space_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token used to represent a space.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39; &#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unk_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token used to represent unknown words.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;unk&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mask_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token used for masking.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;mask&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token used for padding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;&lt;pad&gt;&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_side</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The side to apply padding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;left&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>**kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Additional keyword arguments.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>FileNotFoundError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the vocab_file does not exist.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If any of the arguments are of incorrect type.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\tokenization_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">vocab_file</span><span class="p">,</span>
    <span class="n">bos_token</span><span class="o">=</span><span class="s2">&quot;&lt;s&gt;&quot;</span><span class="p">,</span>
    <span class="n">eos_token</span><span class="o">=</span><span class="s2">&quot;&lt;/s&gt;&quot;</span><span class="p">,</span>
    <span class="n">line_token</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="n">space_token</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">,</span>
    <span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">,</span>
    <span class="n">mask_token</span><span class="o">=</span><span class="s2">&quot;&lt;mask&gt;&quot;</span><span class="p">,</span>
    <span class="n">pad_token</span><span class="o">=</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">,</span>
    <span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialize a CpmBeeTokenizer object.</span>

<span class="sd">    Args:</span>
<span class="sd">        vocab_file (str): The path to the file containing the vocabulary.</span>
<span class="sd">        bos_token (str, optional): The beginning of sentence token.</span>
<span class="sd">        eos_token (str, optional): The end of sentence token.</span>
<span class="sd">        line_token (str, optional): The token used to represent a new line.</span>
<span class="sd">        space_token (str, optional): The token used to represent a space.</span>
<span class="sd">        unk_token (str, optional): The token used to represent unknown words.</span>
<span class="sd">        mask_token (str, optional): The token used for masking.</span>
<span class="sd">        pad_token (str, optional): The token used for padding.</span>
<span class="sd">        padding_side (str, optional): The side to apply padding.</span>
<span class="sd">        **kwargs: Additional keyword arguments.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        FileNotFoundError: If the vocab_file does not exist.</span>
<span class="sd">        TypeError: If any of the arguments are of incorrect type.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">bos_token</span><span class="o">=</span><span class="n">bos_token</span><span class="p">,</span>
        <span class="n">eos_token</span><span class="o">=</span><span class="n">eos_token</span><span class="p">,</span>
        <span class="n">line_token</span><span class="o">=</span><span class="n">line_token</span><span class="p">,</span>
        <span class="n">space_token</span><span class="o">=</span><span class="n">space_token</span><span class="p">,</span>
        <span class="n">unk_token</span><span class="o">=</span><span class="n">unk_token</span><span class="p">,</span>
        <span class="n">mask_token</span><span class="o">=</span><span class="n">mask_token</span><span class="p">,</span>
        <span class="n">pad_token</span><span class="o">=</span><span class="n">pad_token</span><span class="p">,</span>
        <span class="n">padding_side</span><span class="o">=</span><span class="n">padding_side</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">reader</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
            <span class="n">token</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot; &quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;&lt;/_&gt;&quot;</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;&lt;/n&gt;&quot;</span><span class="p">]</span>
    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;&lt;/_&gt;&quot;</span><span class="p">]</span>
    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;&lt;/n&gt;&quot;</span><span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_max_word_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee_special_tokens</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;&lt;&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">k</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;&gt;&quot;</span><span class="p">)}</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ext_special_tokens</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">ext_args_for_model</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;input_id_subs&quot;</span><span class="p">,</span>
        <span class="s2">&quot;input_pos&quot;</span><span class="p">,</span>
        <span class="s2">&quot;context&quot;</span><span class="p">,</span>
        <span class="s2">&quot;segment_ids&quot;</span><span class="p">,</span>
        <span class="s2">&quot;segment_rel_offset&quot;</span><span class="p">,</span>
        <span class="s2">&quot;segment_rel&quot;</span><span class="p">,</span>
        <span class="s2">&quot;sample_ids&quot;</span><span class="p">,</span>
        <span class="s2">&quot;num_segments&quot;</span><span class="p">,</span>
        <span class="s2">&quot;predict_segments&quot;</span><span class="p">,</span>
        <span class="s2">&quot;answer_placeholders&quot;</span><span class="p">,</span>
        <span class="s2">&quot;ext_table&quot;</span><span class="p">,</span>
        <span class="s2">&quot;token_id_table&quot;</span><span class="p">,</span>
    <span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.__len__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTokenizer</span><span class="o">.</span><span class="fm">__len__</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.__len__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Size of the full vocabulary with the added tokens.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\tokenization_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Size of the full vocabulary with the added tokens.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.check" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTokenizer</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="n">token</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.check" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Checks if a token is present in the encoder.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeTokenizer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer">CpmBeeTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to be checked in the encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Any">Any</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\tokenization_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">check</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if a token is present in the encoder.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CpmBeeTokenizer): An instance of the CpmBeeTokenizer class.</span>
<span class="sd">        token (Any): The token to be checked in the encoder.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.convert_data_to_id" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTokenizer</span><span class="o">.</span><span class="n">convert_data_to_id</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">prev_ext_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shuffle_answer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.convert_data_to_id" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Parse a dict to data ids. Exclusive for CPMBee. It will</p>
<ol>
<li>parse the dict to segments and get segment_rel, which for calculating of position_bias.</li>
<li>tokenize every segment.</li>
</ol>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\tokenization_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span>
<span class="normal">957</span>
<span class="normal">958</span>
<span class="normal">959</span>
<span class="normal">960</span>
<span class="normal">961</span>
<span class="normal">962</span>
<span class="normal">963</span>
<span class="normal">964</span>
<span class="normal">965</span>
<span class="normal">966</span>
<span class="normal">967</span>
<span class="normal">968</span>
<span class="normal">969</span>
<span class="normal">970</span>
<span class="normal">971</span>
<span class="normal">972</span>
<span class="normal">973</span>
<span class="normal">974</span>
<span class="normal">975</span>
<span class="normal">976</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">convert_data_to_id</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">data</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">prev_ext_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_PrevExtTableStates</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shuffle_answer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">max_depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parse a dict to data ids. Exclusive for CPMBee. It will</span>

<span class="sd">    1. parse the dict to segments and get segment_rel, which for calculating of position_bias.</span>
<span class="sd">    2. tokenize every segment.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">root</span><span class="p">:</span> <span class="n">_DictTree</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;&lt;root&gt;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;children&quot;</span><span class="p">:</span> <span class="p">[],</span>
        <span class="s2">&quot;depth&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;segment_id&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;need_predict&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="n">segments</span> <span class="o">=</span> <span class="p">[</span><span class="n">root</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_build_dict_tree</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">CPMBeeInputType</span><span class="p">,</span> <span class="n">depth</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">need_predict</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">_DictTree</span><span class="p">]:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">ret_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">_DictTree</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">curr_items</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">need_predict</span> <span class="ow">and</span> <span class="n">shuffle_answer</span><span class="p">:</span>
                <span class="n">access_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">curr_items</span><span class="p">))</span>
                <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">access_idx</span><span class="p">)</span>
                <span class="n">curr_items</span> <span class="o">=</span> <span class="p">[</span><span class="n">curr_items</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">access_idx</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">curr_items</span><span class="p">:</span>
                <span class="n">child_info</span><span class="p">:</span> <span class="n">_DictTree</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">k</span><span class="p">,</span>
                    <span class="s2">&quot;children&quot;</span><span class="p">:</span> <span class="p">[],</span>
                    <span class="s2">&quot;depth&quot;</span><span class="p">:</span> <span class="n">depth</span><span class="p">,</span>
                    <span class="s2">&quot;segment_id&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">segments</span><span class="p">),</span>
                    <span class="s2">&quot;need_predict&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># only leaves are contexts</span>
                <span class="p">}</span>
                <span class="n">segments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child_info</span><span class="p">)</span>
                <span class="n">child_info</span><span class="p">[</span><span class="s2">&quot;children&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_build_dict_tree</span><span class="p">(</span>
                    <span class="n">v</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">need_predict</span> <span class="ow">or</span> <span class="p">(</span><span class="n">depth</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;&lt;ans&gt;&quot;</span><span class="p">)</span>
                <span class="p">)</span>  <span class="c1"># elements in &lt;root&gt;.&lt;ans&gt;</span>

                <span class="n">ret_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child_info</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">ret_list</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">str</span><span class="p">),</span> <span class="s2">&quot;Invalid data </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">ret</span><span class="p">:</span> <span class="n">_DictTree</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">data</span><span class="p">,</span>
                <span class="s2">&quot;children&quot;</span><span class="p">:</span> <span class="p">[],</span>
                <span class="s2">&quot;depth&quot;</span><span class="p">:</span> <span class="n">depth</span><span class="p">,</span>
                <span class="s2">&quot;segment_id&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">segments</span><span class="p">),</span>
                <span class="s2">&quot;need_predict&quot;</span><span class="p">:</span> <span class="n">need_predict</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="n">segments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">ret</span><span class="p">]</span>

    <span class="n">root</span><span class="p">[</span><span class="s2">&quot;children&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_build_dict_tree</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">num_segments</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">segments</span><span class="p">)</span>
    <span class="n">segment_rel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_segments</span> <span class="o">*</span> <span class="n">num_segments</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_build_segment_rel</span><span class="p">(</span><span class="n">node</span><span class="p">:</span> <span class="n">_DictTree</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
        <span class="n">ret</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[(</span><span class="n">node</span><span class="p">[</span><span class="s2">&quot;segment_id&quot;</span><span class="p">],</span> <span class="n">node</span><span class="p">[</span><span class="s2">&quot;depth&quot;</span><span class="p">])]</span>
        <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">node</span><span class="p">[</span><span class="s2">&quot;children&quot;</span><span class="p">]:</span>
            <span class="n">sub</span> <span class="o">=</span> <span class="n">_build_segment_rel</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">seg_id_1</span><span class="p">,</span> <span class="n">depth_1</span> <span class="ow">in</span> <span class="n">sub</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">seg_id_2</span><span class="p">,</span> <span class="n">depth_2</span> <span class="ow">in</span> <span class="n">ret</span><span class="p">:</span>
                    <span class="n">n_up</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">depth_1</span> <span class="o">-</span> <span class="n">node</span><span class="p">[</span><span class="s2">&quot;depth&quot;</span><span class="p">],</span> <span class="n">max_depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="n">n_down</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">depth_2</span> <span class="o">-</span> <span class="n">node</span><span class="p">[</span><span class="s2">&quot;depth&quot;</span><span class="p">],</span> <span class="n">max_depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="n">segment_rel</span><span class="p">[</span><span class="n">seg_id_1</span> <span class="o">*</span> <span class="n">num_segments</span> <span class="o">+</span> <span class="n">seg_id_2</span><span class="p">]</span> <span class="o">=</span> <span class="n">rel_to_bucket</span><span class="p">(</span>
                        <span class="n">n_up</span><span class="p">,</span> <span class="n">n_down</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span>
                    <span class="p">)</span>
                    <span class="n">segment_rel</span><span class="p">[</span><span class="n">seg_id_2</span> <span class="o">*</span> <span class="n">num_segments</span> <span class="o">+</span> <span class="n">seg_id_1</span><span class="p">]</span> <span class="o">=</span> <span class="n">rel_to_bucket</span><span class="p">(</span>
                        <span class="n">n_down</span><span class="p">,</span> <span class="n">n_up</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span>
                    <span class="p">)</span>
            <span class="n">ret</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">sub</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="n">_build_segment_rel</span><span class="p">(</span><span class="n">root</span><span class="p">)</span>

    <span class="n">input_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">input_id_subs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">segment_bound</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="n">prev_ext_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span> <span class="o">=</span> <span class="n">prev_ext_states</span><span class="p">[</span><span class="s2">&quot;ext_table&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span> <span class="o">=</span> <span class="n">prev_ext_states</span><span class="p">[</span><span class="s2">&quot;token_id_table&quot;</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">seg</span> <span class="ow">in</span> <span class="n">segments</span><span class="p">:</span>
        <span class="c1"># tokenize</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">seg</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">],</span> <span class="n">for_cpmbee</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

        <span class="n">token_id_subs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">reid_token_ids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span><span class="p">:</span>
                <span class="c1"># unk or special token</span>
                <span class="n">token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;&lt;&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">token</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;&gt;&quot;</span><span class="p">):</span>
                    <span class="c1"># special token</span>
                    <span class="k">if</span> <span class="s2">&quot;_&quot;</span> <span class="ow">in</span> <span class="n">token</span><span class="p">:</span>
                        <span class="n">token_name</span> <span class="o">=</span> <span class="n">token</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">,</span> <span class="n">maxsplit</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">token_name</span> <span class="o">=</span> <span class="n">token</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">token_name</span> <span class="o">=</span> <span class="s2">&quot;&lt;</span><span class="si">{}</span><span class="s2">&gt;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">token_name</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">token_name</span> <span class="o">=</span> <span class="s2">&quot;&lt;unk&gt;&quot;</span>

                <span class="k">if</span> <span class="n">token_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span><span class="p">[</span><span class="n">token_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">if</span> <span class="n">idx</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span><span class="p">[</span><span class="n">token_name</span><span class="p">]:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span><span class="p">[</span><span class="n">token_name</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span><span class="p">[</span><span class="n">token_name</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">token_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid token </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>
                <span class="n">reid_token_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="n">token_name</span><span class="p">])</span>
                <span class="n">token_id_subs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span><span class="p">[</span><span class="n">token_name</span><span class="p">][</span><span class="n">idx</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">reid_token_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
                <span class="n">token_id_subs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">bos_token_id</span><span class="p">]</span> <span class="o">+</span> <span class="n">reid_token_ids</span>
        <span class="n">token_id_subs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">token_id_subs</span>
        <span class="c1"># eos_id 表示 no need_predict</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">seg</span><span class="p">[</span><span class="s2">&quot;need_predict&quot;</span><span class="p">]:</span>  <span class="c1"># eos</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span>
            <span class="n">token_id_subs</span> <span class="o">=</span> <span class="n">token_id_subs</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># no eos</span>
            <span class="k">pass</span>
        <span class="n">begin</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">input_ids</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="n">input_id_subs</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">token_id_subs</span><span class="p">)</span>
        <span class="n">end</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">segment_bound</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">))</span>

    <span class="n">ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">id_subs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_id_subs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">segs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>  <span class="c1"># 按segment_bound对seg编号</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">segment_bound</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">segments</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;need_predict&quot;</span><span class="p">]:</span>
            <span class="n">context</span><span class="p">[</span><span class="n">begin</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">segs</span><span class="p">[</span><span class="n">begin</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>

    <span class="n">curr_ext_table_states</span><span class="p">:</span> <span class="n">_PrevExtTableStates</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;ext_table&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span><span class="p">,</span>
        <span class="s2">&quot;token_id_table&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_id_table</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">ids</span><span class="p">,</span> <span class="n">id_subs</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">segs</span><span class="p">,</span> <span class="n">segment_rel</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">,</span> <span class="n">curr_ext_table_states</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.convert_tokens_to_string" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_string</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.convert_tokens_to_string" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Converts a list of tokens into a single string.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeTokenizer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer">CpmBeeTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of tokens to be converted into a string.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[str]</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>str</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A string representation of the tokens.</p>
              </div>
                <p>
                  <span class="doc-returns-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
                </p>
            </td>
          </tr>
      </tbody>
    </table>
        <p>This method takes in two parameters, self and tokens. The self parameter is an instance of the CpmBeeTokenizer
class and is used to access the class's attributes and methods. The tokens parameter is a
list of strings representing individual tokens.</p>
<p>The function returns a string that is obtained by concatenating all the tokens together using the ''.join() method.
This method does not modify the original list of tokens.</p>
<p>No exceptions are raised by this method.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\tokenization_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">convert_tokens_to_string</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a list of tokens into a single string.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CpmBeeTokenizer): An instance of the CpmBeeTokenizer class.</span>
<span class="sd">        tokens (List[str]): A list of tokens to be converted into a string.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: A string representation of the tokens.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>

<span class="sd">    This method takes in two parameters, self and tokens. The self parameter is an instance of the CpmBeeTokenizer</span>
<span class="sd">    class and is used to access the class&#39;s attributes and methods. The tokens parameter is a</span>
<span class="sd">    list of strings representing individual tokens.</span>

<span class="sd">    The function returns a string that is obtained by concatenating all the tokens together using the &#39;&#39;.join() method.</span>
<span class="sd">    This method does not modify the original list of tokens.</span>

<span class="sd">    No exceptions are raised by this method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.get_piece" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTokenizer</span><span class="o">.</span><span class="n">get_piece</span><span class="p">(</span><span class="n">text</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.get_piece" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Match with maximum length.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\tokenization_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_piece</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Match with maximum length.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">len_text</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)):</span>
        <span class="n">sub</span> <span class="o">=</span> <span class="n">text</span><span class="p">[:</span> <span class="n">len_text</span> <span class="o">-</span> <span class="n">i</span><span class="p">]</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">sub</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">sub</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">sub</span>
    <span class="k">return</span> <span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.get_vocab" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTokenizer</span><span class="o">.</span><span class="n">get_vocab</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.get_vocab" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Get the vocabulary of the CpmBeeTokenizer instance.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CpmBeeTokenizer class.
This parameter represents the current instance of the tokenizer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer">CpmBeeTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>dict</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A dictionary containing the combined encoder and added tokens encoder.
The keys represent tokens, and the values represent their corresponding IDs.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\tokenization_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the vocabulary of the CpmBeeTokenizer instance.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CpmBeeTokenizer): The instance of the CpmBeeTokenizer class.</span>
<span class="sd">            This parameter represents the current instance of the tokenizer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: A dictionary containing the combined encoder and added tokens encoder.</span>
<span class="sd">            The keys represent tokens, and the values represent their corresponding IDs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.prepare_for_finetune" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTokenizer</span><span class="o">.</span><span class="n">prepare_for_finetune</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">2048</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.prepare_for_finetune" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Prepares the input data for fine-tuning.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CpmBeeTokenizer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer">CpmBeeTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>data_list</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of dictionaries containing the input data.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="typing.Dict">Dict</span>]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_length</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum length of the input data. Defaults to 2048.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2048</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\tokenization_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">prepare_for_finetune</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">data_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prepares the input data for fine-tuning.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CpmBeeTokenizer): The instance of the CpmBeeTokenizer class.</span>
<span class="sd">        data_list (List[Dict]): A list of dictionaries containing the input data.</span>
<span class="sd">        max_length (int, optional): The maximum length of the input data. Defaults to 2048.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">_inputs_sub</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">_context</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">_sample_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">_segments</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">_num_segments</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">_segment_rel_offset</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">_segment_rel</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">_spans</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">_raw_data</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">raw_data</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">data_list</span><span class="p">:</span>
        <span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">input_id_subs</span><span class="p">,</span>
            <span class="n">context</span><span class="p">,</span>
            <span class="n">segment_ids</span><span class="p">,</span>
            <span class="n">segment_rel</span><span class="p">,</span>
            <span class="n">n_segments</span><span class="p">,</span>
            <span class="n">_</span>
        <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_data_to_id</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:</span> <span class="n">max_length</span><span class="p">]</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[:</span> <span class="n">max_length</span><span class="p">]</span>
        <span class="n">segment_ids</span> <span class="o">=</span> <span class="n">segment_ids</span><span class="p">[:</span> <span class="n">max_length</span><span class="p">]</span>
        <span class="n">raw_data</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">raw_data</span><span class="p">[</span><span class="s2">&quot;samples&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">sample_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">segment_rel_offset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">num_segments</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">n_segments</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

        <span class="n">_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">_inputs_sub</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_id_subs</span><span class="p">)</span>
        <span class="n">_context</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="n">_sample_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample_ids</span><span class="p">)</span>
        <span class="n">_segments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">)</span>
        <span class="n">_num_segments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">num_segments</span><span class="p">)</span>
        <span class="n">_segment_rel_offset</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">segment_rel_offset</span><span class="p">)</span>
        <span class="n">_segment_rel</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">segment_rel</span><span class="p">)</span>
        <span class="n">_spans</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
        <span class="n">_raw_data</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">raw_data</span><span class="p">])</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">_inputs</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">inputs_sub</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
    <span class="n">sample_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">segments</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">num_segments</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">segment_rel_offset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">tgt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">max_rel</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">max_rel</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_rel</span><span class="p">,</span> <span class="n">_segment_rel</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">segment_rel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_rel</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">spans</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">length</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">batch_ext_table_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">batch_ext_table_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">batch_ext_table_sub</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">raw_data_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">instance_length</span> <span class="o">=</span> <span class="n">_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">rel_size</span> <span class="o">=</span> <span class="n">_segment_rel</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">instance_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">inputs_sub</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">instance_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">_inputs_sub</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">context</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">instance_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">_context</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">sample_ids</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">instance_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">_sample_ids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">segments</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">instance_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">_segments</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">num_segments</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">instance_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">_num_segments</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">segment_rel_offset</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">instance_length</span><span class="p">]</span> <span class="o">=</span> <span class="n">_segment_rel_offset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">segment_rel</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">rel_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">_segment_rel</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="n">span_begin</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">span_id</span><span class="p">,</span> <span class="n">span_end</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">_spans</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
            <span class="n">spans</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">span_begin</span><span class="p">:</span><span class="n">span_end</span><span class="p">]</span> <span class="o">=</span> <span class="n">span_id</span>
            <span class="n">span_begin</span> <span class="o">=</span> <span class="n">span_end</span>
        <span class="n">length</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">instance_length</span>
        <span class="n">raw_data_list</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">_raw_data</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">instance_length</span><span class="p">):</span>
            <span class="n">idx</span><span class="p">,</span> <span class="n">idx_sub</span> <span class="o">=</span> <span class="n">_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="n">_inputs_sub</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
            <span class="n">tgt_idx</span> <span class="o">=</span> <span class="n">idx</span>
            <span class="k">if</span> <span class="n">idx_sub</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># need to be in ext table</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_sub</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">batch_ext_table_map</span><span class="p">:</span>
                    <span class="n">batch_ext_table_map</span><span class="p">[(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_sub</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_ext_table_map</span><span class="p">)</span>
                    <span class="n">batch_ext_table_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
                    <span class="n">batch_ext_table_sub</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx_sub</span><span class="p">)</span>
                <span class="n">tgt_idx</span> <span class="o">=</span> <span class="n">batch_ext_table_map</span><span class="p">[(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_sub</span><span class="p">)]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span>
            <span class="k">if</span> <span class="n">j</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">context</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">idx</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bos_token_id</span><span class="p">:</span>
                    <span class="n">tgt</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">tgt_idx</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">tgt</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span>
        <span class="k">if</span> <span class="n">context</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">instance_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">tgt</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">instance_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_ext_table_map</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># placeholder</span>
        <span class="n">batch_ext_table_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">batch_ext_table_sub</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">BatchEncoding</span><span class="p">({</span>
        <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">inputs</span><span class="p">,</span>
        <span class="s2">&quot;input_id_sub&quot;</span><span class="p">:</span> <span class="n">inputs_sub</span><span class="p">,</span>
        <span class="s2">&quot;length&quot;</span><span class="p">:</span> <span class="n">length</span><span class="p">,</span>
        <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">context</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;sample_ids&quot;</span><span class="p">:</span> <span class="n">sample_ids</span><span class="p">,</span>
        <span class="s2">&quot;num_segments&quot;</span><span class="p">:</span> <span class="n">num_segments</span><span class="p">,</span>
        <span class="s2">&quot;segment&quot;</span><span class="p">:</span> <span class="n">segments</span><span class="p">,</span>
        <span class="s2">&quot;segment_rel_offset&quot;</span><span class="p">:</span> <span class="n">segment_rel_offset</span><span class="p">,</span>
        <span class="s2">&quot;segment_rel&quot;</span><span class="p">:</span> <span class="n">segment_rel</span><span class="p">,</span>
        <span class="s2">&quot;span&quot;</span><span class="p">:</span> <span class="n">spans</span><span class="p">,</span>
        <span class="s2">&quot;labels&quot;</span><span class="p">:</span> <span class="n">tgt</span><span class="p">,</span>
        <span class="s2">&quot;ext_table_ids&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_ext_table_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
        <span class="s2">&quot;ext_table_sub&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_ext_table_sub</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="p">},</span> <span class="n">tensor_type</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.prepare_for_model" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTokenizer</span><span class="o">.</span><span class="n">prepare_for_model</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_special_tokens_mask</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_length</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prepend_batch_axis</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.prepare_for_model" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It
adds special tokens, truncates sequences if overflowing while taking into account the special tokens and
manages a moving window (with user defined stride) for overflowing tokens. Please Note, for <em>pair_ids</em>
different than <code>None</code> and <em>truncation_strategy = longest_first</em> or <code>True</code>, it is not possible to return
overflowing tokens. Such a combination of arguments will raise an error.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tokenized input ids of the first sequence. Can be obtained from a string by chaining the <code>tokenize</code> and
<code>convert_tokens_to_ids</code> methods.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pair_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tokenized input ids of the second sequence. Can be obtained from a string by chaining the <code>tokenize</code>
and <code>convert_tokens_to_ids</code> methods.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[int]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\tokenization_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">prepare_for_model</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">pair_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">truncation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">return_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">prepend_batch_axis</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It</span>
<span class="sd">    adds special tokens, truncates sequences if overflowing while taking into account the special tokens and</span>
<span class="sd">    manages a moving window (with user defined stride) for overflowing tokens. Please Note, for *pair_ids*</span>
<span class="sd">    different than `None` and *truncation_strategy = longest_first* or `True`, it is not possible to return</span>
<span class="sd">    overflowing tokens. Such a combination of arguments will raise an error.</span>

<span class="sd">    Args:</span>
<span class="sd">        ids (`List[int]`):</span>
<span class="sd">            Tokenized input ids of the first sequence. Can be obtained from a string by chaining the `tokenize` and</span>
<span class="sd">            `convert_tokens_to_ids` methods.</span>
<span class="sd">        pair_ids (`List[int]`, *optional*):</span>
<span class="sd">            Tokenized input ids of the second sequence. Can be obtained from a string by chaining the `tokenize`</span>
<span class="sd">            and `convert_tokens_to_ids` methods.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Backward compatibility for &#39;truncation_strategy&#39;, &#39;pad_to_max_length&#39;</span>
    <span class="n">padding_strategy</span><span class="p">,</span> <span class="n">truncation_strategy</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_padding_truncation_strategies</span><span class="p">(</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">pair</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">pair_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">len_ids</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
    <span class="n">len_pair_ids</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">pair</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">return_token_type_ids</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">add_special_tokens</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Asking to return token_type_ids while setting add_special_tokens to False &quot;</span>
            <span class="s2">&quot;results in an undefined behavior. Please set add_special_tokens to True or &quot;</span>
            <span class="s2">&quot;set return_token_type_ids to None.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">return_overflowing_tokens</span>
        <span class="ow">and</span> <span class="n">truncation_strategy</span> <span class="o">==</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">LONGEST_FIRST</span>
        <span class="ow">and</span> <span class="n">pair_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Not possible to return overflowing tokens for pair of sequences with the &quot;</span>
            <span class="s2">&quot;`longest_first`. Please select another truncation strategy than `longest_first`, &quot;</span>
            <span class="s2">&quot;for instance `only_second` or `only_first`.&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Load from model defaults</span>
    <span class="k">if</span> <span class="n">return_token_type_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">return_token_type_ids</span> <span class="o">=</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span>
    <span class="k">if</span> <span class="n">return_attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span>

    <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Compute the total size of the returned encodings</span>
    <span class="n">total_len</span> <span class="o">=</span> <span class="n">len_ids</span> <span class="o">+</span> <span class="n">len_pair_ids</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_special_tokens_to_add</span><span class="p">(</span><span class="n">pair</span><span class="o">=</span><span class="n">pair</span><span class="p">)</span> <span class="k">if</span> <span class="n">add_special_tokens</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Truncation: Handle max sequence length</span>
    <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">truncation_strategy</span> <span class="o">!=</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">DO_NOT_TRUNCATE</span> <span class="ow">and</span> <span class="n">max_length</span> <span class="ow">and</span> <span class="n">total_len</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
        <span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">,</span> <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncate_sequences</span><span class="p">(</span>
            <span class="n">ids</span><span class="p">,</span>
            <span class="n">pair_ids</span><span class="o">=</span><span class="n">pair_ids</span><span class="p">,</span>
            <span class="n">num_tokens_to_remove</span><span class="o">=</span><span class="n">total_len</span> <span class="o">-</span> <span class="n">max_length</span><span class="p">,</span>
            <span class="n">truncation_strategy</span><span class="o">=</span><span class="n">truncation_strategy</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">return_overflowing_tokens</span><span class="p">:</span>
        <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;overflowing_tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">overflowing_tokens</span>
        <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;num_truncated_tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_len</span> <span class="o">-</span> <span class="n">max_length</span>

    <span class="c1"># Add special tokens</span>
    <span class="k">if</span> <span class="n">add_special_tokens</span><span class="p">:</span>
        <span class="n">sequence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_inputs_with_special_tokens</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">)</span>
        <span class="n">token_type_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_token_type_ids_from_sequences</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sequence</span> <span class="o">=</span> <span class="n">ids</span> <span class="o">+</span> <span class="n">pair_ids</span> <span class="k">if</span> <span class="n">pair</span> <span class="k">else</span> <span class="n">ids</span>
        <span class="n">token_type_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">+</span> <span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">pair</span> <span class="k">else</span> <span class="p">[])</span>

    <span class="c1"># Build output dictionary</span>
    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sequence</span>
    <span class="k">if</span> <span class="n">return_token_type_ids</span><span class="p">:</span>
        <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_type_ids</span>
    <span class="k">if</span> <span class="n">return_special_tokens_mask</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">add_special_tokens</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_special_tokens_mask</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>

    <span class="c1"># Check lengths</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_eventual_warn_about_too_long_sequence</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

    <span class="c1"># Padding</span>
    <span class="k">if</span> <span class="n">padding_strategy</span> <span class="o">!=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span> <span class="ow">or</span> <span class="n">return_attention_mask</span><span class="p">:</span>
        <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
            <span class="n">encoded_inputs</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding_strategy</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
            <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
            <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">return_length</span><span class="p">:</span>
        <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>

    <span class="c1"># for CPMBee, encode all the model arguments</span>
    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ext_args_for_model</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

    <span class="n">batch_outputs</span> <span class="o">=</span> <span class="n">BatchEncoding</span><span class="p">(</span>
        <span class="n">encoded_inputs</span><span class="p">,</span> <span class="n">tensor_type</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span> <span class="n">prepend_batch_axis</span><span class="o">=</span><span class="n">prepend_batch_axis</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">batch_outputs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.save_vocabulary" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTokenizer</span><span class="o">.</span><span class="n">save_vocabulary</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.save_vocabulary" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Save the vocabulary to a file.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CpmBeeTokenizer class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer">CpmBeeTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The directory where the vocabulary file will be saved.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>filename_prefix</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An optional prefix to prepend to the filename. Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Tuple">Tuple</span>[str]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Tuple[str]: A tuple containing the path to the saved vocabulary file.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>IOError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If there is an issue with reading or writing the vocabulary file.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the provided save_directory is not a valid directory.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>KeyError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If any of the keys used for encoding tokens are not found in the encoder dictionary.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\tokenization_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">save_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the vocabulary to a file.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CpmBeeTokenizer): The instance of the CpmBeeTokenizer class.</span>
<span class="sd">        save_directory (str): The directory where the vocabulary file will be saved.</span>
<span class="sd">        filename_prefix (Optional[str]): An optional prefix to prepend to the filename. Default is None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[str]: A tuple containing the path to the saved vocabulary file.</span>

<span class="sd">    Raises:</span>
<span class="sd">        IOError: If there is an issue with reading or writing the vocabulary file.</span>
<span class="sd">        ValueError: If the provided save_directory is not a valid directory.</span>
<span class="sd">        KeyError: If any of the keys used for encoding tokens are not found in the encoder dictionary.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
        <span class="n">vocab_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">save_directory</span><span class="p">,</span> <span class="p">(</span><span class="n">filename_prefix</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="k">if</span> <span class="n">filename_prefix</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">VOCAB_FILES_NAMES</span><span class="p">[</span><span class="s2">&quot;vocab_file&quot;</span><span class="p">]</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">vocab_file</span> <span class="o">=</span> <span class="p">(</span><span class="n">filename_prefix</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="k">if</span> <span class="n">filename_prefix</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">save_directory</span>
    <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;&lt;/n&gt;&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">]</span>
    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot;&lt;/_&gt;&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot; &quot;</span><span class="p">]</span>
    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="s2">&quot; &quot;</span><span class="p">]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">token_index</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="k">if</span> <span class="n">index</span> <span class="o">!=</span> <span class="n">token_index</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Saving vocabulary to </span><span class="si">{</span><span class="n">vocab_file</span><span class="si">}</span><span class="s2">: vocabulary indices are not consecutive.&quot;</span>
                    <span class="s2">&quot; Please check that the vocabulary is not corrupted!&quot;</span>
                <span class="p">)</span>
                <span class="n">index</span> <span class="o">=</span> <span class="n">token_index</span>
            <span class="n">writer</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">token</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">vocab_file</span><span class="p">,)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.tokenize" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer.tokenize" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Override the <code>tokenize</code> to meet the needs of CPMBee:</p>
<ol>
<li>Mark the special token with <code>&lt;</code> and <code>&gt;</code>. The <code>&lt;&gt;</code> will be ignored.</li>
<li>Split sentences by the marked special tokens.</li>
<li>Record the marked special token by <code>ext_table</code> and <code>ext_table_rev</code>.</li>
<li>Tokenize the sentence without special tokens.</li>
</ol>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\tokenization_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="n">TextInput</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Override the `tokenize` to meet the needs of CPMBee:</span>

<span class="sd">    1. Mark the special token with `&lt;` and `&gt;`. The `&lt;&gt;` will be ignored.</span>
<span class="sd">    2. Split sentences by the marked special tokens.</span>
<span class="sd">    3. Record the marked special token by `ext_table` and `ext_table_rev`.</span>
<span class="sd">    4. Tokenize the sentence without special tokens.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">for_cpmbee</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;for_cpmbee&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">all_special_tokens_extended</span> <span class="o">=</span> <span class="p">{</span>
        <span class="nb">str</span><span class="p">(</span><span class="n">t</span><span class="p">):</span> <span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_tokens_extended</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="n">sentence_split</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">]</span>
    <span class="n">is_special_token</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">is_special_token</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="s2">&quot;&lt;&quot;</span><span class="p">:</span>
                <span class="n">tail</span> <span class="o">=</span> <span class="n">sentence_split</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">sentence_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">tail</span>
                <span class="n">sentence_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
                <span class="n">is_special_token</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">elif</span> <span class="n">c</span> <span class="o">==</span> <span class="s2">&quot;&gt;&quot;</span><span class="p">:</span>
                <span class="c1"># end of special token</span>
                <span class="n">sentence_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">c</span>
                <span class="k">if</span> <span class="n">sentence_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;&lt;&gt;&quot;</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">is_special_token</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">sentence_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">sentence_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">c</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="s2">&quot;&lt;&quot;</span><span class="p">:</span>
                <span class="n">is_special_token</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">sentence_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">sentence_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">c</span>
    <span class="k">if</span> <span class="n">is_special_token</span><span class="p">:</span>
        <span class="n">tail</span> <span class="o">=</span> <span class="n">sentence_split</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sentence_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">tail</span>

    <span class="n">output_tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">part</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sentence_split</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&amp;</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># special token</span>
            <span class="n">output_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">part</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">for_cpmbee</span> <span class="ow">and</span> <span class="p">(</span><span class="n">part</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">part</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">[</span><span class="n">part</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ext_table</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ext_table_rev</span><span class="p">[</span><span class="n">part</span><span class="p">]]</span> <span class="o">=</span> <span class="n">part</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output_tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tokenize</span><span class="p">(</span><span class="n">part</span><span class="p">,</span> <span class="n">for_cpmbee</span><span class="o">=</span><span class="n">for_cpmbee</span><span class="p">))</span>

    <span class="c1"># drop spaces</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">added_tokens_encoder</span><span class="p">:</span>
            <span class="n">token</span> <span class="o">=</span> <span class="n">all_special_tokens_extended</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">left</span> <span class="o">=</span> <span class="n">output_tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">right</span> <span class="o">=</span> <span class="n">output_tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">rstrip</span> <span class="ow">and</span> <span class="n">right</span><span class="p">:</span>
                    <span class="c1"># A bit counter-intuitive but we strip the left of the string</span>
                    <span class="c1"># since tok_extended.rstrip means the special token is eating all white spaces on its right</span>
                    <span class="n">output_tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">right</span><span class="o">.</span><span class="n">lstrip</span><span class="p">()</span>
                <span class="c1"># Strip white spaces on the left</span>
                <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">lstrip</span> <span class="ow">and</span> <span class="n">left</span><span class="p">:</span>
                    <span class="n">output_tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">left</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span>  <span class="c1"># Opposite here</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">right</span><span class="p">:</span>
                    <span class="n">output_tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">right</span><span class="o">.</span><span class="n">lstrip</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">left</span><span class="p">:</span>
                    <span class="n">output_tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">left</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span>

    <span class="n">skipped_tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">output_tokens</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">token</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">skipped_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">skipped_tokens</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.rel_to_bucket" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">tokenization_cpmbee</span><span class="o">.</span><span class="n">rel_to_bucket</span><span class="p">(</span><span class="n">n_up</span><span class="p">,</span> <span class="n">n_down</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.rel_to_bucket" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Calculates the relative position of an item in a bucket based on the number of items above and below it.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>n_up</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of items above the item.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_down</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of items below the item.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_depth</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum depth of the bucket. Defaults to 8.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>int</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The relative position of the item in the bucket.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\tokenization_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">rel_to_bucket</span><span class="p">(</span><span class="n">n_up</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_down</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the relative position of an item in a bucket based on the number of items above and below it.</span>

<span class="sd">    Args:</span>
<span class="sd">        n_up (int): The number of items above the item.</span>
<span class="sd">        n_down (int): The number of items below the item.</span>
<span class="sd">        max_depth (int, optional): The maximum depth of the bucket. Defaults to 8.</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: The relative position of the item in the bucket.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">n_up</span> <span class="o">*</span> <span class="n">max_depth</span> <span class="o">+</span> <span class="n">n_down</span>
    <span class="k">if</span> <span class="n">ret</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ret</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># bucket 1 is reserved for incontext samples</span>
        <span class="k">return</span> <span class="n">ret</span> <span class="o">+</span> <span class="mi">1</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents first">

        <p>MindSpore CpmBee model.</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This class represents the attention mechanism used in the CpmBee model. It inherits from the nn.Module class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.dim_model">dim_model</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The hidden size of the model.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.num_heads">num_heads</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The number of attention heads.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.dim_head">dim_head</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The dimension of each attention head.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.project_q">project_q</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Linear layer for projecting the query.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear">CpmBeeLinear</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.project_k">project_k</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Linear layer for projecting the key.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear">CpmBeeLinear</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.project_v">project_v</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Linear layer for projecting the value.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear">CpmBeeLinear</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.attention_out">attention_out</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Linear layer for the output of the attention mechanism.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear">CpmBeeLinear</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.softmax">softmax</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Softmax function for computing attention weights.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.Softmax">Softmax</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.dropout">dropout</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Dropout layer for regularization (optional).</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.Dropout">Dropout</span> or None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.__init__" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the CpmBeeAttention class.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.forward" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the attention mechanism.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents the attention mechanism used in the CpmBee model. It inherits from the nn.Module class.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        dim_model (int): The hidden size of the model.</span>
<span class="sd">        num_heads (int): The number of attention heads.</span>
<span class="sd">        dim_head (int): The dimension of each attention head.</span>
<span class="sd">        project_q (CpmBeeLinear): Linear layer for projecting the query.</span>
<span class="sd">        project_k (CpmBeeLinear): Linear layer for projecting the key.</span>
<span class="sd">        project_v (CpmBeeLinear): Linear layer for projecting the value.</span>
<span class="sd">        attention_out (CpmBeeLinear): Linear layer for the output of the attention mechanism.</span>
<span class="sd">        softmax (nn.Softmax): Softmax function for computing attention weights.</span>
<span class="sd">        dropout (nn.Dropout or None): Dropout layer for regularization (optional).</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__:</span>
<span class="sd">            Initializes the CpmBeeAttention class.</span>

<span class="sd">        forward:</span>
<span class="sd">            Constructs the attention mechanism.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the CpmBeeAttention class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            config (CpmBeeConfig):</span>
<span class="sd">                The configuration object containing the following attributes:</span>

<span class="sd">                - hidden_size (int): The dimension of the model.</span>
<span class="sd">                - num_attention_heads (int): The number of attention heads.</span>
<span class="sd">                - dim_head (int): The dimension of each attention head.</span>
<span class="sd">                - ms_dtype: The data type used for the linear layers.</span>
<span class="sd">                - dropout_p (float, optional): The probability of an element to be zeroed during dropout.</span>
<span class="sd">                If not provided, no dropout is applied.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dim_head</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">project_q</span> <span class="o">=</span> <span class="n">CpmBeeLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">project_k</span> <span class="o">=</span> <span class="n">CpmBeeLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">project_v</span> <span class="o">=</span> <span class="n">CpmBeeLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attention_out</span> <span class="o">=</span> <span class="n">CpmBeeLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_q</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">hidden_kv</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">position_bias</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_q (`mindspore.Tensor`):</span>
<span class="sd">                Input of transformer block(self-attention block). It can be the raw embedding of a batch of sequences.</span>
<span class="sd">            hidden_kv (`mindspore.Tensor` of shape `(batch, len_k, dim_model)`)):</span>
<span class="sd">                Tensor *key_value* and *query* of shape `(batch, len_k, dim_model)`</span>
<span class="sd">            attention_mask (`mindspore.Tensor` of shape `(batch, len_seq, len_seq)`):</span>
<span class="sd">                Avoid invalid areas to participate in the calculation of self-attention.</span>
<span class="sd">            position_bias (`mindspore.Tensor` of shape `(batch, len_seq, len_seq)`):</span>
<span class="sd">                Provide positional information to self-attention block.</span>
<span class="sd">            output_attentions (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers.</span>
<span class="sd">            past_key_values (`Tuple[mindspore.Tensor, mindspore.Tensor]`, *optional*):</span>
<span class="sd">                Cached past key and value projection states.</span>
<span class="sd">            use_cache (`bool`, *optional*):</span>
<span class="sd">                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding</span>
<span class="sd">                (see `past_key_values`).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hidden_q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">len_q</span> <span class="o">=</span> <span class="n">hidden_q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">len_k</span> <span class="o">=</span> <span class="n">hidden_kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">project_q</span><span class="p">(</span><span class="n">hidden_q</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">project_k</span><span class="p">(</span><span class="n">hidden_kv</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">project_v</span><span class="p">(</span><span class="n">hidden_kv</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">len_k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">len_k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_key_values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">key</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_key_values</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">value</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">len_k</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>

        <span class="c1"># (batch_size, num_heads, len_q, dim_head) @ (batch_size, num_heads, dim_head, len_k) -&gt; (batch_size, num_heads, len_q, len_k)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">score</span> <span class="o">+</span> <span class="n">position_bias</span>

        <span class="n">score</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
            <span class="n">score</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="n">len_k</span><span class="p">)</span> <span class="o">==</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span>
            <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

        <span class="n">score</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
            <span class="n">score</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="n">len_k</span><span class="p">)</span> <span class="o">==</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span>
            <span class="mf">0.</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">score</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

        <span class="c1"># (batch_size, num_heads, len_q, len_k) @ (batch_size, num_heads, len_k, dim_head) -&gt; (batch_size, num_heads, len_q, dim_head)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

        <span class="n">score</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">)</span>

        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_out</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

        <span class="n">past_key_values</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">past_key_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">score</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">past_key_values</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeAttention</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the CpmBeeAttention class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object containing the following attributes:</p>
<ul>
<li>hidden_size (int): The dimension of the model.</li>
<li>num_attention_heads (int): The number of attention heads.</li>
<li>dim_head (int): The dimension of each attention head.</li>
<li>ms_dtype: The data type used for the linear layers.</li>
<li>dropout_p (float, optional): The probability of an element to be zeroed during dropout.
If not provided, no dropout is applied.</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig">CpmBeeConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the CpmBeeAttention class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        config (CpmBeeConfig):</span>
<span class="sd">            The configuration object containing the following attributes:</span>

<span class="sd">            - hidden_size (int): The dimension of the model.</span>
<span class="sd">            - num_attention_heads (int): The number of attention heads.</span>
<span class="sd">            - dim_head (int): The dimension of each attention head.</span>
<span class="sd">            - ms_dtype: The data type used for the linear layers.</span>
<span class="sd">            - dropout_p (float, optional): The probability of an element to be zeroed during dropout.</span>
<span class="sd">            If not provided, no dropout is applied.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dim_head</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">project_q</span> <span class="o">=</span> <span class="n">CpmBeeLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">project_k</span> <span class="o">=</span> <span class="n">CpmBeeLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">project_v</span> <span class="o">=</span> <span class="n">CpmBeeLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">attention_out</span> <span class="o">=</span> <span class="n">CpmBeeLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_model</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeAttention</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_q</span><span class="p">,</span> <span class="n">hidden_kv</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">position_bias</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>hidden_q</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input of transformer block(self-attention block). It can be the raw embedding of a batch of sequences.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_kv</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tensor <em>key_value</em> and <em>query</em> of shape <code>(batch, len_k, dim_model)</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch, len_k, dim_model)`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Avoid invalid areas to participate in the calculation of self-attention.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch, len_seq, len_seq)`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Provide positional information to self-attention block.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch, len_seq, len_seq)`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the attentions tensors of all attention layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_key_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Cached past key and value projection states.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple[mindspore.Tensor, mindspore.Tensor]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding
(see <code>past_key_values</code>).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_q</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">hidden_kv</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">position_bias</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        hidden_q (`mindspore.Tensor`):</span>
<span class="sd">            Input of transformer block(self-attention block). It can be the raw embedding of a batch of sequences.</span>
<span class="sd">        hidden_kv (`mindspore.Tensor` of shape `(batch, len_k, dim_model)`)):</span>
<span class="sd">            Tensor *key_value* and *query* of shape `(batch, len_k, dim_model)`</span>
<span class="sd">        attention_mask (`mindspore.Tensor` of shape `(batch, len_seq, len_seq)`):</span>
<span class="sd">            Avoid invalid areas to participate in the calculation of self-attention.</span>
<span class="sd">        position_bias (`mindspore.Tensor` of shape `(batch, len_seq, len_seq)`):</span>
<span class="sd">            Provide positional information to self-attention block.</span>
<span class="sd">        output_attentions (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the attentions tensors of all attention layers.</span>
<span class="sd">        past_key_values (`Tuple[mindspore.Tensor, mindspore.Tensor]`, *optional*):</span>
<span class="sd">            Cached past key and value projection states.</span>
<span class="sd">        use_cache (`bool`, *optional*):</span>
<span class="sd">            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding</span>
<span class="sd">            (see `past_key_values`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hidden_q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">len_q</span> <span class="o">=</span> <span class="n">hidden_q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">len_k</span> <span class="o">=</span> <span class="n">hidden_kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">project_q</span><span class="p">(</span><span class="n">hidden_q</span><span class="p">)</span>
    <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">project_k</span><span class="p">(</span><span class="n">hidden_kv</span><span class="p">)</span>
    <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">project_v</span><span class="p">(</span><span class="n">hidden_kv</span><span class="p">)</span>

    <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">len_k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">len_k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_key_values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">key</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_key_values</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">value</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">len_k</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>

    <span class="c1"># (batch_size, num_heads, len_q, dim_head) @ (batch_size, num_heads, dim_head, len_k) -&gt; (batch_size, num_heads, len_q, len_k)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">score</span> <span class="o">+</span> <span class="n">position_bias</span>

    <span class="n">score</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
        <span class="n">score</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="n">len_k</span><span class="p">)</span> <span class="o">==</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span>
        <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

    <span class="n">score</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
        <span class="n">score</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="n">len_k</span><span class="p">)</span> <span class="o">==</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span>
        <span class="mf">0.</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">score</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

    <span class="c1"># (batch_size, num_heads, len_q, len_k) @ (batch_size, num_heads, len_k, dim_head) -&gt; (batch_size, num_heads, len_q, dim_head)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="n">score</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_head</span><span class="p">)</span>

    <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_out</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

    <span class="n">past_key_values</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
        <span class="n">past_key_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">score</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">past_key_values</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.beam_search.BeamHypotheses" href="../../../../../api/transformers/generation/beam_search/#mindnlp.transformers.generation.beam_search.BeamHypotheses">BeamHypotheses</a></code></p>


        <p>This class represents a set of beam hypotheses for the CpmBee model. It is derived from the BeamHypotheses class.</p>
<p>The CpmBeeBeamHypotheses class is used to store and manage a list of beam hypotheses along with their scores
and beam indices. Each hypothesis consists of a sequence of predicted tokens and a corresponding sum of log
probabilities. The class provides methods to add new hypotheses, update the list of hypotheses, and retrieve
the best hypotheses based on their scores.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses.beams">beams</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A list of tuples representing the beam hypotheses.
Each tuple contains the hypothesis score, the predicted token sequence, and the beam indices.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="typing.Tuple">Tuple</span>[float, <span title="typing.List">List</span>, <span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]]]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses.worst_score">worst_score</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The score of the worst hypothesis in the list.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses.num_beams">num_beams</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The maximum number of beam hypotheses to be stored.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses.length_penalty">length_penalty</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The length penalty factor applied to the hypothesis scores.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses.add" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses.add">add</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Add a new hypothesis to the list of beam hypotheses. The hypothesis is represented by a sequence of
predicted tokens and its sum of log probabilities. Optionally, the beam indices can also be provided.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses.update">update</span></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Update the list of beam hypotheses by removing the worst hypothesis if the maximum number of hypotheses
is exceeded.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses.get_best">get_best</span></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Retrieve the best <code>num_best</code> beam hypotheses based on their scores. The hypotheses are returned as a list
of tuples, where each tuple contains the hypothesis score, the predicted token sequence, and the beam indices.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeBeamHypotheses</span><span class="p">(</span><span class="n">BeamHypotheses</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents a set of beam hypotheses for the CpmBee model. It is derived from the BeamHypotheses class.</span>

<span class="sd">    The CpmBeeBeamHypotheses class is used to store and manage a list of beam hypotheses along with their scores</span>
<span class="sd">    and beam indices. Each hypothesis consists of a sequence of predicted tokens and a corresponding sum of log</span>
<span class="sd">    probabilities. The class provides methods to add new hypotheses, update the list of hypotheses, and retrieve</span>
<span class="sd">    the best hypotheses based on their scores.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        beams (List[Tuple[float, List, Optional[mindspore.Tensor]]]): A list of tuples representing the beam hypotheses.</span>
<span class="sd">            Each tuple contains the hypothesis score, the predicted token sequence, and the beam indices.</span>
<span class="sd">        worst_score (float): The score of the worst hypothesis in the list.</span>
<span class="sd">        num_beams (int): The maximum number of beam hypotheses to be stored.</span>
<span class="sd">        length_penalty (float): The length penalty factor applied to the hypothesis scores.</span>

<span class="sd">    Methods:</span>
<span class="sd">        add:</span>
<span class="sd">            Add a new hypothesis to the list of beam hypotheses. The hypothesis is represented by a sequence of</span>
<span class="sd">            predicted tokens and its sum of log probabilities. Optionally, the beam indices can also be provided.</span>

<span class="sd">        update:</span>
<span class="sd">            Update the list of beam hypotheses by removing the worst hypothesis if the maximum number of hypotheses</span>
<span class="sd">            is exceeded.</span>

<span class="sd">        get_best:</span>
<span class="sd">            Retrieve the best `num_best` beam hypotheses based on their scores. The hypotheses are returned as a list</span>
<span class="sd">            of tuples, where each tuple contains the hypothesis score, the predicted token sequence, and the beam indices.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hyp</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span> <span class="n">sum_logprobs</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">beam_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add a new hypothesis to the list.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">sum_logprobs</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hyp</span><span class="p">)</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">length_penalty</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="ow">or</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">score</span><span class="p">,</span> <span class="n">hyp</span><span class="p">,</span> <span class="n">beam_indices</span><span class="p">))</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">:</span>
                <span class="n">sorted_next_scores</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([(</span><span class="n">s</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="p">)])</span>
                <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="p">[</span><span class="n">sorted_next_scores</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span> <span class="o">=</span> <span class="n">sorted_next_scores</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses.add" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeBeamHypotheses</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hyp</span><span class="p">,</span> <span class="n">sum_logprobs</span><span class="p">,</span> <span class="n">beam_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamHypotheses.add" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Add a new hypothesis to the list.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1598</span>
<span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hyp</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span> <span class="n">sum_logprobs</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">beam_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add a new hypothesis to the list.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">sum_logprobs</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hyp</span><span class="p">)</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">length_penalty</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="ow">or</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">score</span><span class="p">,</span> <span class="n">hyp</span><span class="p">,</span> <span class="n">beam_indices</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">:</span>
            <span class="n">sorted_next_scores</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([(</span><span class="n">s</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="p">)])</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="p">[</span><span class="n">sorted_next_scores</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span> <span class="o">=</span> <span class="n">sorted_next_scores</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.generation.beam_search.BeamSearchScorer" href="../../../../../api/transformers/generation/beam_search/#mindnlp.transformers.generation.beam_search.BeamSearchScorer">BeamSearchScorer</a></code></p>


        <p>Override BeamSearchScorer for CPMBee to support:</p>
<ol>
<li>Replace beam_tokens by beam_states, containing <code>idx</code>, <code>ans</code>, <code>nx_token_id</code>...</li>
<li>The <code>process</code> will update the beam_states</li>
<li>The <code>finalize</code> will just return the best hypotheses as a list.</li>
</ol>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeBeamSearchScorer</span><span class="p">(</span><span class="n">BeamSearchScorer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Override BeamSearchScorer for CPMBee to support:</span>

<span class="sd">    1. Replace beam_tokens by beam_states, containing `idx`, `ans`, `nx_token_id`...</span>
<span class="sd">    2. The `process` will update the beam_states</span>
<span class="sd">    3. The `finalize` will just return the best hypotheses as a list.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_beams</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">length_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">do_early_stopping</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">num_beam_hyps_to_keep</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">num_beam_groups</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the CpmBeeBeamSearchScorer object.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_size (int): The batch size for beam search.</span>
<span class="sd">            num_beams (int): The number of beams for beam search.</span>
<span class="sd">            length_penalty (float, optional): The length penalty for beam search. Defaults to 1.0.</span>
<span class="sd">            do_early_stopping (bool or str, optional): Flag to indicate if early stopping should be performed.</span>
<span class="sd">                Defaults to False.</span>
<span class="sd">            num_beam_hyps_to_keep (int, optional): The number of beam hypotheses to keep. Defaults to 1.</span>
<span class="sd">            num_beam_groups (int, optional): The number of beam groups for beam search. Defaults to 1.</span>
<span class="sd">            max_length (int, optional): The maximum length for beam search. Defaults to None.</span>
<span class="sd">            **model_kwargs: Additional model-specific keyword arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the provided batch size, num_beams, num_beam_groups, or max_length is not a positive integer.</span>
<span class="sd">            TypeError: If the provided length_penalty is not a float or if do_early_stopping is not a bool or str.</span>
<span class="sd">            RuntimeError: If an error occurs during initialization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">length_penalty</span><span class="p">,</span> <span class="n">do_early_stopping</span><span class="p">,</span> <span class="n">num_beam_hyps_to_keep</span><span class="p">,</span> <span class="n">num_beam_groups</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">=</span> <span class="n">num_beams</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">length_penalty</span> <span class="o">=</span> <span class="n">length_penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_early_stopping</span> <span class="o">=</span> <span class="n">do_early_stopping</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_beam_hyps_to_keep</span> <span class="o">=</span> <span class="n">num_beam_hyps_to_keep</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_beam_groups</span> <span class="o">=</span> <span class="n">num_beam_groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beam_groups</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_is_init</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_beam_hyps</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">CpmBeeBeamHypotheses</span><span class="p">(</span>
                <span class="n">num_beams</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
                <span class="n">length_penalty</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">length_penalty</span><span class="p">,</span>
                <span class="n">early_stopping</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_early_stopping</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_done</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="kc">False</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">beam_states</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">sent_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">instance_beam_states</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">):</span>
                <span class="n">instance_beam_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;idx&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                        <span class="s2">&quot;ans&quot;</span><span class="p">:</span> <span class="p">[],</span>
                        <span class="s2">&quot;nx_token_id&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
                        <span class="s2">&quot;nx_token_sub&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                        <span class="s2">&quot;nx_segment_id&quot;</span><span class="p">:</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;predict_segments&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                        <span class="s2">&quot;nx_position&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                    <span class="p">}</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beam_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">instance_beam_states</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">process</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">cur_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">_next_scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">next_scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">next_tokens</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ext_table_sub_cpu</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ext_table_ids_cpu</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Process the beam search for the CpmBeeBeamSearchScorer.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the CpmBeeBeamSearchScorer class.</span>
<span class="sd">            batch_size (int): The batch size for processing.</span>
<span class="sd">            cur_len (int): The current length of the sequence being processed.</span>
<span class="sd">            _next_scores (mindspore.Tensor): The scores for the next tokens.</span>
<span class="sd">            next_scores (mindspore.Tensor): The scores for the next tokens.</span>
<span class="sd">            next_tokens (mindspore.Tensor): The tokens for the next sequence.</span>
<span class="sd">            vocab_size (Optional[int]): The size of the vocabulary. Defaults to None.</span>
<span class="sd">            pad_token_id (Optional[int]): The token ID for padding. Defaults to None.</span>
<span class="sd">            bos_token_id (Optional[int]): The token ID for the beginning of sequence. Defaults to None.</span>
<span class="sd">            eos_token_id (Optional[Union[int, List[int]]]): The token ID for the end of sequence. Defaults to None.</span>
<span class="sd">            max_length (Optional[int]): The maximum length of the sequence. Defaults to None.</span>
<span class="sd">            ext_table_sub_cpu (Optional[mindspore.Tensor]): The CPU tensor for extended table sub.</span>
<span class="sd">            ext_table_ids_cpu (Optional[mindspore.Tensor]): The CPU tensor for extended table IDs.</span>
<span class="sd">            **model_kwargs: Additional keyword arguments for the model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple[mindspore.Tensor]: A tuple containing the next beam scores, next beam states, and next beam indices.</span>

<span class="sd">        Raises:</span>
<span class="sd">            AssertionError: If the length of next_instance_beam_states is not equal to zero when cur_len is equal to</span>
<span class="sd">                max_length, or not equal to self.num_beams otherwise.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">next_beam_state</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">sent_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_done</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_done</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beam_hyps</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span><span class="o">.</span><span class="n">is_done</span><span class="p">(</span>
                <span class="n">next_scores</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">cur_len</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_done</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]:</span>
                <span class="n">next_beam_state</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="p">(</span>
                            <span class="p">{</span>
                                <span class="s2">&quot;idx&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                                <span class="s2">&quot;ans&quot;</span><span class="p">:</span> <span class="p">[],</span>
                                <span class="s2">&quot;nx_token_id&quot;</span><span class="p">:</span> <span class="n">pad_token_id</span><span class="p">,</span>
                                <span class="s2">&quot;nx_token_sub&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                                <span class="s2">&quot;nx_segment_id&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                                <span class="s2">&quot;nx_position&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                            <span class="p">},</span>
                            <span class="mi">0</span><span class="p">,</span>
                            <span class="mi">0</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
                    <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span>
                <span class="p">)</span>
                <span class="k">continue</span>

            <span class="n">next_instance_beam_states</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">[</span><span class="n">sent_id</span><span class="p">],</span> <span class="n">next_scores</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]):</span>
                <span class="n">beam_id</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">_next_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="s2">&quot;floor&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">word_id</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx</span> <span class="o">%</span> <span class="n">_next_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

                <span class="n">curr_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beam_states</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="n">beam_id</span><span class="p">]</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">word_id</span> <span class="o">==</span> <span class="n">eos_token_id</span>
                    <span class="ow">and</span> <span class="p">(</span><span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;idx&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;predict_segments&quot;</span><span class="p">]))</span>
                <span class="p">)</span> <span class="ow">or</span> <span class="n">cur_len</span> <span class="o">==</span> <span class="n">max_length</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_beam_hyps</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">beam_states</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="n">beam_id</span><span class="p">][</span><span class="s2">&quot;ans&quot;</span><span class="p">]</span>
                        <span class="o">+</span> <span class="p">[</span>
                            <span class="p">(</span>
                                <span class="n">word_id</span><span class="p">,</span>
                                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;predict_segments&quot;</span><span class="p">][</span><span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;idx&quot;</span><span class="p">]][</span><span class="mi">1</span><span class="p">],</span>
                            <span class="p">)</span>
                        <span class="p">],</span>
                        <span class="n">value</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="n">word_id</span> <span class="o">==</span> <span class="n">eos_token_id</span><span class="p">:</span>
                    <span class="n">next_instance_beam_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="p">(</span>
                            <span class="p">{</span>
                                <span class="s2">&quot;idx&quot;</span><span class="p">:</span> <span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;idx&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                                <span class="s2">&quot;ans&quot;</span><span class="p">:</span> <span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;ans&quot;</span><span class="p">]</span>
                                <span class="o">+</span> <span class="p">[</span>
                                    <span class="p">(</span>
                                        <span class="n">word_id</span><span class="p">,</span>
                                        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;predict_segments&quot;</span><span class="p">][</span><span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;idx&quot;</span><span class="p">]][</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="p">)</span>
                                <span class="p">],</span>
                                <span class="s2">&quot;nx_token_id&quot;</span><span class="p">:</span> <span class="n">bos_token_id</span><span class="p">,</span>
                                <span class="s2">&quot;nx_token_sub&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                                <span class="s2">&quot;nx_segment_id&quot;</span><span class="p">:</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;predict_segments&quot;</span><span class="p">][</span>
                                    <span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;idx&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
                                <span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                <span class="s2">&quot;nx_position&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                            <span class="p">},</span>
                            <span class="n">value</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                            <span class="n">sent_id</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">+</span> <span class="n">beam_id</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="n">raw_word_id</span> <span class="o">=</span> <span class="n">word_id</span>
                    <span class="n">word_id_sub</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="k">if</span> <span class="n">word_id</span> <span class="o">&gt;=</span> <span class="n">vocab_size</span><span class="p">:</span>
                        <span class="n">word_id</span> <span class="o">-=</span> <span class="n">vocab_size</span>
                        <span class="n">word_id_sub</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">ext_table_sub_cpu</span><span class="p">[</span><span class="n">word_id</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
                        <span class="n">word_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">ext_table_ids_cpu</span><span class="p">[</span><span class="n">word_id</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

                    <span class="n">next_instance_beam_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="p">(</span>
                            <span class="p">{</span>
                                <span class="s2">&quot;idx&quot;</span><span class="p">:</span> <span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;idx&quot;</span><span class="p">],</span>
                                <span class="s2">&quot;ans&quot;</span><span class="p">:</span> <span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;ans&quot;</span><span class="p">]</span>
                                <span class="o">+</span> <span class="p">[</span>
                                    <span class="p">(</span>
                                        <span class="n">raw_word_id</span><span class="p">,</span>
                                        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;predict_segments&quot;</span><span class="p">][</span><span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;idx&quot;</span><span class="p">]][</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="p">)</span>
                                <span class="p">],</span>
                                <span class="s2">&quot;nx_token_id&quot;</span><span class="p">:</span> <span class="n">word_id</span><span class="p">,</span>
                                <span class="s2">&quot;nx_token_sub&quot;</span><span class="p">:</span> <span class="n">word_id_sub</span><span class="p">,</span>
                                <span class="s2">&quot;nx_segment_id&quot;</span><span class="p">:</span> <span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;nx_segment_id&quot;</span><span class="p">],</span>
                                <span class="s2">&quot;nx_position&quot;</span><span class="p">:</span> <span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;nx_position&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                            <span class="p">},</span>
                            <span class="n">value</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                            <span class="n">sent_id</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">+</span> <span class="n">beam_id</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">next_instance_beam_states</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">:</span>
                    <span class="k">break</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">next_instance_beam_states</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">cur_len</span> <span class="o">==</span> <span class="n">max_length</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span>
            <span class="n">next_beam_state</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_instance_beam_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">cur_len</span> <span class="o">==</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="n">beam_reorder_idx</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">beam_new_scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">beam_states</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">sent_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">instance_beam_states</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">beam_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">):</span>
                <span class="n">state</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">beam_idx</span> <span class="o">=</span> <span class="n">next_beam_state</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="n">beam_id</span><span class="p">]</span>
                <span class="n">beam_reorder_idx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beam_idx</span><span class="p">)</span>
                <span class="n">beam_new_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                <span class="n">instance_beam_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">beam_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">instance_beam_states</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beam_states</span> <span class="o">=</span> <span class="n">beam_states</span>

        <span class="k">return</span> <span class="n">UserDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;next_beam_scores&quot;</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">beam_new_scores</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                <span class="s2">&quot;next_beam_states&quot;</span><span class="p">:</span> <span class="n">beam_states</span><span class="p">,</span>
                <span class="s2">&quot;next_beam_indices&quot;</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">beam_reorder_idx</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
            <span class="p">}</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">finalize</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Finalizes the beam search scoring process and returns the best hypotheses.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the CpmBeeBeamSearchScorer class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple containing mindspore.Tensor objects representing the best hypotheses.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>

<span class="sd">        This method iterates over the beam hypotheses generated during the beam search process and selects the</span>
<span class="sd">        best hypothesis from each beam. The best hypothesis is determined based on the maximum score assigned to it.</span>
<span class="sd">        The selected best hypotheses are then returned as a tuple of mindspore.Tensor objects.</span>

<span class="sd">        Note:</span>
<span class="sd">            - The beam hypotheses are internally stored in the _beam_hyps attribute of the CpmBeeBeamSearchScorer instance.</span>
<span class="sd">            - The best hypothesis is determined by selecting the hypothesis with the maximum score from each beam.</span>

<span class="sd">        Example:</span>
<span class="sd">            ```python</span>
<span class="sd">            &gt;&gt;&gt; scorer = CpmBeeBeamSearchScorer()</span>
<span class="sd">            &gt;&gt;&gt; results = scorer.finalize()</span>
<span class="sd">            &gt;&gt;&gt; # results contains the best hypotheses as mindspore.Tensor objects.</span>
<span class="sd">            ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">hypotheses</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_beam_hyps</span><span class="p">):</span>
            <span class="n">best_hyp</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">hypotheses</span><span class="o">.</span><span class="n">beams</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_hyp</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">results</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">apply_repetition_penalty</span><span class="p">(</span>
        <span class="n">logits</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">,</span>
        <span class="n">num_beams</span><span class="p">,</span>
        <span class="n">prev_output_tokens</span><span class="p">,</span>
        <span class="n">repetition_penalty</span><span class="p">,</span>
        <span class="n">start_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">end_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">window_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies repetition penalty to the logits for beam search in the CpmBeeBeamSearchScorer class.</span>

<span class="sd">        Args:</span>
<span class="sd">            logits (Tensor): The logits representing the scores for each token in the vocabulary.</span>
<span class="sd">                Shape: (batch_size * num_beams, vocab_size).</span>
<span class="sd">            batch_size (int): The size of the batch.</span>
<span class="sd">            num_beams (int): The number of beams used in the beam search.</span>
<span class="sd">            prev_output_tokens (Tensor): The previously generated tokens. Shape: (batch_size * num_beams, sequence_length).</span>
<span class="sd">            repetition_penalty (float): The coefficient for the repetition penalty. Must be &gt;= 1.</span>
<span class="sd">            start_idx (int, optional): The start index of the window for calculating repetition penalty. Defaults to None.</span>
<span class="sd">            end_idx (int, optional): The end index of the window for calculating repetition penalty. Defaults to None.</span>
<span class="sd">            window_size (int, optional): The size of the window for calculating repetition penalty. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            AssertionError: If repetition_penalty is less than 1.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># only conduct repetition penalty for the output</span>
        <span class="k">assert</span> <span class="n">repetition_penalty</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;repetition penalty coefficient should &gt;= 1&quot;</span>
        <span class="c1"># repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">start_idx</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">end_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">prev_output_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">end_idx</span> <span class="o">&gt;=</span> <span class="n">start_idx</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">window_size</span><span class="p">:</span>
                        <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">prev_output_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">][</span>
                            <span class="nb">max</span><span class="p">(</span><span class="n">start_idx</span><span class="p">,</span> <span class="n">end_idx</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">window_size</span><span class="p">)</span> <span class="p">:</span> <span class="n">end_idx</span> <span class="o">+</span> <span class="mi">1</span>
                        <span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">prev_output_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">start_idx</span> <span class="p">:</span> <span class="n">end_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">output_tokens</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">previous_token</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">):</span>
                <span class="c1"># if score &lt; 0 then repetition penalty has to</span>
                <span class="c1"># multiplied to reduce the previous token probability</span>
                <span class="k">if</span> <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">previous_token</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">previous_token</span><span class="p">]</span> <span class="o">*=</span> <span class="n">repetition_penalty</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">previous_token</span><span class="p">]</span> <span class="o">/=</span> <span class="n">repetition_penalty</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeBeamSearchScorer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">length_penalty</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">do_early_stopping</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_beam_hyps_to_keep</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_beam_groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes the CpmBeeBeamSearchScorer object.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>batch_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The batch size for beam search.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_beams</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of beams for beam search.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>length_penalty</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The length penalty for beam search. Defaults to 1.0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>do_early_stopping</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Flag to indicate if early stopping should be performed.
Defaults to False.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool or str</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_beam_hyps_to_keep</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of beam hypotheses to keep. Defaults to 1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_beam_groups</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of beam groups for beam search. Defaults to 1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_length</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum length for beam search. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>**model_kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Additional model-specific keyword arguments.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the provided batch size, num_beams, num_beam_groups, or max_length is not a positive integer.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the provided length_penalty is not a float or if do_early_stopping is not a bool or str.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If an error occurs during initialization.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_beams</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">length_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">do_early_stopping</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">num_beam_hyps_to_keep</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">num_beam_groups</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes the CpmBeeBeamSearchScorer object.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_size (int): The batch size for beam search.</span>
<span class="sd">        num_beams (int): The number of beams for beam search.</span>
<span class="sd">        length_penalty (float, optional): The length penalty for beam search. Defaults to 1.0.</span>
<span class="sd">        do_early_stopping (bool or str, optional): Flag to indicate if early stopping should be performed.</span>
<span class="sd">            Defaults to False.</span>
<span class="sd">        num_beam_hyps_to_keep (int, optional): The number of beam hypotheses to keep. Defaults to 1.</span>
<span class="sd">        num_beam_groups (int, optional): The number of beam groups for beam search. Defaults to 1.</span>
<span class="sd">        max_length (int, optional): The maximum length for beam search. Defaults to None.</span>
<span class="sd">        **model_kwargs: Additional model-specific keyword arguments.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the provided batch size, num_beams, num_beam_groups, or max_length is not a positive integer.</span>
<span class="sd">        TypeError: If the provided length_penalty is not a float or if do_early_stopping is not a bool or str.</span>
<span class="sd">        RuntimeError: If an error occurs during initialization.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">length_penalty</span><span class="p">,</span> <span class="n">do_early_stopping</span><span class="p">,</span> <span class="n">num_beam_hyps_to_keep</span><span class="p">,</span> <span class="n">num_beam_groups</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">=</span> <span class="n">num_beams</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">length_penalty</span> <span class="o">=</span> <span class="n">length_penalty</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_early_stopping</span> <span class="o">=</span> <span class="n">do_early_stopping</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_beam_hyps_to_keep</span> <span class="o">=</span> <span class="n">num_beam_hyps_to_keep</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_beam_groups</span> <span class="o">=</span> <span class="n">num_beam_groups</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beam_groups</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_is_init</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_beam_hyps</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">CpmBeeBeamHypotheses</span><span class="p">(</span>
            <span class="n">num_beams</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
            <span class="n">length_penalty</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">length_penalty</span><span class="p">,</span>
            <span class="n">early_stopping</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_early_stopping</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_done</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="kc">False</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">beam_states</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sent_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">instance_beam_states</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">):</span>
            <span class="n">instance_beam_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;idx&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                    <span class="s2">&quot;ans&quot;</span><span class="p">:</span> <span class="p">[],</span>
                    <span class="s2">&quot;nx_token_id&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
                    <span class="s2">&quot;nx_token_sub&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                    <span class="s2">&quot;nx_segment_id&quot;</span><span class="p">:</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;predict_segments&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                    <span class="s2">&quot;nx_position&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beam_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">instance_beam_states</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer.apply_repetition_penalty" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeBeamSearchScorer</span><span class="o">.</span><span class="n">apply_repetition_penalty</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">prev_output_tokens</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="p">,</span> <span class="n">start_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">end_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer.apply_repetition_penalty" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Applies repetition penalty to the logits for beam search in the CpmBeeBeamSearchScorer class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>logits</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The logits representing the scores for each token in the vocabulary.
Shape: (batch_size * num_beams, vocab_size).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Tensor</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>batch_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the batch.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_beams</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of beams used in the beam search.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prev_output_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The previously generated tokens. Shape: (batch_size * num_beams, sequence_length).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>Tensor</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>repetition_penalty</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The coefficient for the repetition penalty. Must be &gt;= 1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>float</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>start_idx</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The start index of the window for calculating repetition penalty. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>end_idx</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The end index of the window for calculating repetition penalty. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>window_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the window for calculating repetition penalty. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>AssertionError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If repetition_penalty is less than 1.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">apply_repetition_penalty</span><span class="p">(</span>
    <span class="n">logits</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">,</span>
    <span class="n">num_beams</span><span class="p">,</span>
    <span class="n">prev_output_tokens</span><span class="p">,</span>
    <span class="n">repetition_penalty</span><span class="p">,</span>
    <span class="n">start_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">end_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">window_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies repetition penalty to the logits for beam search in the CpmBeeBeamSearchScorer class.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): The logits representing the scores for each token in the vocabulary.</span>
<span class="sd">            Shape: (batch_size * num_beams, vocab_size).</span>
<span class="sd">        batch_size (int): The size of the batch.</span>
<span class="sd">        num_beams (int): The number of beams used in the beam search.</span>
<span class="sd">        prev_output_tokens (Tensor): The previously generated tokens. Shape: (batch_size * num_beams, sequence_length).</span>
<span class="sd">        repetition_penalty (float): The coefficient for the repetition penalty. Must be &gt;= 1.</span>
<span class="sd">        start_idx (int, optional): The start index of the window for calculating repetition penalty. Defaults to None.</span>
<span class="sd">        end_idx (int, optional): The end index of the window for calculating repetition penalty. Defaults to None.</span>
<span class="sd">        window_size (int, optional): The size of the window for calculating repetition penalty. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        AssertionError: If repetition_penalty is less than 1.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># only conduct repetition penalty for the output</span>
    <span class="k">assert</span> <span class="n">repetition_penalty</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;repetition penalty coefficient should &gt;= 1&quot;</span>
    <span class="c1"># repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">start_idx</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">end_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">prev_output_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">end_idx</span> <span class="o">&gt;=</span> <span class="n">start_idx</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">window_size</span><span class="p">:</span>
                    <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">prev_output_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">][</span>
                        <span class="nb">max</span><span class="p">(</span><span class="n">start_idx</span><span class="p">,</span> <span class="n">end_idx</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">window_size</span><span class="p">)</span> <span class="p">:</span> <span class="n">end_idx</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">prev_output_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">start_idx</span> <span class="p">:</span> <span class="n">end_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">previous_token</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">):</span>
            <span class="c1"># if score &lt; 0 then repetition penalty has to</span>
            <span class="c1"># multiplied to reduce the previous token probability</span>
            <span class="k">if</span> <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">previous_token</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">previous_token</span><span class="p">]</span> <span class="o">*=</span> <span class="n">repetition_penalty</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">previous_token</span><span class="p">]</span> <span class="o">/=</span> <span class="n">repetition_penalty</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer.finalize" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeBeamSearchScorer</span><span class="o">.</span><span class="n">finalize</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer.finalize" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Finalizes the beam search scoring process and returns the best hypotheses.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CpmBeeBeamSearchScorer class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Tuple">Tuple</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A tuple containing mindspore.Tensor objects representing the best hypotheses.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <p>This method iterates over the beam hypotheses generated during the beam search process and selects the
best hypothesis from each beam. The best hypothesis is determined based on the maximum score assigned to it.
The selected best hypotheses are then returned as a tuple of mindspore.Tensor objects.</p>


<details class="note" open>
  <summary>Note</summary>
  <ul>
<li>The beam hypotheses are internally stored in the _beam_hyps attribute of the CpmBeeBeamSearchScorer instance.</li>
<li>The best hypothesis is determined by selecting the hypothesis with the maximum score from each beam.</li>
</ul>
</details>

<details class="example" open>
  <summary>Example</summary>
  <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">scorer</span> <span class="o">=</span> <span class="n">CpmBeeBeamSearchScorer</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">results</span> <span class="o">=</span> <span class="n">scorer</span><span class="o">.</span><span class="n">finalize</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># results contains the best hypotheses as mindspore.Tensor objects.</span>
</code></pre></div>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">finalize</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finalizes the beam search scoring process and returns the best hypotheses.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the CpmBeeBeamSearchScorer class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple containing mindspore.Tensor objects representing the best hypotheses.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>

<span class="sd">    This method iterates over the beam hypotheses generated during the beam search process and selects the</span>
<span class="sd">    best hypothesis from each beam. The best hypothesis is determined based on the maximum score assigned to it.</span>
<span class="sd">    The selected best hypotheses are then returned as a tuple of mindspore.Tensor objects.</span>

<span class="sd">    Note:</span>
<span class="sd">        - The beam hypotheses are internally stored in the _beam_hyps attribute of the CpmBeeBeamSearchScorer instance.</span>
<span class="sd">        - The best hypothesis is determined by selecting the hypothesis with the maximum score from each beam.</span>

<span class="sd">    Example:</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; scorer = CpmBeeBeamSearchScorer()</span>
<span class="sd">        &gt;&gt;&gt; results = scorer.finalize()</span>
<span class="sd">        &gt;&gt;&gt; # results contains the best hypotheses as mindspore.Tensor objects.</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">hypotheses</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_beam_hyps</span><span class="p">):</span>
        <span class="n">best_hyp</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">hypotheses</span><span class="o">.</span><span class="n">beams</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_hyp</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer.process" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeBeamSearchScorer</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">,</span> <span class="n">_next_scores</span><span class="p">,</span> <span class="n">next_scores</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bos_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ext_table_sub_cpu</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ext_table_ids_cpu</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBeamSearchScorer.process" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Process the beam search for the CpmBeeBeamSearchScorer.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CpmBeeBeamSearchScorer class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>batch_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The batch size for processing.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cur_len</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The current length of the sequence being processed.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>_next_scores</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The scores for the next tokens.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>next_scores</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The scores for the next tokens.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>next_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The tokens for the next sequence.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocab_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The size of the vocabulary. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pad_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token ID for padding. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token ID for the beginning of sequence. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eos_token_id</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token ID for the end of sequence. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[int, <span title="typing.List">List</span>[int]]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_length</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum length of the sequence. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ext_table_sub_cpu</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The CPU tensor for extended table sub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ext_table_ids_cpu</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The CPU tensor for extended table IDs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>**model_kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Additional keyword arguments for the model.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Tuple">Tuple</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Tuple[mindspore.Tensor]: A tuple containing the next beam scores, next beam states, and next beam indices.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>AssertionError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the length of next_instance_beam_states is not equal to zero when cur_len is equal to
max_length, or not equal to self.num_beams otherwise.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">process</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">cur_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">_next_scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">next_scores</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">next_tokens</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ext_table_sub_cpu</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ext_table_ids_cpu</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Process the beam search for the CpmBeeBeamSearchScorer.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the CpmBeeBeamSearchScorer class.</span>
<span class="sd">        batch_size (int): The batch size for processing.</span>
<span class="sd">        cur_len (int): The current length of the sequence being processed.</span>
<span class="sd">        _next_scores (mindspore.Tensor): The scores for the next tokens.</span>
<span class="sd">        next_scores (mindspore.Tensor): The scores for the next tokens.</span>
<span class="sd">        next_tokens (mindspore.Tensor): The tokens for the next sequence.</span>
<span class="sd">        vocab_size (Optional[int]): The size of the vocabulary. Defaults to None.</span>
<span class="sd">        pad_token_id (Optional[int]): The token ID for padding. Defaults to None.</span>
<span class="sd">        bos_token_id (Optional[int]): The token ID for the beginning of sequence. Defaults to None.</span>
<span class="sd">        eos_token_id (Optional[Union[int, List[int]]]): The token ID for the end of sequence. Defaults to None.</span>
<span class="sd">        max_length (Optional[int]): The maximum length of the sequence. Defaults to None.</span>
<span class="sd">        ext_table_sub_cpu (Optional[mindspore.Tensor]): The CPU tensor for extended table sub.</span>
<span class="sd">        ext_table_ids_cpu (Optional[mindspore.Tensor]): The CPU tensor for extended table IDs.</span>
<span class="sd">        **model_kwargs: Additional keyword arguments for the model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[mindspore.Tensor]: A tuple containing the next beam scores, next beam states, and next beam indices.</span>

<span class="sd">    Raises:</span>
<span class="sd">        AssertionError: If the length of next_instance_beam_states is not equal to zero when cur_len is equal to</span>
<span class="sd">            max_length, or not equal to self.num_beams otherwise.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">next_beam_state</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sent_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_done</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_done</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beam_hyps</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span><span class="o">.</span><span class="n">is_done</span><span class="p">(</span>
            <span class="n">next_scores</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">cur_len</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_done</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]:</span>
            <span class="n">next_beam_state</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="p">(</span>
                        <span class="p">{</span>
                            <span class="s2">&quot;idx&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                            <span class="s2">&quot;ans&quot;</span><span class="p">:</span> <span class="p">[],</span>
                            <span class="s2">&quot;nx_token_id&quot;</span><span class="p">:</span> <span class="n">pad_token_id</span><span class="p">,</span>
                            <span class="s2">&quot;nx_token_sub&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                            <span class="s2">&quot;nx_segment_id&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                            <span class="s2">&quot;nx_position&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                        <span class="p">},</span>
                        <span class="mi">0</span><span class="p">,</span>
                        <span class="mi">0</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">]</span>
                <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span>
            <span class="p">)</span>
            <span class="k">continue</span>

        <span class="n">next_instance_beam_states</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">[</span><span class="n">sent_id</span><span class="p">],</span> <span class="n">next_scores</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]):</span>
            <span class="n">beam_id</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">_next_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="s2">&quot;floor&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">word_id</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx</span> <span class="o">%</span> <span class="n">_next_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="n">curr_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beam_states</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="n">beam_id</span><span class="p">]</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">word_id</span> <span class="o">==</span> <span class="n">eos_token_id</span>
                <span class="ow">and</span> <span class="p">(</span><span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;idx&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;predict_segments&quot;</span><span class="p">]))</span>
            <span class="p">)</span> <span class="ow">or</span> <span class="n">cur_len</span> <span class="o">==</span> <span class="n">max_length</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_beam_hyps</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">beam_states</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="n">beam_id</span><span class="p">][</span><span class="s2">&quot;ans&quot;</span><span class="p">]</span>
                    <span class="o">+</span> <span class="p">[</span>
                        <span class="p">(</span>
                            <span class="n">word_id</span><span class="p">,</span>
                            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;predict_segments&quot;</span><span class="p">][</span><span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;idx&quot;</span><span class="p">]][</span><span class="mi">1</span><span class="p">],</span>
                        <span class="p">)</span>
                    <span class="p">],</span>
                    <span class="n">value</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">word_id</span> <span class="o">==</span> <span class="n">eos_token_id</span><span class="p">:</span>
                <span class="n">next_instance_beam_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="p">(</span>
                        <span class="p">{</span>
                            <span class="s2">&quot;idx&quot;</span><span class="p">:</span> <span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;idx&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                            <span class="s2">&quot;ans&quot;</span><span class="p">:</span> <span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;ans&quot;</span><span class="p">]</span>
                            <span class="o">+</span> <span class="p">[</span>
                                <span class="p">(</span>
                                    <span class="n">word_id</span><span class="p">,</span>
                                    <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;predict_segments&quot;</span><span class="p">][</span><span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;idx&quot;</span><span class="p">]][</span><span class="mi">1</span><span class="p">],</span>
                                <span class="p">)</span>
                            <span class="p">],</span>
                            <span class="s2">&quot;nx_token_id&quot;</span><span class="p">:</span> <span class="n">bos_token_id</span><span class="p">,</span>
                            <span class="s2">&quot;nx_token_sub&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                            <span class="s2">&quot;nx_segment_id&quot;</span><span class="p">:</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;predict_segments&quot;</span><span class="p">][</span>
                                <span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;idx&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
                            <span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                            <span class="s2">&quot;nx_position&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                        <span class="p">},</span>
                        <span class="n">value</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                        <span class="n">sent_id</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">+</span> <span class="n">beam_id</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">raw_word_id</span> <span class="o">=</span> <span class="n">word_id</span>
                <span class="n">word_id_sub</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">if</span> <span class="n">word_id</span> <span class="o">&gt;=</span> <span class="n">vocab_size</span><span class="p">:</span>
                    <span class="n">word_id</span> <span class="o">-=</span> <span class="n">vocab_size</span>
                    <span class="n">word_id_sub</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">ext_table_sub_cpu</span><span class="p">[</span><span class="n">word_id</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
                    <span class="n">word_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">ext_table_ids_cpu</span><span class="p">[</span><span class="n">word_id</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

                <span class="n">next_instance_beam_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="p">(</span>
                        <span class="p">{</span>
                            <span class="s2">&quot;idx&quot;</span><span class="p">:</span> <span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;idx&quot;</span><span class="p">],</span>
                            <span class="s2">&quot;ans&quot;</span><span class="p">:</span> <span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;ans&quot;</span><span class="p">]</span>
                            <span class="o">+</span> <span class="p">[</span>
                                <span class="p">(</span>
                                    <span class="n">raw_word_id</span><span class="p">,</span>
                                    <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;predict_segments&quot;</span><span class="p">][</span><span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;idx&quot;</span><span class="p">]][</span><span class="mi">1</span><span class="p">],</span>
                                <span class="p">)</span>
                            <span class="p">],</span>
                            <span class="s2">&quot;nx_token_id&quot;</span><span class="p">:</span> <span class="n">word_id</span><span class="p">,</span>
                            <span class="s2">&quot;nx_token_sub&quot;</span><span class="p">:</span> <span class="n">word_id_sub</span><span class="p">,</span>
                            <span class="s2">&quot;nx_segment_id&quot;</span><span class="p">:</span> <span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;nx_segment_id&quot;</span><span class="p">],</span>
                            <span class="s2">&quot;nx_position&quot;</span><span class="p">:</span> <span class="n">curr_info</span><span class="p">[</span><span class="s2">&quot;nx_position&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                        <span class="p">},</span>
                        <span class="n">value</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                        <span class="n">sent_id</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">+</span> <span class="n">beam_id</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">next_instance_beam_states</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">:</span>
                <span class="k">break</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">next_instance_beam_states</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">cur_len</span> <span class="o">==</span> <span class="n">max_length</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span>
        <span class="n">next_beam_state</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_instance_beam_states</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">cur_len</span> <span class="o">==</span> <span class="n">max_length</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="n">beam_reorder_idx</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">beam_new_scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">beam_states</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sent_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">instance_beam_states</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">beam_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">):</span>
            <span class="n">state</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">beam_idx</span> <span class="o">=</span> <span class="n">next_beam_state</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="n">beam_id</span><span class="p">]</span>
            <span class="n">beam_reorder_idx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beam_idx</span><span class="p">)</span>
            <span class="n">beam_new_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
            <span class="n">instance_beam_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">beam_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">instance_beam_states</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">beam_states</span> <span class="o">=</span> <span class="n">beam_states</span>

    <span class="k">return</span> <span class="n">UserDict</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;next_beam_scores&quot;</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">beam_new_scores</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
            <span class="s2">&quot;next_beam_states&quot;</span><span class="p">:</span> <span class="n">beam_states</span><span class="p">,</span>
            <span class="s2">&quot;next_beam_indices&quot;</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">beam_reorder_idx</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This class represents a position bias computation module in the CpmBee model.
It is used to calculate the relative position buckets for attention mechanism.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.num_heads">num_heads</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The number of attention heads.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.num_buckets">num_buckets</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The number of position bias buckets.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.num_segment_bucket">num_segment_bucket</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The number of segment buckets used for position bias.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.max_distance">max_distance</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The maximum distance for position bias calculation.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.relative_attention_bias">relative_attention_bias</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The learnable parameter used for relative attention bias calculation.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Parameter">Parameter</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.__init__" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the CpmBeeBucketPositionBias instance.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.forward" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the position bias based on the given query and key positions and relative buckets.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias._position_bucket">_position_bucket</span></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Computes the position bucket for the given relative position.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeBucketPositionBias</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents a position bias computation module in the CpmBee model.</span>
<span class="sd">    It is used to calculate the relative position buckets for attention mechanism.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        num_heads (int): The number of attention heads.</span>
<span class="sd">        num_buckets (int): The number of position bias buckets.</span>
<span class="sd">        num_segment_bucket (int): The number of segment buckets used for position bias.</span>
<span class="sd">        max_distance (int): The maximum distance for position bias calculation.</span>
<span class="sd">        relative_attention_bias (mindspore.Parameter): The learnable parameter used for relative attention bias calculation.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__:</span>
<span class="sd">            Initializes the CpmBeeBucketPositionBias instance.</span>

<span class="sd">        forward:</span>
<span class="sd">            Constructs the position bias based on the given query and key positions and relative buckets.</span>

<span class="sd">        _position_bucket:</span>
<span class="sd">            Computes the position bucket for the given relative position.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes an instance of the CpmBeeBucketPositionBias class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            config (CpmBeeConfig):</span>
<span class="sd">                The configuration object containing various parameters.</span>

<span class="sd">                - num_attention_heads (int): The number of attention heads.</span>
<span class="sd">                - position_bias_num_buckets (int): The number of buckets for position bias.</span>
<span class="sd">                - position_bias_num_segment_buckets (int): The number of buckets for segment bias.</span>
<span class="sd">                - position_bias_max_distance (int): The maximum distance for position bias.</span>
<span class="sd">                - ms_dtype: The dtype for the position bias parameter.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">position_bias_num_buckets</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_segment_bucket</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">position_bias_num_segment_buckets</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_distance</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">position_bias_max_distance</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">relative_attention_bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="n">config</span><span class="o">.</span><span class="n">position_bias_num_buckets</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">position_bias_num_segment_buckets</span><span class="p">,</span>
                <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_pos</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">key_pos</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">rel_buckets</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method forwards relative position bias embeddings based on the input query positions, key positions,</span>
<span class="sd">        and relative buckets.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeBucketPositionBias): An instance of the CpmBeeBucketPositionBias class.</span>
<span class="sd">            query_pos (mindspore.Tensor): A tensor representing the positions of queries in the input sequence.</span>
<span class="sd">            key_pos (mindspore.Tensor): A tensor representing the positions of keys in the input sequence.</span>
<span class="sd">            rel_buckets (mindspore.Tensor): A tensor containing relative position buckets.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: This method does not return any value explicitly.</span>
<span class="sd">                The forwarded embeddings are stored in the &#39;embeds&#39; variable within the method.</span>

<span class="sd">        Raises:</span>
<span class="sd">            AssertionError:</span>
<span class="sd">                - If the number of batches in key_pos and query_pos tensors are not equal.</span>
<span class="sd">                - If the number of batches in rel_buckets and key_pos tensors are not equal.</span>
<span class="sd">                - If the number of query positions in the rel_buckets tensor does not match the query positions tensor.</span>
<span class="sd">                - If the number of key positions in the rel_buckets tensor does not match the key positions tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">key_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">keylen</span> <span class="o">=</span> <span class="n">key_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">querylen</span> <span class="o">=</span> <span class="n">query_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">key_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">query_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;key_pos.shape[0] should be equal to query_pos.shape[0], but got </span><span class="si">{</span><span class="n">key_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">query_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">!&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">rel_buckets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">batch</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;rel_buckets.shape[0] should be equal to batch, but got </span><span class="si">{</span><span class="n">rel_buckets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">batch</span><span class="si">}</span><span class="s2">!&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">rel_buckets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">querylen</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;rel_buckets.shape[1] should be equal to querylen, but got </span><span class="si">{</span><span class="n">rel_buckets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">querylen</span><span class="si">}</span><span class="s2">!&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">rel_buckets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">keylen</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;rel_buckets.shape[2] should be equal to keylen, but got </span><span class="si">{</span><span class="n">rel_buckets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">keylen</span><span class="si">}</span><span class="s2">!&quot;</span>
            <span class="p">)</span>

        <span class="n">relative_position_bucket</span> <span class="o">=</span> <span class="n">rel_buckets</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span>

        <span class="n">inner_segment_bucket</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_position_bucket</span><span class="p">(</span>
            <span class="n">key_pos</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">query_pos</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">],</span>
            <span class="n">num_buckets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">,</span>
            <span class="n">max_distance</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_distance</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">relative_position_bucket</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">rel_buckets</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
            <span class="n">inner_segment_bucket</span><span class="p">,</span>
            <span class="n">relative_position_bucket</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">embeds</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">relative_position_bucket</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">relative_attention_bias</span><span class="p">)</span>
        <span class="n">embeds</span> <span class="o">=</span> <span class="n">embeds</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">embeds</span>

    <span class="k">def</span> <span class="nf">_position_bucket</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">relative_position</span><span class="p">,</span> <span class="n">num_buckets</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">max_distance</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method calculates the position bucket for a given relative position within a specified range.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the CpmBeeBucketPositionBias class.</span>
<span class="sd">            relative_position (int): The relative position for which the bucket needs to be calculated.</span>
<span class="sd">            num_buckets (int, optional): The number of buckets to categorize the relative position into. Defaults to 32.</span>
<span class="sd">            max_distance (int, optional): The maximum distance for categorizing the relative position. Defaults to 128.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None:</span>
<span class="sd">                This method does not return a value as it directly updates the &#39;relative_buckets&#39; attribute of</span>
<span class="sd">                the CpmBeeBucketPositionBias instance.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the &#39;relative_position&#39; or &#39;num_buckets&#39; is not a positive integer.</span>
<span class="sd">            ValueError: If the &#39;max_distance&#39; is not a positive integer greater than 0.</span>
<span class="sd">            TypeError: If the &#39;relative_position&#39;, &#39;num_buckets&#39;, or &#39;max_distance&#39; is not of type int.</span>
<span class="sd">            ValueError: If the &#39;num_buckets&#39; is less than or equal to 0.</span>
<span class="sd">            ValueError: If the &#39;max_distance&#39; is less than or equal to 0.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relative_buckets</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">num_buckets</span> <span class="o">//=</span> <span class="mi">2</span>
        <span class="n">relative_buckets</span> <span class="o">=</span> <span class="p">(</span><span class="n">relative_position</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_buckets</span>
        <span class="n">relative_position</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">relative_position</span><span class="p">)</span>
        <span class="n">max_exact</span> <span class="o">=</span> <span class="n">num_buckets</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">is_small</span> <span class="o">=</span> <span class="n">relative_position</span> <span class="o">&lt;</span> <span class="n">max_exact</span>
        <span class="n">relative_postion_if_large</span> <span class="o">=</span> <span class="n">max_exact</span> <span class="o">+</span> <span class="p">(</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">relative_position</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">/</span> <span class="n">max_exact</span><span class="p">)</span>
            <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">max_distance</span> <span class="o">/</span> <span class="n">max_exact</span><span class="p">)</span>
            <span class="o">*</span> <span class="p">(</span><span class="n">num_buckets</span> <span class="o">-</span> <span class="n">max_exact</span><span class="p">)</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">relative_postion_if_large</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span>
            <span class="n">relative_postion_if_large</span><span class="p">,</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">relative_postion_if_large</span><span class="p">,</span> <span class="n">num_buckets</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">relative_buckets</span> <span class="o">+=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_small</span><span class="p">,</span> <span class="n">relative_position</span><span class="p">,</span> <span class="n">relative_postion_if_large</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">relative_buckets</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeBucketPositionBias</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the CpmBeeBucketPositionBias class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object containing various parameters.</p>
<ul>
<li>num_attention_heads (int): The number of attention heads.</li>
<li>position_bias_num_buckets (int): The number of buckets for position bias.</li>
<li>position_bias_num_segment_buckets (int): The number of buckets for segment bias.</li>
<li>position_bias_max_distance (int): The maximum distance for position bias.</li>
<li>ms_dtype: The dtype for the position bias parameter.</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig">CpmBeeConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code>None</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initializes an instance of the CpmBeeBucketPositionBias class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        config (CpmBeeConfig):</span>
<span class="sd">            The configuration object containing various parameters.</span>

<span class="sd">            - num_attention_heads (int): The number of attention heads.</span>
<span class="sd">            - position_bias_num_buckets (int): The number of buckets for position bias.</span>
<span class="sd">            - position_bias_num_segment_buckets (int): The number of buckets for segment bias.</span>
<span class="sd">            - position_bias_max_distance (int): The maximum distance for position bias.</span>
<span class="sd">            - ms_dtype: The dtype for the position bias parameter.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">position_bias_num_buckets</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_segment_bucket</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">position_bias_num_segment_buckets</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_distance</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">position_bias_max_distance</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">relative_attention_bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">position_bias_num_buckets</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">position_bias_num_segment_buckets</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeBucketPositionBias</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">query_pos</span><span class="p">,</span> <span class="n">key_pos</span><span class="p">,</span> <span class="n">rel_buckets</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method forwards relative position bias embeddings based on the input query positions, key positions,
and relative buckets.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeBucketPositionBias class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeBucketPositionBias">CpmBeeBucketPositionBias</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>query_pos</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A tensor representing the positions of queries in the input sequence.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>key_pos</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A tensor representing the positions of keys in the input sequence.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rel_buckets</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A tensor containing relative position buckets.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>None</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>This method does not return any value explicitly.
The forwarded embeddings are stored in the 'embeds' variable within the method.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>AssertionError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <ul>
<li>If the number of batches in key_pos and query_pos tensors are not equal.</li>
<li>If the number of batches in rel_buckets and key_pos tensors are not equal.</li>
<li>If the number of query positions in the rel_buckets tensor does not match the query positions tensor.</li>
<li>If the number of key positions in the rel_buckets tensor does not match the key positions tensor.</li>
</ul>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_pos</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">key_pos</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">rel_buckets</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method forwards relative position bias embeddings based on the input query positions, key positions,</span>
<span class="sd">    and relative buckets.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CpmBeeBucketPositionBias): An instance of the CpmBeeBucketPositionBias class.</span>
<span class="sd">        query_pos (mindspore.Tensor): A tensor representing the positions of queries in the input sequence.</span>
<span class="sd">        key_pos (mindspore.Tensor): A tensor representing the positions of keys in the input sequence.</span>
<span class="sd">        rel_buckets (mindspore.Tensor): A tensor containing relative position buckets.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None: This method does not return any value explicitly.</span>
<span class="sd">            The forwarded embeddings are stored in the &#39;embeds&#39; variable within the method.</span>

<span class="sd">    Raises:</span>
<span class="sd">        AssertionError:</span>
<span class="sd">            - If the number of batches in key_pos and query_pos tensors are not equal.</span>
<span class="sd">            - If the number of batches in rel_buckets and key_pos tensors are not equal.</span>
<span class="sd">            - If the number of query positions in the rel_buckets tensor does not match the query positions tensor.</span>
<span class="sd">            - If the number of key positions in the rel_buckets tensor does not match the key positions tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="n">key_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">keylen</span> <span class="o">=</span> <span class="n">key_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">querylen</span> <span class="o">=</span> <span class="n">query_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">key_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">query_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;key_pos.shape[0] should be equal to query_pos.shape[0], but got </span><span class="si">{</span><span class="n">key_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">query_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">!&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">rel_buckets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">batch</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;rel_buckets.shape[0] should be equal to batch, but got </span><span class="si">{</span><span class="n">rel_buckets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">batch</span><span class="si">}</span><span class="s2">!&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">rel_buckets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">querylen</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;rel_buckets.shape[1] should be equal to querylen, but got </span><span class="si">{</span><span class="n">rel_buckets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">querylen</span><span class="si">}</span><span class="s2">!&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">rel_buckets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">keylen</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;rel_buckets.shape[2] should be equal to keylen, but got </span><span class="si">{</span><span class="n">rel_buckets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">keylen</span><span class="si">}</span><span class="s2">!&quot;</span>
        <span class="p">)</span>

    <span class="n">relative_position_bucket</span> <span class="o">=</span> <span class="n">rel_buckets</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span>

    <span class="n">inner_segment_bucket</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_position_bucket</span><span class="p">(</span>
        <span class="n">key_pos</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">query_pos</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">],</span>
        <span class="n">num_buckets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">,</span>
        <span class="n">max_distance</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_distance</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">relative_position_bucket</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">rel_buckets</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">inner_segment_bucket</span><span class="p">,</span>
        <span class="n">relative_position_bucket</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">embeds</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">relative_position_bucket</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">relative_attention_bias</span><span class="p">)</span>
    <span class="n">embeds</span> <span class="o">=</span> <span class="n">embeds</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">embeds</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This class represents a dense gated activation module in the CpmBee framework.
It performs a nonlinear transformation on an input tensor from one feature space to another using
a gated activation function.</p>
<p>The class inherits from the <code>nn.Module</code> class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT.w_0">w_0</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeLinear class representing the first linear transformation.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear">CpmBeeLinear</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT.w_1">w_1</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeLinear class representing the second linear transformation.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear">CpmBeeLinear</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT.act">act</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of the GELU activation function.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.GELU">GELU</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT.__init__" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the CpmBeeDenseGatedACT class.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT.forward" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Transforms an input tensor from one feature space to another via a nonlinear operation.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeDenseGatedACT</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents a dense gated activation module in the CpmBee framework.</span>
<span class="sd">    It performs a nonlinear transformation on an input tensor from one feature space to another using</span>
<span class="sd">    a gated activation function.</span>

<span class="sd">    The class inherits from the `nn.Module` class.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        w_0 (CpmBeeLinear): An instance of the CpmBeeLinear class representing the first linear transformation.</span>
<span class="sd">        w_1 (CpmBeeLinear): An instance of the CpmBeeLinear class representing the second linear transformation.</span>
<span class="sd">        act (nn.GELU): An instance of the GELU activation function.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__: Initializes the CpmBeeDenseGatedACT class.</span>
<span class="sd">        forward: Transforms an input tensor from one feature space to another via a nonlinear operation.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a new instance of the CpmBeeDenseGatedACT class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The current CpmBeeDenseGatedACT object.</span>
<span class="sd">            config (CpmBeeConfig): An instance of the CpmBeeConfig class containing configuration parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_0</span> <span class="o">=</span> <span class="n">CpmBeeLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">dim_ff</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_1</span> <span class="o">=</span> <span class="n">CpmBeeLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">dim_ff</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Transform an input tensor from one feature space to another via a nonlinear operation</span>

<span class="sd">        Args:</span>
<span class="sd">            hidden_states (`mindspore.Tensor` of shape `(batch, seq_len, dim_in)`)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">gate_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_0</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_1</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">gate_score</span> <span class="o">*</span> <span class="n">hidden_states</span>
        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeDenseGatedACT</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a new instance of the CpmBeeDenseGatedACT class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The current CpmBeeDenseGatedACT object.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeConfig class containing configuration parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig">CpmBeeConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a new instance of the CpmBeeDenseGatedACT class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The current CpmBeeDenseGatedACT object.</span>
<span class="sd">        config (CpmBeeConfig): An instance of the CpmBeeConfig class containing configuration parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w_0</span> <span class="o">=</span> <span class="n">CpmBeeLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">dim_ff</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w_1</span> <span class="o">=</span> <span class="n">CpmBeeLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">dim_ff</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeDenseGatedACT</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeDenseGatedACT.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Transform an input tensor from one feature space to another via a nonlinear operation</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Transform an input tensor from one feature space to another via a nonlinear operation</span>

<span class="sd">    Args:</span>
<span class="sd">        hidden_states (`mindspore.Tensor` of shape `(batch, seq_len, dim_in)`)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gate_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_0</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_1</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">gate_score</span> <span class="o">*</span> <span class="n">hidden_states</span>
    <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Embedding">Embedding</span></code></p>


        <p>Contains a RotaryEmbedding.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeEmbeddingExt</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Contains a RotaryEmbedding.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the CpmBeeEmbeddingExt object.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the CpmBeeEmbeddingExt class.</span>
<span class="sd">            config (CpmBeeConfig):</span>
<span class="sd">                An instance of CpmBeeConfig containing configuration parameters for the embedding.</span>

<span class="sd">                - vocab_size (int): The size of the vocabulary.</span>
<span class="sd">                - hidden_size (int): The size of the hidden layer.</span>
<span class="sd">                - ms_dtype: The data type for model parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span> <span class="o">=</span> <span class="n">CpmBeeRotaryEmbedding</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ids_sub</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Construct and return the embeddings of the given input IDs and sub-IDs for the CpmBeeEmbeddingExt class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeEmbeddingExt): An instance of the CpmBeeEmbeddingExt class.</span>
<span class="sd">            ids (mindspore.Tensor):</span>
<span class="sd">                The input IDs tensor:</span>

<span class="sd">                - Shape: (batch_size, sequence_length).</span>
<span class="sd">                - Type: int32 or int64.</span>
<span class="sd">                - Purpose: Represent the input IDs for which embeddings need to be forwarded.</span>
<span class="sd">            ids_sub (mindspore.Tensor):</span>
<span class="sd">                The sub-IDs tensor.</span>

<span class="sd">                - Shape: (batch_size, sequence_length).</span>
<span class="sd">                - Type: int32 or int64.</span>
<span class="sd">                - Purpose: Represent the sub-IDs for modifying the embeddings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">embeds</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_model</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span><span class="p">(</span><span class="n">embeds</span><span class="p">,</span> <span class="n">ids_sub</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">projection</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ext_table</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method projects the input tensor &#39;x&#39; using a dense layer and optionally concatenates it with another tensor &#39;ext_table&#39;.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: Instance of the class CpmBeeEmbeddingExt.</span>
<span class="sd">            x (mindspore.Tensor): Input tensor to be projected. It should have a shape compatible with the weight tensor.</span>
<span class="sd">            ext_table (Optional[mindspore.Tensor], optional): Additional tensor to be concatenated with the projected tensor &#39;x&#39;.</span>
<span class="sd">                It should have a compatible shape with &#39;x&#39;. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            mindspore.Tensor or None: The projected tensor &#39;x&#39; after applying the dense layer operation.</span>
<span class="sd">                If &#39;ext_table&#39; is provided and has a non-zero shape, the concatenated tensor is returned.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_model</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ext_table</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="mi">0</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ext_table</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="n">logits_ext</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ext_table</span><span class="p">)</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">logits</span><span class="p">,</span> <span class="n">logits_ext</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeEmbeddingExt</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initialize the CpmBeeEmbeddingExt object.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CpmBeeEmbeddingExt class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of CpmBeeConfig containing configuration parameters for the embedding.</p>
<ul>
<li>vocab_size (int): The size of the vocabulary.</li>
<li>hidden_size (int): The size of the hidden layer.</li>
<li>ms_dtype: The data type for model parameters.</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig">CpmBeeConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialize the CpmBeeEmbeddingExt object.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the CpmBeeEmbeddingExt class.</span>
<span class="sd">        config (CpmBeeConfig):</span>
<span class="sd">            An instance of CpmBeeConfig containing configuration parameters for the embedding.</span>

<span class="sd">            - vocab_size (int): The size of the vocabulary.</span>
<span class="sd">            - hidden_size (int): The size of the hidden layer.</span>
<span class="sd">            - ms_dtype: The data type for model parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dim_model</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span> <span class="o">=</span> <span class="n">CpmBeeRotaryEmbedding</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeEmbeddingExt</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">ids_sub</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Construct and return the embeddings of the given input IDs and sub-IDs for the CpmBeeEmbeddingExt class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeEmbeddingExt class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt">CpmBeeEmbeddingExt</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input IDs tensor:</p>
<ul>
<li>Shape: (batch_size, sequence_length).</li>
<li>Type: int32 or int64.</li>
<li>Purpose: Represent the input IDs for which embeddings need to be forwarded.</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ids_sub</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sub-IDs tensor.</p>
<ul>
<li>Shape: (batch_size, sequence_length).</li>
<li>Type: int32 or int64.</li>
<li>Purpose: Represent the sub-IDs for modifying the embeddings.</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ids_sub</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct and return the embeddings of the given input IDs and sub-IDs for the CpmBeeEmbeddingExt class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CpmBeeEmbeddingExt): An instance of the CpmBeeEmbeddingExt class.</span>
<span class="sd">        ids (mindspore.Tensor):</span>
<span class="sd">            The input IDs tensor:</span>

<span class="sd">            - Shape: (batch_size, sequence_length).</span>
<span class="sd">            - Type: int32 or int64.</span>
<span class="sd">            - Purpose: Represent the input IDs for which embeddings need to be forwarded.</span>
<span class="sd">        ids_sub (mindspore.Tensor):</span>
<span class="sd">            The sub-IDs tensor.</span>

<span class="sd">            - Shape: (batch_size, sequence_length).</span>
<span class="sd">            - Type: int32 or int64.</span>
<span class="sd">            - Purpose: Represent the sub-IDs for modifying the embeddings.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">embeds</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_model</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span><span class="p">(</span><span class="n">embeds</span><span class="p">,</span> <span class="n">ids_sub</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt.projection" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeEmbeddingExt</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ext_table</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEmbeddingExt.projection" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method projects the input tensor 'x' using a dense layer and optionally concatenates it with another tensor 'ext_table'.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Instance of the class CpmBeeEmbeddingExt.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input tensor to be projected. It should have a shape compatible with the weight tensor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ext_table</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Additional tensor to be concatenated with the projected tensor 'x'.
It should have a compatible shape with 'x'. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>mindspore.Tensor or None: The projected tensor 'x' after applying the dense layer operation.
If 'ext_table' is provided and has a non-zero shape, the concatenated tensor is returned.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">projection</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">ext_table</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method projects the input tensor &#39;x&#39; using a dense layer and optionally concatenates it with another tensor &#39;ext_table&#39;.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: Instance of the class CpmBeeEmbeddingExt.</span>
<span class="sd">        x (mindspore.Tensor): Input tensor to be projected. It should have a shape compatible with the weight tensor.</span>
<span class="sd">        ext_table (Optional[mindspore.Tensor], optional): Additional tensor to be concatenated with the projected tensor &#39;x&#39;.</span>
<span class="sd">            It should have a compatible shape with &#39;x&#39;. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        mindspore.Tensor or None: The projected tensor &#39;x&#39; after applying the dense layer operation.</span>
<span class="sd">            If &#39;ext_table&#39; is provided and has a non-zero shape, the concatenated tensor is returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_model</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ext_table</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="mi">0</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ext_table</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="n">logits_ext</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ext_table</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">logits</span><span class="p">,</span> <span class="n">logits_ext</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>CpmBeeEncoder is a class that represents an encoder module for the CpmBeeTransformer model.
This class inherits from nn.Module and is responsible for processing input data through multiple transformer blocks.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder.num_layers">num_layers</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The number of transformer blocks in the encoder.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder.layers">layers</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>List of CpmBeeTransformerBlock instances representing each transformer block in the encoder.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.ModuleList">ModuleList</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder.output_layernorm">output_layernorm</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Layer normalization module for the encoder output.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm">CpmBeeLayerNorm</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder.__init__" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the CpmBeeEncoder instance with the provided configuration.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder.forward" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Processes the input hidden_states through the encoder layers.</p>
<p>Args:</p>
<ul>
<li>hidden_states (mindspore.Tensor): Input tensor of shape (batch, seq_len, dim_model).</li>
<li>attention_mask (mindspore.Tensor):
Tensor to mask invalid areas during calculation of shape (batch, seq_len, seq_len).</li>
<li>position_bias (mindspore.Tensor):
Tensor providing position information to the attention mechanism of shape (num_heads, seq_len, seq_len).</li>
<li>output_attentions (bool, optional): Indicates whether to return attention tensors of all layers.</li>
<li>output_hidden_states (bool, optional): Indicates whether to return hidden states of all layers.</li>
<li>past_key_values (Tuple[mindspore.Tensor, mindspore.Tensor], optional): Cached past key and value projection states.</li>
<li>use_cache (bool, optional): If True, past key and value states are returned for speeding up decoding.</li>
</ul>
<p>Returns:</p>
<ul>
<li>mindspore.Tensor: Processed hidden states after passing through all encoder layers.</li>
<li>Tuple[mindspore.Tensor, ...]: Cached key values if 'use_cache' is enabled.</li>
<li>Tuple[mindspore.Tensor, ...]: Hidden states of all layers if 'output_hidden_states' is enabled.</li>
<li>Tuple[mindspore.Tensor, ...]: Attention weights of all layers if 'output_attentions' is enabled.</li>
</ul>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    CpmBeeEncoder is a class that represents an encoder module for the CpmBeeTransformer model.</span>
<span class="sd">    This class inherits from nn.Module and is responsible for processing input data through multiple transformer blocks.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        num_layers (int): The number of transformer blocks in the encoder.</span>
<span class="sd">        layers (nn.ModuleList): List of CpmBeeTransformerBlock instances representing each transformer block in the encoder.</span>
<span class="sd">        output_layernorm (CpmBeeLayerNorm): Layer normalization module for the encoder output.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__:</span>
<span class="sd">            Initializes the CpmBeeEncoder instance with the provided configuration.</span>

<span class="sd">        forward:</span>
<span class="sd">            Processes the input hidden_states through the encoder layers.</span>

<span class="sd">             Args:</span>

<span class="sd">            - hidden_states (mindspore.Tensor): Input tensor of shape (batch, seq_len, dim_model).</span>
<span class="sd">            - attention_mask (mindspore.Tensor):</span>
<span class="sd">            Tensor to mask invalid areas during calculation of shape (batch, seq_len, seq_len).</span>
<span class="sd">            - position_bias (mindspore.Tensor):</span>
<span class="sd">            Tensor providing position information to the attention mechanism of shape (num_heads, seq_len, seq_len).</span>
<span class="sd">            - output_attentions (bool, optional): Indicates whether to return attention tensors of all layers.</span>
<span class="sd">            - output_hidden_states (bool, optional): Indicates whether to return hidden states of all layers.</span>
<span class="sd">            - past_key_values (Tuple[mindspore.Tensor, mindspore.Tensor], optional): Cached past key and value projection states.</span>
<span class="sd">            - use_cache (bool, optional): If True, past key and value states are returned for speeding up decoding.</span>

<span class="sd">            Returns:</span>

<span class="sd">            - mindspore.Tensor: Processed hidden states after passing through all encoder layers.</span>
<span class="sd">            - Tuple[mindspore.Tensor, ...]: Cached key values if &#39;use_cache&#39; is enabled.</span>
<span class="sd">            - Tuple[mindspore.Tensor, ...]: Hidden states of all layers if &#39;output_hidden_states&#39; is enabled.</span>
<span class="sd">            - Tuple[mindspore.Tensor, ...]: Attention weights of all layers if &#39;output_attentions&#39; is enabled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a new instance of the CpmBeeEncoder class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the CpmBeeEncoder class.</span>
<span class="sd">            config (CpmBeeConfig): An instance of the CpmBeeConfig class containing configuration parameters for the encoder.</span>
<span class="sd">                This parameter is used to configure the encoder&#39;s behavior and settings.</span>
<span class="sd">                The config parameter must be of type CpmBeeConfig.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            AssertionError: If the length of config.mask_modules does not equal the number of hidden layers specified in config.</span>
<span class="sd">            AssertionError: If the length of mask_module within config.mask_modules is not 2 for each mask_module in the list.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">mask_modules</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">mask_modules</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="s2">&quot;The total number of masks should equal to num_layers&quot;</span>
            <span class="k">for</span> <span class="n">mask_module</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">mask_modules</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">mask_module</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;For encoder, each mask should be (mask_att, mask_ffn)&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">config</span><span class="o">.</span><span class="n">mask_modules</span> <span class="o">=</span> <span class="p">[(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">CpmBeeTransformerBlock</span><span class="p">(</span>
                    <span class="n">config</span><span class="p">,</span> <span class="n">mask_att</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">mask_modules</span><span class="p">[</span><span class="n">ith</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">mask_ffn</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">mask_modules</span><span class="p">[</span><span class="n">ith</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">ith</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">output_layernorm</span> <span class="o">=</span> <span class="n">CpmBeeLayerNorm</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">position_bias</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_states (`mindspore.Tensor`):</span>
<span class="sd">                Input to the layer of shape `(batch, seq_len, dim_model)`</span>
<span class="sd">            attention_mask (`mindspore.Tensor`):</span>
<span class="sd">                Avoid invalid areas to participate in the calculation of shape `(batch, seq_len, seq_len)`</span>
<span class="sd">            position_bias (`mindspore.Tensor`):</span>
<span class="sd">                Provides position information to attention mechanism of shape `(num_heads, seq_len, seq_len)`</span>
<span class="sd">            output_attentions (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers.</span>
<span class="sd">            output_hidden_states (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the hidden states of all layers.</span>
<span class="sd">            past_key_values (`Tuple[mindspore.Tensor, mindspore.Tensor])`, *optional*):</span>
<span class="sd">                Cached past key and value projection states</span>
<span class="sd">            use_cache (`bool`, *optional*):</span>
<span class="sd">                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding</span>
<span class="sd">                (see `past_key_values`).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_self_attns</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">current_key_values</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">position_bias</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">current_key_value</span> <span class="o">=</span> <span class="n">layer_outputs</span>
            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                <span class="n">all_self_attns</span> <span class="o">+=</span> <span class="p">(</span><span class="n">attn_weights</span><span class="p">,)</span>
            <span class="k">if</span> <span class="n">current_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">current_key_values</span> <span class="o">=</span> <span class="n">current_key_values</span> <span class="o">+</span> <span class="p">(</span><span class="n">current_key_value</span><span class="p">,)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">current_key_values</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_self_attns</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeEncoder</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a new instance of the CpmBeeEncoder class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CpmBeeEncoder class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeConfig class containing configuration parameters for the encoder.
This parameter is used to configure the encoder's behavior and settings.
The config parameter must be of type CpmBeeConfig.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig">CpmBeeConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>AssertionError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the length of config.mask_modules does not equal the number of hidden layers specified in config.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>AssertionError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the length of mask_module within config.mask_modules is not 2 for each mask_module in the list.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a new instance of the CpmBeeEncoder class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the CpmBeeEncoder class.</span>
<span class="sd">        config (CpmBeeConfig): An instance of the CpmBeeConfig class containing configuration parameters for the encoder.</span>
<span class="sd">            This parameter is used to configure the encoder&#39;s behavior and settings.</span>
<span class="sd">            The config parameter must be of type CpmBeeConfig.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        AssertionError: If the length of config.mask_modules does not equal the number of hidden layers specified in config.</span>
<span class="sd">        AssertionError: If the length of mask_module within config.mask_modules is not 2 for each mask_module in the list.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">mask_modules</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">mask_modules</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="s2">&quot;The total number of masks should equal to num_layers&quot;</span>
        <span class="k">for</span> <span class="n">mask_module</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">mask_modules</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">mask_module</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;For encoder, each mask should be (mask_att, mask_ffn)&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">config</span><span class="o">.</span><span class="n">mask_modules</span> <span class="o">=</span> <span class="p">[(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">CpmBeeTransformerBlock</span><span class="p">(</span>
                <span class="n">config</span><span class="p">,</span> <span class="n">mask_att</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">mask_modules</span><span class="p">[</span><span class="n">ith</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">mask_ffn</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">mask_modules</span><span class="p">[</span><span class="n">ith</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">ith</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">output_layernorm</span> <span class="o">=</span> <span class="n">CpmBeeLayerNorm</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeEncoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">position_bias</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeEncoder.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input to the layer of shape <code>(batch, seq_len, dim_model)</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Avoid invalid areas to participate in the calculation of shape <code>(batch, seq_len, seq_len)</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Provides position information to attention mechanism of shape <code>(num_heads, seq_len, seq_len)</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the attentions tensors of all attention layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the hidden states of all layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_key_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Cached past key and value projection states</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple[mindspore.Tensor, mindspore.Tensor])`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding
(see <code>past_key_values</code>).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">position_bias</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        hidden_states (`mindspore.Tensor`):</span>
<span class="sd">            Input to the layer of shape `(batch, seq_len, dim_model)`</span>
<span class="sd">        attention_mask (`mindspore.Tensor`):</span>
<span class="sd">            Avoid invalid areas to participate in the calculation of shape `(batch, seq_len, seq_len)`</span>
<span class="sd">        position_bias (`mindspore.Tensor`):</span>
<span class="sd">            Provides position information to attention mechanism of shape `(num_heads, seq_len, seq_len)`</span>
<span class="sd">        output_attentions (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the attentions tensors of all attention layers.</span>
<span class="sd">        output_hidden_states (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the hidden states of all layers.</span>
<span class="sd">        past_key_values (`Tuple[mindspore.Tensor, mindspore.Tensor])`, *optional*):</span>
<span class="sd">            Cached past key and value projection states</span>
<span class="sd">        use_cache (`bool`, *optional*):</span>
<span class="sd">            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding</span>
<span class="sd">            (see `past_key_values`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">all_self_attns</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">current_key_values</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
        <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">position_bias</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">current_key_value</span> <span class="o">=</span> <span class="n">layer_outputs</span>
        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">all_self_attns</span> <span class="o">+=</span> <span class="p">(</span><span class="n">attn_weights</span><span class="p">,)</span>
        <span class="k">if</span> <span class="n">current_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">current_key_values</span> <span class="o">=</span> <span class="n">current_key_values</span> <span class="o">+</span> <span class="p">(</span><span class="n">current_key_value</span><span class="p">,)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
        <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">current_key_values</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_self_attns</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This class represents a feed-forward block in the CpmBee model. It is used to process hidden states before the feed-forward layer.</p>
<p>The CpmBeeFFNBlock class inherits from nn.Module.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock.layernorm_before_ffn">layernorm_before_ffn</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeLayerNorm class that performs layer normalization before the feed-forward layer.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm">CpmBeeLayerNorm</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock.ffn">ffn</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeFeedForward class that represents the feed-forward layer.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward">CpmBeeFeedForward</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock.dropout">dropout</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An optional dropout layer. If None, no dropout is applied.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.Dropout">Dropout</span> or None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock.__init__" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the CpmBeeFFNBlock object.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock.forward" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Processes the hidden states before the feed-forward layer.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeFFNBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents a feed-forward block in the CpmBee model. It is used to process hidden states before the feed-forward layer.</span>

<span class="sd">    The CpmBeeFFNBlock class inherits from nn.Module.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        layernorm_before_ffn (CpmBeeLayerNorm): An instance of the CpmBeeLayerNorm class that performs layer normalization before the feed-forward layer.</span>
<span class="sd">        ffn (CpmBeeFeedForward): An instance of the CpmBeeFeedForward class that represents the feed-forward layer.</span>
<span class="sd">        dropout (nn.Dropout or None): An optional dropout layer. If None, no dropout is applied.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__: Initializes the CpmBeeFFNBlock object.</span>
<span class="sd">        forward: Processes the hidden states before the feed-forward layer.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a CpmBeeFFNBlock instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The current object instance.</span>
<span class="sd">            config (CpmBeeConfig): The configuration object containing the parameters for the CpmBeeFFNBlock.</span>
<span class="sd">                This object must be an instance of CpmBeeConfig class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_before_ffn</span> <span class="o">=</span> <span class="n">CpmBeeLayerNorm</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">CpmBeeFeedForward</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_states (`mindspore.Tensor` of shape `(batch, len_seq, dim_model)`):</span>
<span class="sd">                Hidden states before feed forward layer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ln_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_before_ffn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">ln_outputs</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1.05</span>
        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeFFNBlock</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a CpmBeeFFNBlock instance.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The current object instance.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object containing the parameters for the CpmBeeFFNBlock.
This object must be an instance of CpmBeeConfig class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig">CpmBeeConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a CpmBeeFFNBlock instance.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The current object instance.</span>
<span class="sd">        config (CpmBeeConfig): The configuration object containing the parameters for the CpmBeeFFNBlock.</span>
<span class="sd">            This object must be an instance of CpmBeeConfig class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_before_ffn</span> <span class="o">=</span> <span class="n">CpmBeeLayerNorm</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">CpmBeeFeedForward</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeFFNBlock</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFFNBlock.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Hidden states before feed forward layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch, len_seq, dim_model)`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        hidden_states (`mindspore.Tensor` of shape `(batch, len_seq, dim_model)`):</span>
<span class="sd">            Hidden states before feed forward layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ln_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_before_ffn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">ln_outputs</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1.05</span>
    <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This class represents a feedforward neural network layer for the CpmBee model.
It consists of a dense gated activation layer (<code>CpmBeeDenseGatedACT</code>), optional dropout layer,
and a linear transformation layer (<code>CpmBeeLinear</code>).</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward.w_in">w_in</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Instance of <code>CpmBeeDenseGatedACT</code> for processing input hidden states.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward.dropout">dropout</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Optional dropout layer for regularization.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward.w_out">w_out</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Instance of <code>CpmBeeLinear</code> for transforming hidden states to output.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward.__init__" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructor method initializing the feedforward layer.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward.forward" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Method for processing input hidden states through the feedforward layer.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Configuration object of type <code>CpmBeeConfig</code> containing layer specifications.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig">CpmBeeConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input tensor of shape <code>(batch, seq_len, dim_in)</code> representing hidden states.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>mindspore.Tensor: Transformed hidden states after passing through the feedforward layer.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents a feedforward neural network layer for the CpmBee model.</span>
<span class="sd">    It consists of a dense gated activation layer (`CpmBeeDenseGatedACT`), optional dropout layer,</span>
<span class="sd">    and a linear transformation layer (`CpmBeeLinear`).</span>

<span class="sd">    Attributes:</span>
<span class="sd">        w_in: Instance of `CpmBeeDenseGatedACT` for processing input hidden states.</span>
<span class="sd">        dropout: Optional dropout layer for regularization.</span>
<span class="sd">        w_out: Instance of `CpmBeeLinear` for transforming hidden states to output.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__: Constructor method initializing the feedforward layer.</span>
<span class="sd">        forward: Method for processing input hidden states through the feedforward layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        config: Configuration object of type `CpmBeeConfig` containing layer specifications.</span>
<span class="sd">        hidden_states: Input tensor of shape `(batch, seq_len, dim_in)` representing hidden states.</span>

<span class="sd">    Returns:</span>
<span class="sd">        mindspore.Tensor: Transformed hidden states after passing through the feedforward layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the CpmBeeFeedForward class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the class.</span>
<span class="sd">            config (CpmBeeConfig): An object of the CpmBeeConfig class containing configuration parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_in</span> <span class="o">=</span> <span class="n">CpmBeeDenseGatedACT</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">w_out</span> <span class="o">=</span> <span class="n">CpmBeeLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dim_ff</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_states (`mindspore.Tensor` of shape `(batch, seq_len, dim_in)`)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_in</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_out</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeFeedForward</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the CpmBeeFeedForward class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An object of the CpmBeeConfig class containing configuration parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig">CpmBeeConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the CpmBeeFeedForward class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the class.</span>
<span class="sd">        config (CpmBeeConfig): An object of the CpmBeeConfig class containing configuration parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w_in</span> <span class="o">=</span> <span class="n">CpmBeeDenseGatedACT</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">w_out</span> <span class="o">=</span> <span class="n">CpmBeeLinear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dim_ff</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeFeedForward</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeFeedForward.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        hidden_states (`mindspore.Tensor` of shape `(batch, seq_len, dim_in)`)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_in</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_out</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeePreTrainedModel" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeePreTrainedModel">CpmBeePreTrainedModel</a></code></p>


        <p>This class represents a CPMBee model for Causal Language Modeling tasks. It inherits from CpmBeePreTrainedModel and
implements methods for model initialization, inference, beam search generation, input embeddings handling, and more.</p>
<p>The class includes methods for initializing the model, forwarding the model for inference, performing inference,
getting and setting input embeddings, getting and setting output embeddings, preparing inputs for generation,
updating model kwargs for generation, reordering cache during generation, expanding inputs for generation,
adjusting logits during generation, performing beam search for generation, and generating outputs based on
input data using beam search.</p>
<p>The <code>generate</code> method processes input data using the model to generate responses, filling placeholders in the
input data with generated text. It accepts a dictionary or a list of dictionaries as input and
returns a dictionary or a list of dictionaries with the '<ans>' field filled with generated text.</p>
<p>For more details on the methods and their parameters, please refer to the method docstrings within the class
implementation.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span>
<span class="normal">2105</span>
<span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span>
<span class="normal">2109</span>
<span class="normal">2110</span>
<span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span>
<span class="normal">2141</span>
<span class="normal">2142</span>
<span class="normal">2143</span>
<span class="normal">2144</span>
<span class="normal">2145</span>
<span class="normal">2146</span>
<span class="normal">2147</span>
<span class="normal">2148</span>
<span class="normal">2149</span>
<span class="normal">2150</span>
<span class="normal">2151</span>
<span class="normal">2152</span>
<span class="normal">2153</span>
<span class="normal">2154</span>
<span class="normal">2155</span>
<span class="normal">2156</span>
<span class="normal">2157</span>
<span class="normal">2158</span>
<span class="normal">2159</span>
<span class="normal">2160</span>
<span class="normal">2161</span>
<span class="normal">2162</span>
<span class="normal">2163</span>
<span class="normal">2164</span>
<span class="normal">2165</span>
<span class="normal">2166</span>
<span class="normal">2167</span>
<span class="normal">2168</span>
<span class="normal">2169</span>
<span class="normal">2170</span>
<span class="normal">2171</span>
<span class="normal">2172</span>
<span class="normal">2173</span>
<span class="normal">2174</span>
<span class="normal">2175</span>
<span class="normal">2176</span>
<span class="normal">2177</span>
<span class="normal">2178</span>
<span class="normal">2179</span>
<span class="normal">2180</span>
<span class="normal">2181</span>
<span class="normal">2182</span>
<span class="normal">2183</span>
<span class="normal">2184</span>
<span class="normal">2185</span>
<span class="normal">2186</span>
<span class="normal">2187</span>
<span class="normal">2188</span>
<span class="normal">2189</span>
<span class="normal">2190</span>
<span class="normal">2191</span>
<span class="normal">2192</span>
<span class="normal">2193</span>
<span class="normal">2194</span>
<span class="normal">2195</span>
<span class="normal">2196</span>
<span class="normal">2197</span>
<span class="normal">2198</span>
<span class="normal">2199</span>
<span class="normal">2200</span>
<span class="normal">2201</span>
<span class="normal">2202</span>
<span class="normal">2203</span>
<span class="normal">2204</span>
<span class="normal">2205</span>
<span class="normal">2206</span>
<span class="normal">2207</span>
<span class="normal">2208</span>
<span class="normal">2209</span>
<span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span>
<span class="normal">2219</span>
<span class="normal">2220</span>
<span class="normal">2221</span>
<span class="normal">2222</span>
<span class="normal">2223</span>
<span class="normal">2224</span>
<span class="normal">2225</span>
<span class="normal">2226</span>
<span class="normal">2227</span>
<span class="normal">2228</span>
<span class="normal">2229</span>
<span class="normal">2230</span>
<span class="normal">2231</span>
<span class="normal">2232</span>
<span class="normal">2233</span>
<span class="normal">2234</span>
<span class="normal">2235</span>
<span class="normal">2236</span>
<span class="normal">2237</span>
<span class="normal">2238</span>
<span class="normal">2239</span>
<span class="normal">2240</span>
<span class="normal">2241</span>
<span class="normal">2242</span>
<span class="normal">2243</span>
<span class="normal">2244</span>
<span class="normal">2245</span>
<span class="normal">2246</span>
<span class="normal">2247</span>
<span class="normal">2248</span>
<span class="normal">2249</span>
<span class="normal">2250</span>
<span class="normal">2251</span>
<span class="normal">2252</span>
<span class="normal">2253</span>
<span class="normal">2254</span>
<span class="normal">2255</span>
<span class="normal">2256</span>
<span class="normal">2257</span>
<span class="normal">2258</span>
<span class="normal">2259</span>
<span class="normal">2260</span>
<span class="normal">2261</span>
<span class="normal">2262</span>
<span class="normal">2263</span>
<span class="normal">2264</span>
<span class="normal">2265</span>
<span class="normal">2266</span>
<span class="normal">2267</span>
<span class="normal">2268</span>
<span class="normal">2269</span>
<span class="normal">2270</span>
<span class="normal">2271</span>
<span class="normal">2272</span>
<span class="normal">2273</span>
<span class="normal">2274</span>
<span class="normal">2275</span>
<span class="normal">2276</span>
<span class="normal">2277</span>
<span class="normal">2278</span>
<span class="normal">2279</span>
<span class="normal">2280</span>
<span class="normal">2281</span>
<span class="normal">2282</span>
<span class="normal">2283</span>
<span class="normal">2284</span>
<span class="normal">2285</span>
<span class="normal">2286</span>
<span class="normal">2287</span>
<span class="normal">2288</span>
<span class="normal">2289</span>
<span class="normal">2290</span>
<span class="normal">2291</span>
<span class="normal">2292</span>
<span class="normal">2293</span>
<span class="normal">2294</span>
<span class="normal">2295</span>
<span class="normal">2296</span>
<span class="normal">2297</span>
<span class="normal">2298</span>
<span class="normal">2299</span>
<span class="normal">2300</span>
<span class="normal">2301</span>
<span class="normal">2302</span>
<span class="normal">2303</span>
<span class="normal">2304</span>
<span class="normal">2305</span>
<span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span>
<span class="normal">2313</span>
<span class="normal">2314</span>
<span class="normal">2315</span>
<span class="normal">2316</span>
<span class="normal">2317</span>
<span class="normal">2318</span>
<span class="normal">2319</span>
<span class="normal">2320</span>
<span class="normal">2321</span>
<span class="normal">2322</span>
<span class="normal">2323</span>
<span class="normal">2324</span>
<span class="normal">2325</span>
<span class="normal">2326</span>
<span class="normal">2327</span>
<span class="normal">2328</span>
<span class="normal">2329</span>
<span class="normal">2330</span>
<span class="normal">2331</span>
<span class="normal">2332</span>
<span class="normal">2333</span>
<span class="normal">2334</span>
<span class="normal">2335</span>
<span class="normal">2336</span>
<span class="normal">2337</span>
<span class="normal">2338</span>
<span class="normal">2339</span>
<span class="normal">2340</span>
<span class="normal">2341</span>
<span class="normal">2342</span>
<span class="normal">2343</span>
<span class="normal">2344</span>
<span class="normal">2345</span>
<span class="normal">2346</span>
<span class="normal">2347</span>
<span class="normal">2348</span>
<span class="normal">2349</span>
<span class="normal">2350</span>
<span class="normal">2351</span>
<span class="normal">2352</span>
<span class="normal">2353</span>
<span class="normal">2354</span>
<span class="normal">2355</span>
<span class="normal">2356</span>
<span class="normal">2357</span>
<span class="normal">2358</span>
<span class="normal">2359</span>
<span class="normal">2360</span>
<span class="normal">2361</span>
<span class="normal">2362</span>
<span class="normal">2363</span>
<span class="normal">2364</span>
<span class="normal">2365</span>
<span class="normal">2366</span>
<span class="normal">2367</span>
<span class="normal">2368</span>
<span class="normal">2369</span>
<span class="normal">2370</span>
<span class="normal">2371</span>
<span class="normal">2372</span>
<span class="normal">2373</span>
<span class="normal">2374</span>
<span class="normal">2375</span>
<span class="normal">2376</span>
<span class="normal">2377</span>
<span class="normal">2378</span>
<span class="normal">2379</span>
<span class="normal">2380</span>
<span class="normal">2381</span>
<span class="normal">2382</span>
<span class="normal">2383</span>
<span class="normal">2384</span>
<span class="normal">2385</span>
<span class="normal">2386</span>
<span class="normal">2387</span>
<span class="normal">2388</span>
<span class="normal">2389</span>
<span class="normal">2390</span>
<span class="normal">2391</span>
<span class="normal">2392</span>
<span class="normal">2393</span>
<span class="normal">2394</span>
<span class="normal">2395</span>
<span class="normal">2396</span>
<span class="normal">2397</span>
<span class="normal">2398</span>
<span class="normal">2399</span>
<span class="normal">2400</span>
<span class="normal">2401</span>
<span class="normal">2402</span>
<span class="normal">2403</span>
<span class="normal">2404</span>
<span class="normal">2405</span>
<span class="normal">2406</span>
<span class="normal">2407</span>
<span class="normal">2408</span>
<span class="normal">2409</span>
<span class="normal">2410</span>
<span class="normal">2411</span>
<span class="normal">2412</span>
<span class="normal">2413</span>
<span class="normal">2414</span>
<span class="normal">2415</span>
<span class="normal">2416</span>
<span class="normal">2417</span>
<span class="normal">2418</span>
<span class="normal">2419</span>
<span class="normal">2420</span>
<span class="normal">2421</span>
<span class="normal">2422</span>
<span class="normal">2423</span>
<span class="normal">2424</span>
<span class="normal">2425</span>
<span class="normal">2426</span>
<span class="normal">2427</span>
<span class="normal">2428</span>
<span class="normal">2429</span>
<span class="normal">2430</span>
<span class="normal">2431</span>
<span class="normal">2432</span>
<span class="normal">2433</span>
<span class="normal">2434</span>
<span class="normal">2435</span>
<span class="normal">2436</span>
<span class="normal">2437</span>
<span class="normal">2438</span>
<span class="normal">2439</span>
<span class="normal">2440</span>
<span class="normal">2441</span>
<span class="normal">2442</span>
<span class="normal">2443</span>
<span class="normal">2444</span>
<span class="normal">2445</span>
<span class="normal">2446</span>
<span class="normal">2447</span>
<span class="normal">2448</span>
<span class="normal">2449</span>
<span class="normal">2450</span>
<span class="normal">2451</span>
<span class="normal">2452</span>
<span class="normal">2453</span>
<span class="normal">2454</span>
<span class="normal">2455</span>
<span class="normal">2456</span>
<span class="normal">2457</span>
<span class="normal">2458</span>
<span class="normal">2459</span>
<span class="normal">2460</span>
<span class="normal">2461</span>
<span class="normal">2462</span>
<span class="normal">2463</span>
<span class="normal">2464</span>
<span class="normal">2465</span>
<span class="normal">2466</span>
<span class="normal">2467</span>
<span class="normal">2468</span>
<span class="normal">2469</span>
<span class="normal">2470</span>
<span class="normal">2471</span>
<span class="normal">2472</span>
<span class="normal">2473</span>
<span class="normal">2474</span>
<span class="normal">2475</span>
<span class="normal">2476</span>
<span class="normal">2477</span>
<span class="normal">2478</span>
<span class="normal">2479</span>
<span class="normal">2480</span>
<span class="normal">2481</span>
<span class="normal">2482</span>
<span class="normal">2483</span>
<span class="normal">2484</span>
<span class="normal">2485</span>
<span class="normal">2486</span>
<span class="normal">2487</span>
<span class="normal">2488</span>
<span class="normal">2489</span>
<span class="normal">2490</span>
<span class="normal">2491</span>
<span class="normal">2492</span>
<span class="normal">2493</span>
<span class="normal">2494</span>
<span class="normal">2495</span>
<span class="normal">2496</span>
<span class="normal">2497</span>
<span class="normal">2498</span>
<span class="normal">2499</span>
<span class="normal">2500</span>
<span class="normal">2501</span>
<span class="normal">2502</span>
<span class="normal">2503</span>
<span class="normal">2504</span>
<span class="normal">2505</span>
<span class="normal">2506</span>
<span class="normal">2507</span>
<span class="normal">2508</span>
<span class="normal">2509</span>
<span class="normal">2510</span>
<span class="normal">2511</span>
<span class="normal">2512</span>
<span class="normal">2513</span>
<span class="normal">2514</span>
<span class="normal">2515</span>
<span class="normal">2516</span>
<span class="normal">2517</span>
<span class="normal">2518</span>
<span class="normal">2519</span>
<span class="normal">2520</span>
<span class="normal">2521</span>
<span class="normal">2522</span>
<span class="normal">2523</span>
<span class="normal">2524</span>
<span class="normal">2525</span>
<span class="normal">2526</span>
<span class="normal">2527</span>
<span class="normal">2528</span>
<span class="normal">2529</span>
<span class="normal">2530</span>
<span class="normal">2531</span>
<span class="normal">2532</span>
<span class="normal">2533</span>
<span class="normal">2534</span>
<span class="normal">2535</span>
<span class="normal">2536</span>
<span class="normal">2537</span>
<span class="normal">2538</span>
<span class="normal">2539</span>
<span class="normal">2540</span>
<span class="normal">2541</span>
<span class="normal">2542</span>
<span class="normal">2543</span>
<span class="normal">2544</span>
<span class="normal">2545</span>
<span class="normal">2546</span>
<span class="normal">2547</span>
<span class="normal">2548</span>
<span class="normal">2549</span>
<span class="normal">2550</span>
<span class="normal">2551</span>
<span class="normal">2552</span>
<span class="normal">2553</span>
<span class="normal">2554</span>
<span class="normal">2555</span>
<span class="normal">2556</span>
<span class="normal">2557</span>
<span class="normal">2558</span>
<span class="normal">2559</span>
<span class="normal">2560</span>
<span class="normal">2561</span>
<span class="normal">2562</span>
<span class="normal">2563</span>
<span class="normal">2564</span>
<span class="normal">2565</span>
<span class="normal">2566</span>
<span class="normal">2567</span>
<span class="normal">2568</span>
<span class="normal">2569</span>
<span class="normal">2570</span>
<span class="normal">2571</span>
<span class="normal">2572</span>
<span class="normal">2573</span>
<span class="normal">2574</span>
<span class="normal">2575</span>
<span class="normal">2576</span>
<span class="normal">2577</span>
<span class="normal">2578</span>
<span class="normal">2579</span>
<span class="normal">2580</span>
<span class="normal">2581</span>
<span class="normal">2582</span>
<span class="normal">2583</span>
<span class="normal">2584</span>
<span class="normal">2585</span>
<span class="normal">2586</span>
<span class="normal">2587</span>
<span class="normal">2588</span>
<span class="normal">2589</span>
<span class="normal">2590</span>
<span class="normal">2591</span>
<span class="normal">2592</span>
<span class="normal">2593</span>
<span class="normal">2594</span>
<span class="normal">2595</span>
<span class="normal">2596</span>
<span class="normal">2597</span>
<span class="normal">2598</span>
<span class="normal">2599</span>
<span class="normal">2600</span>
<span class="normal">2601</span>
<span class="normal">2602</span>
<span class="normal">2603</span>
<span class="normal">2604</span>
<span class="normal">2605</span>
<span class="normal">2606</span>
<span class="normal">2607</span>
<span class="normal">2608</span>
<span class="normal">2609</span>
<span class="normal">2610</span>
<span class="normal">2611</span>
<span class="normal">2612</span>
<span class="normal">2613</span>
<span class="normal">2614</span>
<span class="normal">2615</span>
<span class="normal">2616</span>
<span class="normal">2617</span>
<span class="normal">2618</span>
<span class="normal">2619</span>
<span class="normal">2620</span>
<span class="normal">2621</span>
<span class="normal">2622</span>
<span class="normal">2623</span>
<span class="normal">2624</span>
<span class="normal">2625</span>
<span class="normal">2626</span>
<span class="normal">2627</span>
<span class="normal">2628</span>
<span class="normal">2629</span>
<span class="normal">2630</span>
<span class="normal">2631</span>
<span class="normal">2632</span>
<span class="normal">2633</span>
<span class="normal">2634</span>
<span class="normal">2635</span>
<span class="normal">2636</span>
<span class="normal">2637</span>
<span class="normal">2638</span>
<span class="normal">2639</span>
<span class="normal">2640</span>
<span class="normal">2641</span>
<span class="normal">2642</span>
<span class="normal">2643</span>
<span class="normal">2644</span>
<span class="normal">2645</span>
<span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span>
<span class="normal">2652</span>
<span class="normal">2653</span>
<span class="normal">2654</span>
<span class="normal">2655</span>
<span class="normal">2656</span>
<span class="normal">2657</span>
<span class="normal">2658</span>
<span class="normal">2659</span>
<span class="normal">2660</span>
<span class="normal">2661</span>
<span class="normal">2662</span>
<span class="normal">2663</span>
<span class="normal">2664</span>
<span class="normal">2665</span>
<span class="normal">2666</span>
<span class="normal">2667</span>
<span class="normal">2668</span>
<span class="normal">2669</span>
<span class="normal">2670</span>
<span class="normal">2671</span>
<span class="normal">2672</span>
<span class="normal">2673</span>
<span class="normal">2674</span>
<span class="normal">2675</span>
<span class="normal">2676</span>
<span class="normal">2677</span>
<span class="normal">2678</span>
<span class="normal">2679</span>
<span class="normal">2680</span>
<span class="normal">2681</span>
<span class="normal">2682</span>
<span class="normal">2683</span>
<span class="normal">2684</span>
<span class="normal">2685</span>
<span class="normal">2686</span>
<span class="normal">2687</span>
<span class="normal">2688</span>
<span class="normal">2689</span>
<span class="normal">2690</span>
<span class="normal">2691</span>
<span class="normal">2692</span>
<span class="normal">2693</span>
<span class="normal">2694</span>
<span class="normal">2695</span>
<span class="normal">2696</span>
<span class="normal">2697</span>
<span class="normal">2698</span>
<span class="normal">2699</span>
<span class="normal">2700</span>
<span class="normal">2701</span>
<span class="normal">2702</span>
<span class="normal">2703</span>
<span class="normal">2704</span>
<span class="normal">2705</span>
<span class="normal">2706</span>
<span class="normal">2707</span>
<span class="normal">2708</span>
<span class="normal">2709</span>
<span class="normal">2710</span>
<span class="normal">2711</span>
<span class="normal">2712</span>
<span class="normal">2713</span>
<span class="normal">2714</span>
<span class="normal">2715</span>
<span class="normal">2716</span>
<span class="normal">2717</span>
<span class="normal">2718</span>
<span class="normal">2719</span>
<span class="normal">2720</span>
<span class="normal">2721</span>
<span class="normal">2722</span>
<span class="normal">2723</span>
<span class="normal">2724</span>
<span class="normal">2725</span>
<span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span>
<span class="normal">2733</span>
<span class="normal">2734</span>
<span class="normal">2735</span>
<span class="normal">2736</span>
<span class="normal">2737</span>
<span class="normal">2738</span>
<span class="normal">2739</span>
<span class="normal">2740</span>
<span class="normal">2741</span>
<span class="normal">2742</span>
<span class="normal">2743</span>
<span class="normal">2744</span>
<span class="normal">2745</span>
<span class="normal">2746</span>
<span class="normal">2747</span>
<span class="normal">2748</span>
<span class="normal">2749</span>
<span class="normal">2750</span>
<span class="normal">2751</span>
<span class="normal">2752</span>
<span class="normal">2753</span>
<span class="normal">2754</span>
<span class="normal">2755</span>
<span class="normal">2756</span>
<span class="normal">2757</span>
<span class="normal">2758</span>
<span class="normal">2759</span>
<span class="normal">2760</span>
<span class="normal">2761</span>
<span class="normal">2762</span>
<span class="normal">2763</span>
<span class="normal">2764</span>
<span class="normal">2765</span>
<span class="normal">2766</span>
<span class="normal">2767</span>
<span class="normal">2768</span>
<span class="normal">2769</span>
<span class="normal">2770</span>
<span class="normal">2771</span>
<span class="normal">2772</span>
<span class="normal">2773</span>
<span class="normal">2774</span>
<span class="normal">2775</span>
<span class="normal">2776</span>
<span class="normal">2777</span>
<span class="normal">2778</span>
<span class="normal">2779</span>
<span class="normal">2780</span>
<span class="normal">2781</span>
<span class="normal">2782</span>
<span class="normal">2783</span>
<span class="normal">2784</span>
<span class="normal">2785</span>
<span class="normal">2786</span>
<span class="normal">2787</span>
<span class="normal">2788</span>
<span class="normal">2789</span>
<span class="normal">2790</span>
<span class="normal">2791</span>
<span class="normal">2792</span>
<span class="normal">2793</span>
<span class="normal">2794</span>
<span class="normal">2795</span>
<span class="normal">2796</span>
<span class="normal">2797</span>
<span class="normal">2798</span>
<span class="normal">2799</span>
<span class="normal">2800</span>
<span class="normal">2801</span>
<span class="normal">2802</span>
<span class="normal">2803</span>
<span class="normal">2804</span>
<span class="normal">2805</span>
<span class="normal">2806</span>
<span class="normal">2807</span>
<span class="normal">2808</span>
<span class="normal">2809</span>
<span class="normal">2810</span>
<span class="normal">2811</span>
<span class="normal">2812</span>
<span class="normal">2813</span>
<span class="normal">2814</span>
<span class="normal">2815</span>
<span class="normal">2816</span>
<span class="normal">2817</span>
<span class="normal">2818</span>
<span class="normal">2819</span>
<span class="normal">2820</span>
<span class="normal">2821</span>
<span class="normal">2822</span>
<span class="normal">2823</span>
<span class="normal">2824</span>
<span class="normal">2825</span>
<span class="normal">2826</span>
<span class="normal">2827</span>
<span class="normal">2828</span>
<span class="normal">2829</span>
<span class="normal">2830</span>
<span class="normal">2831</span>
<span class="normal">2832</span>
<span class="normal">2833</span>
<span class="normal">2834</span>
<span class="normal">2835</span>
<span class="normal">2836</span>
<span class="normal">2837</span>
<span class="normal">2838</span>
<span class="normal">2839</span>
<span class="normal">2840</span>
<span class="normal">2841</span>
<span class="normal">2842</span>
<span class="normal">2843</span>
<span class="normal">2844</span>
<span class="normal">2845</span>
<span class="normal">2846</span>
<span class="normal">2847</span>
<span class="normal">2848</span>
<span class="normal">2849</span>
<span class="normal">2850</span>
<span class="normal">2851</span>
<span class="normal">2852</span>
<span class="normal">2853</span>
<span class="normal">2854</span>
<span class="normal">2855</span>
<span class="normal">2856</span>
<span class="normal">2857</span>
<span class="normal">2858</span>
<span class="normal">2859</span>
<span class="normal">2860</span>
<span class="normal">2861</span>
<span class="normal">2862</span>
<span class="normal">2863</span>
<span class="normal">2864</span>
<span class="normal">2865</span>
<span class="normal">2866</span>
<span class="normal">2867</span>
<span class="normal">2868</span>
<span class="normal">2869</span>
<span class="normal">2870</span>
<span class="normal">2871</span>
<span class="normal">2872</span>
<span class="normal">2873</span>
<span class="normal">2874</span>
<span class="normal">2875</span>
<span class="normal">2876</span>
<span class="normal">2877</span>
<span class="normal">2878</span>
<span class="normal">2879</span>
<span class="normal">2880</span>
<span class="normal">2881</span>
<span class="normal">2882</span>
<span class="normal">2883</span>
<span class="normal">2884</span>
<span class="normal">2885</span>
<span class="normal">2886</span>
<span class="normal">2887</span>
<span class="normal">2888</span>
<span class="normal">2889</span>
<span class="normal">2890</span>
<span class="normal">2891</span>
<span class="normal">2892</span>
<span class="normal">2893</span>
<span class="normal">2894</span>
<span class="normal">2895</span>
<span class="normal">2896</span>
<span class="normal">2897</span>
<span class="normal">2898</span>
<span class="normal">2899</span>
<span class="normal">2900</span>
<span class="normal">2901</span>
<span class="normal">2902</span>
<span class="normal">2903</span>
<span class="normal">2904</span>
<span class="normal">2905</span>
<span class="normal">2906</span>
<span class="normal">2907</span>
<span class="normal">2908</span>
<span class="normal">2909</span>
<span class="normal">2910</span>
<span class="normal">2911</span>
<span class="normal">2912</span>
<span class="normal">2913</span>
<span class="normal">2914</span>
<span class="normal">2915</span>
<span class="normal">2916</span>
<span class="normal">2917</span>
<span class="normal">2918</span>
<span class="normal">2919</span>
<span class="normal">2920</span>
<span class="normal">2921</span>
<span class="normal">2922</span>
<span class="normal">2923</span>
<span class="normal">2924</span>
<span class="normal">2925</span>
<span class="normal">2926</span>
<span class="normal">2927</span>
<span class="normal">2928</span>
<span class="normal">2929</span>
<span class="normal">2930</span>
<span class="normal">2931</span>
<span class="normal">2932</span>
<span class="normal">2933</span>
<span class="normal">2934</span>
<span class="normal">2935</span>
<span class="normal">2936</span>
<span class="normal">2937</span>
<span class="normal">2938</span>
<span class="normal">2939</span>
<span class="normal">2940</span>
<span class="normal">2941</span>
<span class="normal">2942</span>
<span class="normal">2943</span>
<span class="normal">2944</span>
<span class="normal">2945</span>
<span class="normal">2946</span>
<span class="normal">2947</span>
<span class="normal">2948</span>
<span class="normal">2949</span>
<span class="normal">2950</span>
<span class="normal">2951</span>
<span class="normal">2952</span>
<span class="normal">2953</span>
<span class="normal">2954</span>
<span class="normal">2955</span>
<span class="normal">2956</span>
<span class="normal">2957</span>
<span class="normal">2958</span>
<span class="normal">2959</span>
<span class="normal">2960</span>
<span class="normal">2961</span>
<span class="normal">2962</span>
<span class="normal">2963</span>
<span class="normal">2964</span>
<span class="normal">2965</span>
<span class="normal">2966</span>
<span class="normal">2967</span>
<span class="normal">2968</span>
<span class="normal">2969</span>
<span class="normal">2970</span>
<span class="normal">2971</span>
<span class="normal">2972</span>
<span class="normal">2973</span>
<span class="normal">2974</span>
<span class="normal">2975</span>
<span class="normal">2976</span>
<span class="normal">2977</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeForCausalLM</span><span class="p">(</span><span class="n">CpmBeePreTrainedModel</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents a CPMBee model for Causal Language Modeling tasks. It inherits from CpmBeePreTrainedModel and</span>
<span class="sd">    implements methods for model initialization, inference, beam search generation, input embeddings handling, and more.</span>

<span class="sd">    The class includes methods for initializing the model, forwarding the model for inference, performing inference,</span>
<span class="sd">    getting and setting input embeddings, getting and setting output embeddings, preparing inputs for generation,</span>
<span class="sd">    updating model kwargs for generation, reordering cache during generation, expanding inputs for generation,</span>
<span class="sd">    adjusting logits during generation, performing beam search for generation, and generating outputs based on</span>
<span class="sd">    input data using beam search.</span>

<span class="sd">    The `generate` method processes input data using the model to generate responses, filling placeholders in the</span>
<span class="sd">    input data with generated text. It accepts a dictionary or a list of dictionaries as input and</span>
<span class="sd">    returns a dictionary or a list of dictionaries with the &#39;&lt;ans&gt;&#39; field filled with generated text.</span>

<span class="sd">    For more details on the methods and their parameters, please refer to the method docstrings within the class</span>
<span class="sd">    implementation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_tied_weights_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;lm_head.weight&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a new instance of the CpmBeeForCausalLM class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The object instance.</span>
<span class="sd">            config (CpmBeeConfig): The configuration object for the CpmBee model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span> <span class="o">=</span> <span class="n">CpmBeeModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># lm_head.weight is tied to cpmbee.input_embedding.weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_id_sub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_segments</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">segment</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">segment_rel_offset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">segment_rel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">span</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ext_table_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># (ext_table_size) int32</span>
        <span class="n">ext_table_sub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># (ext_table_size) int32</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">CausalLMOutputWithPast</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">                Indices can be obtained using [`CPMBeeTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class="sd">                [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">                [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">            input_id_sub (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                Subscription of input sequence tokens in the vocabulary.</span>

<span class="sd">                Subscription of normal text will be zero while the special tokens of each group will be the 0, 1, 2,</span>
<span class="sd">                ... &lt;ans_0&gt;, &lt;ans_1&gt;, &lt;ans_2&gt; ... belongs to group &lt;ans&gt;. &lt;mask_0&gt;, &lt;mask_1&gt;, &lt;mask_2&gt; ... belongs to</span>
<span class="sd">                group &lt;mask&gt;.</span>
<span class="sd">            length (`mindspore.Tensor` of shape `(batch_size)`):</span>
<span class="sd">                The length of sequences in batch.</span>
<span class="sd">            context (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                Whether this token id is context or not. If is context, the value is 1. If not, the value is 0. If a</span>
<span class="sd">                token id is context, it does not need to be predicted.</span>
<span class="sd">            sample_ids (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                Give a sample id to every token id. The token ids with same sample ids belongs to the same sample.</span>
<span class="sd">            num_segments (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                Total number of segments in the current input.</span>
<span class="sd">            segment (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                Give a segment id to every token id. The token ids with same segment ids belongs to the same sample.</span>

<span class="sd">                Generally, a string key or value in input data will be a segment. For example, input {&quot;input&quot;: &quot;hello,</span>
<span class="sd">                &quot;, &quot;&lt;ans&gt;&quot;: &quot;&quot;}, the segments includes: &quot;input&quot;, &quot;hello, &quot;, &quot;&lt;ans&gt;&quot; and &quot;&quot;.</span>
<span class="sd">            segment_rel_offset (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                The offset of segment rel.</span>
<span class="sd">            segment_rel (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                The segment relevance. A relative implementation of measuring the importance of segments.</span>
<span class="sd">            span (`Dict[str, Union[mindspore.Tensor, List]]`):</span>
<span class="sd">                Span will record every input_ids shape.</span>
<span class="sd">            output_attentions (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers.</span>
<span class="sd">            output_hidden_states (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the hidden states of all layers.</span>
<span class="sd">            past_key_values (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>
<span class="sd">                A dummy arguments for CPMBee. The `past_states` contains pre-computed hidden-states (key and values in</span>
<span class="sd">                the self-attention blocks and in the cross-attention blocks) that can be used (see `past_key_values`</span>
<span class="sd">                input) and other history arguments to speed up sequential decoding.</span>
<span class="sd">            use_cache (`bool`, *optional*):</span>
<span class="sd">                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding</span>
<span class="sd">                (see `past_key_values`).</span>
<span class="sd">            labels (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">                Labels for computing the masked language modeling loss.</span>
<span class="sd">            return_dict (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class="sd">            ext_table_ids (`mindspore.Tensor`, *optional*):</span>
<span class="sd">                ext_table ids for embedding projection.</span>
<span class="sd">            ext_table_sub (`mindspore.Tensor`, *optional*):</span>
<span class="sd">                ext_table subscriptions for embedding projection.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="n">model_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">input_id_sub</span><span class="p">,</span>
            <span class="n">length</span><span class="p">,</span>
            <span class="n">context</span><span class="p">,</span>
            <span class="n">sample_ids</span><span class="p">,</span>
            <span class="n">num_segments</span><span class="p">,</span>
            <span class="n">segment</span><span class="p">,</span>
            <span class="n">segment_rel_offset</span><span class="p">,</span>
            <span class="n">segment_rel</span><span class="p">,</span>
            <span class="n">span</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model_output</span><span class="o">.</span><span class="n">last_hidden_state</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="k">else</span> <span class="n">model_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">ext_table_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ext_table</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">input_embedding</span><span class="p">(</span><span class="n">ext_table_ids</span><span class="p">,</span> <span class="n">ext_table_sub</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ext_table</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">input_embedding</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">ext_table</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">labels</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">model_output</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">CausalLMOutputWithPast</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_output</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">model_output</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">model_output</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">inference</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_id_sub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_segments</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">segment</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">segment_rel_offset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">segment_rel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ext_table_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># (ext_table_size) int32</span>
        <span class="n">ext_table_sub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># (ext_table_size) int32</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">CausalLMOutputWithPast</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            input_ids (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">                Indices can be obtained using [`CPMBeeTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class="sd">                [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">                [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">            input_id_sub (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                Subscription of input sequence tokens in the vocabulary.</span>

<span class="sd">                Subscription of normal text will be zero while the special tokens of each group will be the 0, 1, 2,</span>
<span class="sd">                ... &lt;ans_0&gt;, &lt;ans_1&gt;, &lt;ans_2&gt; ... belongs to group &lt;ans&gt;. &lt;mask_0&gt;, &lt;mask_1&gt;, &lt;mask_2&gt; ... belongs to</span>
<span class="sd">                group &lt;mask&gt;.</span>
<span class="sd">            position (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                The position of input sequence tokens in the vocabulary for each segment. if segment1 is 0, 1, 2 and</span>
<span class="sd">                segment2 is 0, 1, 2, 3, the position will be 0, 1, 2, 0, 1, 2, 3</span>
<span class="sd">            context (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                Whether this token id is context or not. If is context, the value is 1. If not, the value is 0. If a</span>
<span class="sd">                token id is context, it does not need to be predicted.</span>
<span class="sd">            sample_ids (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                Give a sample id to every token id. The token ids with same sample ids belongs to the same sample.</span>
<span class="sd">            num_segments (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                Total number of segments in the current input.</span>
<span class="sd">            segment (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                Give a segment id to every token id. The token ids with same segment ids belongs to the same sample.</span>

<span class="sd">                Generally, a string key or value in input data will be a segment. For example, input {&quot;input&quot;: &quot;hello,</span>
<span class="sd">                &quot;, &quot;&lt;ans&gt;&quot;: &quot;&quot;}, the segments includes: &quot;input&quot;, &quot;hello, &quot;, &quot;&lt;ans&gt;&quot; and &quot;&quot;.</span>
<span class="sd">            segment_rel_offset (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                The offset of segment rel.</span>
<span class="sd">            segment_rel (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">                The segment relevance. A relative implementation of measuring the importance of segments.</span>
<span class="sd">            past_states (`Dict[str, Union[mindspore.Tensor, List]]`):</span>
<span class="sd">                Store the history information including position, context, sample_ids, num_segments, segment and</span>
<span class="sd">                past_key_values.</span>
<span class="sd">            output_attentions (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers.</span>
<span class="sd">            output_hidden_states (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the hidden states of all layers.</span>
<span class="sd">            past_key_values (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>
<span class="sd">                A dummy arguments for CPMBee. The `past_states` contains pre-computed hidden-states (key and values in</span>
<span class="sd">                the self-attention blocks and in the cross-attention blocks) that can be used (see `past_key_values`</span>
<span class="sd">                input) and other history arguments to speed up sequential decoding.</span>
<span class="sd">            use_cache (`bool`, *optional*):</span>
<span class="sd">                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding</span>
<span class="sd">                (see `past_key_values`).</span>
<span class="sd">            labels (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">                Labels for computing the masked language modeling loss.</span>
<span class="sd">            return_dict (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class="sd">            ext_table_ids (`mindspore.Tensor`, *optional*):</span>
<span class="sd">                ext_table ids for embedding projection.</span>
<span class="sd">            ext_table_sub (`mindspore.Tensor`, *optional*):</span>
<span class="sd">                ext_table subscriptions for embedding projection.</span>

<span class="sd">        Example:</span>
<span class="sd">            Text Generation with CpmBeeForCausalLM.</span>
<span class="sd">            ```python</span>
<span class="sd">            &gt;&gt;&gt; from transformers import CpmBeeTokenizer, CpmBeeForCausalLM</span>
<span class="sd">            ...</span>
<span class="sd">            &gt;&gt;&gt; texts = {&quot;input&quot;: &quot;今天天气不错，&quot;, &quot;&lt;ans&gt;&quot;: &quot;&quot;}</span>
<span class="sd">            &gt;&gt;&gt; model = CpmBeeForCausalLM.from_pretrained(&quot;openbmb/cpm-bee-10b&quot;)</span>
<span class="sd">            &gt;&gt;&gt; tokenizer = CPMBeeTokenizer.from_pretrained(&quot;openbmb/cpm-bee-10b&quot;)</span>
<span class="sd">            &gt;&gt;&gt; output_texts = model.generate({&quot;input&quot;: &quot;今天天气不错，&quot;, &quot;&lt;ans&gt;&quot;: &quot;&quot;}, tokenizer)</span>
<span class="sd">            &gt;&gt;&gt; print(output_texts)</span>
<span class="sd">            {&#39;input&#39;: &#39;今天天气不错，&#39;, &#39;&lt;ans&gt;&#39;: &#39;适合睡觉。&#39;}</span>
<span class="sd">            ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="n">model_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">input_id_sub</span><span class="p">,</span>
            <span class="n">position</span><span class="p">,</span>
            <span class="n">context</span><span class="p">,</span>
            <span class="n">sample_ids</span><span class="p">,</span>
            <span class="n">num_segments</span><span class="p">,</span>
            <span class="n">segment</span><span class="p">,</span>
            <span class="n">segment_rel_offset</span><span class="p">,</span>
            <span class="n">segment_rel</span><span class="p">,</span>
            <span class="n">past_states</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model_output</span><span class="o">.</span><span class="n">last_hidden_state</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="k">else</span> <span class="n">model_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">ext_table_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="mi">0</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ext_table_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="n">ext_table</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">input_embedding</span><span class="p">(</span><span class="n">ext_table_ids</span><span class="p">,</span> <span class="n">ext_table_sub</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ext_table</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">input_embedding</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">ext_table</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">model_output</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">CausalLMOutputWithPast</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_output</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">model_output</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">model_output</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method retrieves the input embeddings from the CpmBeeForCausalLM object.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeForCausalLM): The instance of the CpmBeeForCausalLM class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            input_embedding: This method returns the input embeddings, which are of type None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">input_embedding</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the input embeddings for the CpmBeeForCausalLM class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeForCausalLM): The instance of the CpmBeeForCausalLM class.</span>
<span class="sd">            embeddings: The input embeddings to be set for the CpmBeeForCausalLM instance.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">embeddings</span>

    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the output embeddings for the CpmBeeForCausalLM model.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: An instance of the CpmBeeForCausalLM class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span>

    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the output embeddings for the CpmBeeForCausalLM model.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeForCausalLM): The instance of the CpmBeeForCausalLM class.</span>
<span class="sd">            new_embeddings: The new embeddings to be set as the output embeddings.</span>
<span class="sd">                This should be a tensor or an object that can be converted to a tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>

<span class="sd">        This method sets the output embeddings of the CpmBeeForCausalLM model to the provided new embeddings.</span>
<span class="sd">        The new embeddings are assigned to the &#39;lm_head&#39; attribute of the model object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">beam_scorer</span><span class="p">:</span> <span class="n">CpmBeeBeamSearchScorer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_id_subs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_pos</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">segment_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_ext_table_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_ext_table_sub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">other_info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Choose the current input according to beam states.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># init preparation</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;context&quot;</span><span class="p">)</span>
        <span class="n">sample_ids</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;sample_ids&quot;</span><span class="p">)</span>
        <span class="n">segment_rel_offset</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;segment_rel_offset&quot;</span><span class="p">)</span>
        <span class="n">num_segments</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;num_segments&quot;</span><span class="p">)</span>
        <span class="n">segment_rel</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;segment_rel&quot;</span><span class="p">)</span>
        <span class="n">past_states</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_states&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">_input_ids</span> <span class="o">=</span> <span class="n">input_ids</span>

        <span class="c1"># update input in generation</span>
        <span class="k">if</span> <span class="n">beam_scorer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tmp_input</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">tmp_input_sub</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">tmp_position</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">tmp_segment</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">sent_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">beam_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span><span class="p">):</span>
                    <span class="n">tmp_input</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">beam_states</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="n">beam_id</span><span class="p">][</span><span class="s2">&quot;nx_token_id&quot;</span><span class="p">])</span>
                    <span class="n">tmp_input_sub</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">beam_states</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="n">beam_id</span><span class="p">][</span><span class="s2">&quot;nx_token_sub&quot;</span><span class="p">])</span>
                    <span class="n">tmp_position</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">beam_states</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="n">beam_id</span><span class="p">][</span><span class="s2">&quot;nx_position&quot;</span><span class="p">])</span>
                    <span class="n">tmp_segment</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">beam_states</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="n">beam_id</span><span class="p">][</span><span class="s2">&quot;nx_segment_id&quot;</span><span class="p">])</span>

            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;input_id_subs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_id_subs</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="n">tmp_input_sub</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span>
            <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;input_pos&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_pos</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="n">tmp_position</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span>
            <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;segment_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">segment_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="n">tmp_segment</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span>
            <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">input_ids</span><span class="p">,</span>
                    <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tmp_input</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                        <span class="n">batch_size</span> <span class="o">*</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span>
                    <span class="p">),</span>
                <span class="p">],</span>
                <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">_input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">_input_ids</span><span class="p">,</span>
            <span class="s2">&quot;input_id_sub&quot;</span><span class="p">:</span> <span class="n">input_id_subs</span><span class="p">,</span>
            <span class="s2">&quot;position&quot;</span><span class="p">:</span> <span class="n">input_pos</span><span class="p">,</span>
            <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">context</span><span class="p">,</span>
            <span class="s2">&quot;sample_ids&quot;</span><span class="p">:</span> <span class="n">sample_ids</span><span class="p">,</span>
            <span class="s2">&quot;segment_rel_offset&quot;</span><span class="p">:</span> <span class="n">segment_rel_offset</span><span class="p">,</span>
            <span class="s2">&quot;segment&quot;</span><span class="p">:</span> <span class="n">segment_ids</span><span class="p">,</span>
            <span class="s2">&quot;num_segments&quot;</span><span class="p">:</span> <span class="n">num_segments</span><span class="p">,</span>
            <span class="s2">&quot;segment_rel&quot;</span><span class="p">:</span> <span class="n">segment_rel</span><span class="p">,</span>
            <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
            <span class="s2">&quot;ext_table_ids&quot;</span><span class="p">:</span> <span class="n">batch_ext_table_ids</span><span class="p">,</span>
            <span class="s2">&quot;ext_table_sub&quot;</span><span class="p">:</span> <span class="n">batch_ext_table_sub</span><span class="p">,</span>
            <span class="s2">&quot;past_states&quot;</span><span class="p">:</span> <span class="n">past_states</span><span class="p">,</span>
        <span class="p">},</span> <span class="n">input_ids</span>

    <span class="k">def</span> <span class="nf">_update_model_kwargs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">ModelOutput</span><span class="p">,</span>
        <span class="n">model_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Concatenate the history input and current input.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">old_past_states</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_states&quot;</span><span class="p">]</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;buffer_position&quot;</span><span class="p">:</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">old_past_states</span><span class="p">[</span><span class="s2">&quot;buffer_position&quot;</span><span class="p">],</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="s2">&quot;buffer_context&quot;</span><span class="p">:</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">old_past_states</span><span class="p">[</span><span class="s2">&quot;buffer_context&quot;</span><span class="p">],</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="s2">&quot;buffer_sample_ids&quot;</span><span class="p">:</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">old_past_states</span><span class="p">[</span><span class="s2">&quot;buffer_sample_ids&quot;</span><span class="p">],</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;sample_ids&quot;</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="s2">&quot;buffer_num_segments&quot;</span><span class="p">:</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span><span class="n">old_past_states</span><span class="p">[</span><span class="s2">&quot;buffer_num_segments&quot;</span><span class="p">],</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;num_segments&quot;</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
            <span class="p">),</span>
            <span class="s2">&quot;buffer_segments&quot;</span><span class="p">:</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">old_past_states</span><span class="p">[</span><span class="s2">&quot;buffer_segments&quot;</span><span class="p">],</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;segment&quot;</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="s2">&quot;buffer&quot;</span><span class="p">:</span> <span class="n">outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">model_kwargs</span>

    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reorders the cache of past key values for beam search decoding in a CpmBeeForCausalLM object.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeForCausalLM): The instance of the CpmBeeForCausalLM class.</span>
<span class="sd">            past_key_values (Dict): The dictionary containing the cache of past key values.</span>
<span class="sd">                The cache is used during beam search decoding to store previous key-value pairs.</span>
<span class="sd">            beam_idx (mindspore.Tensor): The tensor containing the indices of the beams to be reordered.</span>
<span class="sd">                The indices represent the order in which the beams are to be arranged.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: The method modifies the past_key_values dictionary in-place.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>

<span class="sd">        Note:</span>
<span class="sd">            The method reorders the cache by rearranging the key-value pairs based on the given beam indices.</span>
<span class="sd">            If the cache contains a &#39;buffer&#39; key, the key-value pairs within the buffer are rearranged.</span>
<span class="sd">            If a key-value pair is (None, None), it remains unchanged.</span>
<span class="sd">            Otherwise, the key-value pair is split into separate key and value tensors, and only the tensors</span>
<span class="sd">            corresponding to the specified beam indices are kept in the cache.</span>

<span class="sd">        Example:</span>
<span class="sd">            ```python</span>
<span class="sd">            &gt;&gt;&gt; # Create an instance of the CpmBeeForCausalLM class</span>
<span class="sd">            &gt;&gt;&gt; cpm_bee = CpmBeeForCausalLM()</span>
<span class="sd">            ...</span>
<span class="sd">            &gt;&gt;&gt; # Define the past key values</span>
<span class="sd">            &gt;&gt;&gt; past_key_values = {</span>
<span class="sd">            &gt;&gt;&gt;     &#39;buffer&#39;: [((key1, value1), (key2, value2)), ((key3, value3), (key4, value4))],</span>
<span class="sd">            &gt;&gt;&gt;     &#39;other_key&#39;: tensor([[1, 2, 3], [4, 5, 6]])</span>
<span class="sd">            &gt;&gt;&gt; }</span>
<span class="sd">            ...</span>
<span class="sd">            &gt;&gt;&gt; # Define the beam indices</span>
<span class="sd">            &gt;&gt;&gt; beam_idx = tensor([1, 0])</span>
<span class="sd">            ...</span>
<span class="sd">            &gt;&gt;&gt; # Reorder the cache of past key values</span>
<span class="sd">            &gt;&gt;&gt; cpm_bee._reorder_cache(past_key_values, beam_idx)</span>
<span class="sd">            ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">beam_idx</span> <span class="o">=</span> <span class="n">beam_idx</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">kw</span> <span class="ow">in</span> <span class="n">past_key_values</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">kw</span> <span class="o">==</span> <span class="s2">&quot;buffer&quot;</span><span class="p">:</span>
                <span class="n">buf_list</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="n">kw</span><span class="p">]</span>
                <span class="n">nw_buf_list</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">buf_list</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">buf</span> <span class="o">==</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
                        <span class="n">nw_buf_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">k_buf</span><span class="p">,</span> <span class="n">v_buf</span> <span class="o">=</span> <span class="n">buf</span>
                        <span class="n">nw_buf_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">k_buf</span><span class="p">[</span><span class="n">beam_idx</span><span class="p">,</span> <span class="p">:],</span> <span class="n">v_buf</span><span class="p">[</span><span class="n">beam_idx</span><span class="p">,</span> <span class="p">:]))</span>
                <span class="n">past_key_values</span><span class="p">[</span><span class="n">kw</span><span class="p">]</span> <span class="o">=</span> <span class="n">nw_buf_list</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">past_key_values</span><span class="p">[</span><span class="n">kw</span><span class="p">]</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="n">kw</span><span class="p">][</span><span class="n">beam_idx</span><span class="p">,</span> <span class="p">:]</span>

        <span class="k">return</span> <span class="n">past_key_values</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_expand_inputs_for_generation</span><span class="p">(</span>
        <span class="n">expand_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">is_encoder_decoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Expands tensors from [batch_size, ...] to [batch_size * expand_size, ...]&quot;&quot;&quot;</span>
        <span class="c1"># do not expand ext_table_ids and ext_table_sub</span>
        <span class="k">def</span> <span class="nf">_expand_dict_for_generation</span><span class="p">(</span><span class="n">dict_to_expand</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">dict_to_expand</span><span class="p">:</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">dict_to_expand</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dict_to_expand</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="s2">&quot;ext_table&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">key</span>
                <span class="p">):</span>
                    <span class="n">dict_to_expand</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">dict_to_expand</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">expand_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">dict_to_expand</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">expand_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="n">_expand_dict_for_generation</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.&quot;</span><span class="p">)</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_expand_dict_for_generation</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span>

    <span class="k">def</span> <span class="nf">adjust_logits_during_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">logits</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">beam_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">ext_table_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Implement in subclasses of [`PreTrainedModel`] for custom behavior to adjust the logits in the generate method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">sent_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="k">if</span> <span class="mi">1</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;ext_table&quot;</span><span class="p">]:</span>
                <span class="c1"># unk is not allowed, mask unk</span>
                <span class="n">logits</span><span class="p">[</span><span class="n">sent_id</span> <span class="o">*</span> <span class="n">beam_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">sent_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">beam_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10000</span>
            <span class="n">ext_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;ext_table&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">ext_ids</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">ext_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">vocab_size</span> <span class="o">+</span> <span class="n">ext_table_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="k">if</span> <span class="n">ext_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ext_ids</span><span class="p">:</span>
                    <span class="n">logits</span><span class="p">[</span><span class="n">sent_id</span> <span class="o">*</span> <span class="n">beam_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">sent_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">beam_size</span><span class="p">,</span> <span class="n">ext_id</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10000</span>
        <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span> <span class="nf">beam_search</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">beam_scorer</span><span class="p">:</span> <span class="n">CpmBeeBeamSearchScorer</span><span class="p">,</span>
        <span class="n">repetition_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict_in_generate</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Override the beam_search for CPMBee.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># init values</span>
        <span class="n">logits_processor</span> <span class="o">=</span> <span class="n">logits_processor</span> <span class="k">if</span> <span class="n">logits_processor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>
        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">pad_token_id</span> <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span> <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span>
        <span class="n">bos_token_id</span> <span class="o">=</span> <span class="n">bos_token_id</span> <span class="k">if</span> <span class="n">bos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">bos_token_id</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="k">if</span> <span class="n">vocab_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">vocab_size</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span> <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_new_tokens</span>
        <span class="n">output_scores</span> <span class="o">=</span> <span class="n">output_scores</span> <span class="k">if</span> <span class="n">output_scores</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">output_scores</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="p">)</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">return_dict_in_generate</span>
            <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">return_dict_in_generate</span>
        <span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">_beam_hyps</span><span class="p">)</span>
        <span class="n">num_beams</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span>

        <span class="n">batch_beam_size</span><span class="p">,</span> <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">!=</span> <span class="n">batch_beam_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Batch dimension of `input_ids` should be </span><span class="si">{</span><span class="n">num_beams</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">batch_size</span><span class="si">}</span><span class="s2">, but is </span><span class="si">{</span><span class="n">batch_beam_size</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># init attention / hidden states / scores tuples</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">beam_indices</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">tuple</span><span class="p">(()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_beam_size</span><span class="p">))</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="n">decoder_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">cross_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_hidden_states</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># initialise score of first beam with 0 and the rest with -1e9. This makes sure that only tokens</span>
        <span class="c1"># of the first beam are considered to avoid sampling the exact same tokens across all beams.</span>
        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">beam_scores</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e9</span>
        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_scores</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,))</span>

        <span class="c1"># init inference</span>
        <span class="n">model_inputs</span><span class="p">,</span> <span class="n">input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>
        <span class="n">pred_start_index</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span>
            <span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># update model_kwargs</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;buffer_position&quot;</span><span class="p">:</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">],</span>
            <span class="s2">&quot;buffer_context&quot;</span><span class="p">:</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
            <span class="s2">&quot;buffer_sample_ids&quot;</span><span class="p">:</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;sample_ids&quot;</span><span class="p">],</span>
            <span class="s2">&quot;buffer_num_segments&quot;</span><span class="p">:</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;num_segments&quot;</span><span class="p">],</span>
            <span class="s2">&quot;buffer_segments&quot;</span><span class="p">:</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;segment&quot;</span><span class="p">],</span>
            <span class="s2">&quot;buffer&quot;</span><span class="p">:</span> <span class="n">outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_beam_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="n">batch_beam_size</span><span class="p">,</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;sample_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_beam_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="n">batch_beam_size</span><span class="p">,</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;num_segments&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;num_segments&quot;</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;segment_rel_offset&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;segment_rel_offset&quot;</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">past_key_values</span>

        <span class="n">ext_table_ids_cpu</span> <span class="o">=</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;ext_table_ids&quot;</span><span class="p">]</span>
        <span class="n">ext_table_sub_cpu</span> <span class="o">=</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;ext_table_sub&quot;</span><span class="p">]</span>

        <span class="n">cur_len</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">model_inputs</span><span class="p">,</span> <span class="n">input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">beam_scorer</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span>
            <span class="p">)</span>

            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span>
                <span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">_done</span><span class="p">):</span>
                <span class="k">break</span>
            <span class="c1"># hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`</span>
            <span class="c1"># cannot be generated both before and after the `ops.log_softmax` operation.</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjust_logits_during_generation</span><span class="p">(</span>
                <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">ext_table_ids_cpu</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span>
            <span class="p">)</span>

            <span class="c1"># repetition_penalty</span>
            <span class="n">beam_scorer</span><span class="o">.</span><span class="n">apply_repetition_penalty</span><span class="p">(</span>
                <span class="n">next_token_logits</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="p">,</span>
                <span class="n">num_beams</span><span class="p">,</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">repetition_penalty</span><span class="p">,</span>
                <span class="n">pred_start_index</span><span class="p">,</span>
                <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
                <span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">_next_token_scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span>
                <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
            <span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>

            <span class="n">next_token_scores_processed</span> <span class="o">=</span> <span class="n">logits_processor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">_next_token_scores</span><span class="p">)</span>
            <span class="c1"># next_token_scores_processed = _next_token_scores</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores_processed</span> <span class="o">+</span> <span class="n">beam_scores</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">_next_token_scores</span><span class="p">)</span>

            <span class="c1"># Store scores, attentions and hidden_states when required</span>
            <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="n">scores</span> <span class="o">+=</span> <span class="p">(</span><span class="n">next_token_scores_processed</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                    <span class="n">decoder_attentions</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,)</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                        <span class="n">cross_attentions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,)</span>

                <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                    <span class="n">decoder_hidden_states</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,)</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
                        <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,)</span>
                    <span class="p">)</span>

            <span class="c1"># reshape for beam search</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Sample 2 next tokens for each beam (so we have some spare tokens and match output of beam search)</span>
            <span class="n">next_token_scores</span><span class="p">,</span> <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span>
                <span class="n">next_token_scores</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">largest</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>

            <span class="n">beam_outputs</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">process</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span>
                <span class="n">cur_len</span><span class="p">,</span>
                <span class="n">_next_token_scores</span><span class="p">,</span>
                <span class="n">next_token_scores</span><span class="p">,</span>
                <span class="n">next_tokens</span><span class="p">,</span>
                <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                <span class="n">bos_token_id</span><span class="o">=</span><span class="n">bos_token_id</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">ext_table_ids_cpu</span><span class="o">=</span><span class="n">ext_table_ids_cpu</span><span class="p">,</span>
                <span class="n">ext_table_sub_cpu</span><span class="o">=</span><span class="n">ext_table_sub_cpu</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">beam_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">beam_idx</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_indices&quot;</span><span class="p">]</span>
            <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_scores&quot;</span><span class="p">]</span>

            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">beam_idx</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="p">:]</span>
            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">model_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_states&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reorder_cache</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_states&quot;</span><span class="p">],</span> <span class="n">beam_idx</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">:</span>
                <span class="n">beam_indices</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">((</span><span class="n">beam_indices</span><span class="p">[</span><span class="n">beam_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+</span> <span class="p">(</span><span class="n">beam_idx</span><span class="p">[</span><span class="n">i</span><span class="p">],)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">beam_indices</span><span class="p">))))</span>

            <span class="n">cur_len</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">is_done</span> <span class="ow">or</span> <span class="n">cur_len</span> <span class="o">==</span> <span class="n">max_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">synced_gpus</span><span class="p">:</span>
                    <span class="k">break</span>

        <span class="n">sequence_outputs</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">finalize</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">sequence_outputs</span>

    <span class="k">def</span> <span class="nf">_generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerationConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">repetition_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stopping_criteria</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">StoppingCriteriaList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prefix_allowed_tokens_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">synced_gpus</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">streamer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;BaseStreamer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The generation of CPMBee.</span>

<span class="sd">        1. It will use beam search as generation strategy.</span>
<span class="sd">        2. It will use CpmBeeBeamSearchScorer as the beamsearch scorer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_model_class</span><span class="p">()</span>

        <span class="c1"># priority: `generation_config` argument &gt; `model.generation_config` (the default generation config)</span>
        <span class="k">if</span> <span class="n">generation_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># legacy: users may modify the model configuration to control generation -- update the generation config</span>
            <span class="c1"># model attribute accordingly, if it was created from the model config</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">_from_model_config</span><span class="p">:</span>
                <span class="n">new_generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_model_config</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">new_generation_config</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                        <span class="s2">&quot;You have modified the pretrained model configuration to control generation. This is a&quot;</span>
                        <span class="s2">&quot; deprecated strategy to control generation.&quot;</span>
                        <span class="s2">&quot; Please use a generation configuration file (see&quot;</span>
                        <span class="s2">&quot; https://hf-mirror.com/docs/transformers/main_classes/text_generation)&quot;</span>
                    <span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span> <span class="o">=</span> <span class="n">new_generation_config</span>
            <span class="n">generation_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span>

        <span class="n">generation_config</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">generation_config</span><span class="p">)</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># All unused kwargs must be model kwargs</span>
        <span class="n">generation_config</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_model_kwargs</span><span class="p">(</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

        <span class="c1"># 2. Set generation parameters if not already defined</span>
        <span class="n">logits_processor</span> <span class="o">=</span> <span class="n">logits_processor</span> <span class="k">if</span> <span class="n">logits_processor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>
        <span class="n">stopping_criteria</span> <span class="o">=</span> <span class="n">stopping_criteria</span> <span class="k">if</span> <span class="n">stopping_criteria</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">StoppingCriteriaList</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;The attention mask and the pad token id were not set. As a consequence, you may observe &quot;</span>
                    <span class="s2">&quot;unexpected behavior. Please pass your input&#39;s `attention_mask` to obtain reliable results.&quot;</span>
                <span class="p">)</span>
            <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Setting `pad_token_id` to `eos_token_id`:</span><span class="si">{</span><span class="n">eos_token_id</span><span class="si">}</span><span class="s2"> for open-end generation.&quot;</span><span class="p">)</span>
            <span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span>

        <span class="c1"># 3. Define model inputs</span>
        <span class="c1"># inputs_tensor has to be defined</span>
        <span class="c1"># model_input_name is defined if model-specific keyword input is passed</span>
        <span class="c1"># otherwise model_input_name is None</span>
        <span class="c1"># all model-specific keyword inputs are removed from `model_kwargs`</span>
        <span class="n">inputs_tensor</span><span class="p">,</span> <span class="n">model_input_name</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_model_inputs</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">bos_token_id</span><span class="p">,</span> <span class="n">model_kwargs</span>
        <span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">inputs_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># 4. Define other model kwargs</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;output_attentions&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;use_cache&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">use_cache</span>

        <span class="n">accepts_attention_mask</span> <span class="o">=</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">requires_attention_mask</span> <span class="o">=</span> <span class="s2">&quot;encoder_outputs&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_kwargs</span>

        <span class="k">if</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">requires_attention_mask</span> <span class="ow">and</span> <span class="n">accepts_attention_mask</span><span class="p">:</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_attention_mask_for_generation</span><span class="p">(</span>
                <span class="n">inputs_tensor</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span>
            <span class="p">)</span>

        <span class="c1"># decoder-only models should use left-padding for generation</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="c1"># If `input_ids` was given, check if the last id in any sequence is `pad_token_id`</span>
            <span class="c1"># Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
                <span class="ow">and</span> <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">inputs_tensor</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;A decoder-only architecture is being used, but right-padding was detected! For correct &quot;</span>
                    <span class="s2">&quot;generation results, please set `padding_side=&#39;left&#39;` when initializing the tokenizer.&quot;</span>
                <span class="p">)</span>

        <span class="c1"># 5. Prepare `input_ids` which will be used for auto-regressive generation</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">inputs_tensor</span> <span class="k">if</span> <span class="n">model_input_name</span> <span class="o">==</span> <span class="s2">&quot;input_ids&quot;</span> <span class="k">else</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;input_ids&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">streamer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">streamer</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="c1"># 6. Prepare `max_length` depending on other stopping criteria.</span>
        <span class="n">input_ids_seq_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">has_default_max_length</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;max_length&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">has_default_max_length</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_new_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Using `max_length`&#39;s default (</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="si">}</span><span class="s2">) to control the generation length. &quot;</span>
                <span class="s2">&quot;This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we&quot;</span>
                <span class="s2">&quot; recommend using `max_new_tokens` to control the maximum length of the generation.&quot;</span><span class="p">,</span>
                <span class="ne">UserWarning</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_new_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">has_default_max_length</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Both `max_new_tokens` (=</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_new_tokens</span><span class="si">}</span><span class="s2">) and `max_length`(=&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="si">}</span><span class="s2">) seem to have been set. `max_new_tokens` will take precedence. &quot;</span>
                    <span class="s2">&quot;Please refer to the documentation for more information. &quot;</span>
                    <span class="s2">&quot;(https://hf-mirror.com/docs/transformers/main/en/main_classes/text_generation)&quot;</span>
                <span class="p">)</span>
            <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_new_tokens</span> <span class="o">+</span> <span class="n">input_ids_seq_length</span>

        <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">min_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">min_length</span> <span class="o">&gt;</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unfeasible length constraints: the minimum length (</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">min_length</span><span class="si">}</span><span class="s2">) is larger than&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; the maximum length (</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">input_ids_seq_length</span> <span class="o">&gt;=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">:</span>
            <span class="n">input_ids_string</span> <span class="o">=</span> <span class="s2">&quot;decoder_input_ids&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="s2">&quot;input_ids&quot;</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Input length of </span><span class="si">{</span><span class="n">input_ids_string</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">input_ids_seq_length</span><span class="si">}</span><span class="s2">, but `max_length` is set to&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="si">}</span><span class="s2">. This can lead to unexpected behavior. You should consider&quot;</span>
                <span class="s2">&quot; increasing `max_new_tokens`.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">streamer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># 7. prepare distribution pre_processing samplers</span>
        <span class="n">logits_processor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_logits_processor</span><span class="p">(</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">input_ids_seq_length</span><span class="o">=</span><span class="n">input_ids_seq_length</span><span class="p">,</span>
            <span class="n">encoder_input_ids</span><span class="o">=</span><span class="n">inputs_tensor</span><span class="p">,</span>
            <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="n">prefix_allowed_tokens_fn</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># 8. prepare beam search scorer</span>
        <span class="n">beam_scorer</span> <span class="o">=</span> <span class="n">CpmBeeBeamSearchScorer</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">num_beams</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
            <span class="n">length_penalty</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">length_penalty</span><span class="p">,</span>
            <span class="n">do_early_stopping</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">,</span>
            <span class="n">num_beam_hyps_to_keep</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_return_sequences</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_new_tokens</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># 9. interleave input_ids with `num_beams` additional sequences per batch</span>
        <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_inputs_for_generation</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">expand_size</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span>
            <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
            <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># 10. run beam search</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">beam_search</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">beam_scorer</span><span class="p">,</span>
            <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">repetition_penalty</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_new_tokens</span><span class="p">,</span>
            <span class="n">pad_token_id</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span>
            <span class="n">eos_token_id</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
            <span class="n">vocab_size</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;vocab_size&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="n">output_scores</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">output_scores</span><span class="p">,</span>
            <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="n">generation_config</span><span class="o">.</span><span class="n">return_dict_in_generate</span><span class="p">,</span>
            <span class="n">synced_gpus</span><span class="o">=</span><span class="n">synced_gpus</span><span class="p">,</span>
            <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data_list</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]],</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">CpmBeeTokenizer</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Override the generate for CPMBee. It will accept dict or list(dict) as input and returns dict or list(dict)</span>
<span class="sd">        with `&lt;ans&gt;` filled.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            data_list (`dict` or `list(dict)`):</span>
<span class="sd">                The sequence used as a prompt for the generation or as model inputs to the encoder. If dict, data_list</span>
<span class="sd">                will be wrapped as a list.</span>
<span class="sd">            tokenizer: (`CpmBeeTokenizer`):</span>
<span class="sd">                The tokenizer.</span>
<span class="sd">            generation_config (`~generation.GenerationConfig`, *optional*):</span>
<span class="sd">                The generation configuration to be used as base parametrization for the generation call. `**kwargs`</span>
<span class="sd">                passed to generate matching the attributes of `generation_config` will override them. If</span>
<span class="sd">                `generation_config` is not provided, the default will be used, which had the following loading</span>
<span class="sd">                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model</span>
<span class="sd">                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]&#39;s</span>
<span class="sd">                default values, whose documentation should be checked to parameterize generation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">data_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">data_list</span><span class="p">]</span>
        <span class="n">input_encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">input_encoded</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">input_encoded</span><span class="p">[</span><span class="s2">&quot;vocab_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span>

        <span class="n">decode_res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_encoded</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">sent_id</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">decode_res</span><span class="p">):</span>
            <span class="n">ans_result_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">raw_word_id</span><span class="p">,</span> <span class="n">ans_id</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">ans_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ans_result_map</span><span class="p">:</span>
                    <span class="n">ans_result_map</span><span class="p">[</span><span class="n">ans_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">ans_result_map</span><span class="p">[</span><span class="n">ans_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">raw_word_id</span><span class="p">)</span>

            <span class="n">answer_placeholders</span> <span class="o">=</span> <span class="n">input_encoded</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;answer_placeholders&quot;</span><span class="p">]</span>
            <span class="n">ext_table</span> <span class="o">=</span> <span class="n">input_encoded</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;ext_table&quot;</span><span class="p">]</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data_list</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">ans_id</span><span class="p">,</span> <span class="n">token_ids</span> <span class="ow">in</span> <span class="n">ans_result_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">token_ids</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">:</span>
                    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">token_ids</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">ext_table</span><span class="p">)</span>
                <span class="n">path</span> <span class="o">=</span> <span class="n">answer_placeholders</span><span class="p">[</span><span class="n">ans_id</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">path</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">p</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;&lt;ans&gt;&quot;</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="n">path</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">part</span><span class="p">]</span>
                    <span class="n">p</span><span class="p">[</span><span class="n">path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">text</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">data</span><span class="p">[</span><span class="s2">&quot;&lt;ans&gt;&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text</span>
            <span class="k">for</span> <span class="n">ans_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">answer_placeholders</span><span class="p">)):</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">ans_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ans_result_map</span><span class="p">:</span>
                    <span class="n">path</span> <span class="o">=</span> <span class="n">answer_placeholders</span><span class="p">[</span><span class="n">ans_id</span><span class="p">]</span>
                    <span class="n">p</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;&lt;ans&gt;&quot;</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="n">path</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">part</span><span class="p">]</span>
                    <span class="n">p</span><span class="p">[</span><span class="n">path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">data_list</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeForCausalLM</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a new instance of the CpmBeeForCausalLM class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The object instance.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object for the CpmBee model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig">CpmBeeConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a new instance of the CpmBeeForCausalLM class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The object instance.</span>
<span class="sd">        config (CpmBeeConfig): The configuration object for the CpmBee model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span> <span class="o">=</span> <span class="n">CpmBeeModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="c1"># lm_head.weight is tied to cpmbee.input_embedding.weight</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.adjust_logits_during_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeForCausalLM</span><span class="o">.</span><span class="n">adjust_logits_during_generation</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">beam_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">ext_table_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.adjust_logits_during_generation" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Implement in subclasses of [<code>PreTrainedModel</code>] for custom behavior to adjust the logits in the generate method.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2505</span>
<span class="normal">2506</span>
<span class="normal">2507</span>
<span class="normal">2508</span>
<span class="normal">2509</span>
<span class="normal">2510</span>
<span class="normal">2511</span>
<span class="normal">2512</span>
<span class="normal">2513</span>
<span class="normal">2514</span>
<span class="normal">2515</span>
<span class="normal">2516</span>
<span class="normal">2517</span>
<span class="normal">2518</span>
<span class="normal">2519</span>
<span class="normal">2520</span>
<span class="normal">2521</span>
<span class="normal">2522</span>
<span class="normal">2523</span>
<span class="normal">2524</span>
<span class="normal">2525</span>
<span class="normal">2526</span>
<span class="normal">2527</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">adjust_logits_during_generation</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">logits</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">beam_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">ext_table_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement in subclasses of [`PreTrainedModel`] for custom behavior to adjust the logits in the generate method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">sent_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="k">if</span> <span class="mi">1</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;ext_table&quot;</span><span class="p">]:</span>
            <span class="c1"># unk is not allowed, mask unk</span>
            <span class="n">logits</span><span class="p">[</span><span class="n">sent_id</span> <span class="o">*</span> <span class="n">beam_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">sent_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">beam_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10000</span>
        <span class="n">ext_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;ext_table&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">ext_ids</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">ext_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">vocab_size</span> <span class="o">+</span> <span class="n">ext_table_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">if</span> <span class="n">ext_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ext_ids</span><span class="p">:</span>
                <span class="n">logits</span><span class="p">[</span><span class="n">sent_id</span> <span class="o">*</span> <span class="n">beam_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">sent_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">beam_size</span><span class="p">,</span> <span class="n">ext_id</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10000</span>
    <span class="k">return</span> <span class="n">logits</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.beam_search" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeForCausalLM</span><span class="o">.</span><span class="n">beam_search</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">beam_scorer</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bos_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_scores</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">synced_gpus</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.beam_search" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Override the beam_search for CPMBee.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2529</span>
<span class="normal">2530</span>
<span class="normal">2531</span>
<span class="normal">2532</span>
<span class="normal">2533</span>
<span class="normal">2534</span>
<span class="normal">2535</span>
<span class="normal">2536</span>
<span class="normal">2537</span>
<span class="normal">2538</span>
<span class="normal">2539</span>
<span class="normal">2540</span>
<span class="normal">2541</span>
<span class="normal">2542</span>
<span class="normal">2543</span>
<span class="normal">2544</span>
<span class="normal">2545</span>
<span class="normal">2546</span>
<span class="normal">2547</span>
<span class="normal">2548</span>
<span class="normal">2549</span>
<span class="normal">2550</span>
<span class="normal">2551</span>
<span class="normal">2552</span>
<span class="normal">2553</span>
<span class="normal">2554</span>
<span class="normal">2555</span>
<span class="normal">2556</span>
<span class="normal">2557</span>
<span class="normal">2558</span>
<span class="normal">2559</span>
<span class="normal">2560</span>
<span class="normal">2561</span>
<span class="normal">2562</span>
<span class="normal">2563</span>
<span class="normal">2564</span>
<span class="normal">2565</span>
<span class="normal">2566</span>
<span class="normal">2567</span>
<span class="normal">2568</span>
<span class="normal">2569</span>
<span class="normal">2570</span>
<span class="normal">2571</span>
<span class="normal">2572</span>
<span class="normal">2573</span>
<span class="normal">2574</span>
<span class="normal">2575</span>
<span class="normal">2576</span>
<span class="normal">2577</span>
<span class="normal">2578</span>
<span class="normal">2579</span>
<span class="normal">2580</span>
<span class="normal">2581</span>
<span class="normal">2582</span>
<span class="normal">2583</span>
<span class="normal">2584</span>
<span class="normal">2585</span>
<span class="normal">2586</span>
<span class="normal">2587</span>
<span class="normal">2588</span>
<span class="normal">2589</span>
<span class="normal">2590</span>
<span class="normal">2591</span>
<span class="normal">2592</span>
<span class="normal">2593</span>
<span class="normal">2594</span>
<span class="normal">2595</span>
<span class="normal">2596</span>
<span class="normal">2597</span>
<span class="normal">2598</span>
<span class="normal">2599</span>
<span class="normal">2600</span>
<span class="normal">2601</span>
<span class="normal">2602</span>
<span class="normal">2603</span>
<span class="normal">2604</span>
<span class="normal">2605</span>
<span class="normal">2606</span>
<span class="normal">2607</span>
<span class="normal">2608</span>
<span class="normal">2609</span>
<span class="normal">2610</span>
<span class="normal">2611</span>
<span class="normal">2612</span>
<span class="normal">2613</span>
<span class="normal">2614</span>
<span class="normal">2615</span>
<span class="normal">2616</span>
<span class="normal">2617</span>
<span class="normal">2618</span>
<span class="normal">2619</span>
<span class="normal">2620</span>
<span class="normal">2621</span>
<span class="normal">2622</span>
<span class="normal">2623</span>
<span class="normal">2624</span>
<span class="normal">2625</span>
<span class="normal">2626</span>
<span class="normal">2627</span>
<span class="normal">2628</span>
<span class="normal">2629</span>
<span class="normal">2630</span>
<span class="normal">2631</span>
<span class="normal">2632</span>
<span class="normal">2633</span>
<span class="normal">2634</span>
<span class="normal">2635</span>
<span class="normal">2636</span>
<span class="normal">2637</span>
<span class="normal">2638</span>
<span class="normal">2639</span>
<span class="normal">2640</span>
<span class="normal">2641</span>
<span class="normal">2642</span>
<span class="normal">2643</span>
<span class="normal">2644</span>
<span class="normal">2645</span>
<span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span>
<span class="normal">2652</span>
<span class="normal">2653</span>
<span class="normal">2654</span>
<span class="normal">2655</span>
<span class="normal">2656</span>
<span class="normal">2657</span>
<span class="normal">2658</span>
<span class="normal">2659</span>
<span class="normal">2660</span>
<span class="normal">2661</span>
<span class="normal">2662</span>
<span class="normal">2663</span>
<span class="normal">2664</span>
<span class="normal">2665</span>
<span class="normal">2666</span>
<span class="normal">2667</span>
<span class="normal">2668</span>
<span class="normal">2669</span>
<span class="normal">2670</span>
<span class="normal">2671</span>
<span class="normal">2672</span>
<span class="normal">2673</span>
<span class="normal">2674</span>
<span class="normal">2675</span>
<span class="normal">2676</span>
<span class="normal">2677</span>
<span class="normal">2678</span>
<span class="normal">2679</span>
<span class="normal">2680</span>
<span class="normal">2681</span>
<span class="normal">2682</span>
<span class="normal">2683</span>
<span class="normal">2684</span>
<span class="normal">2685</span>
<span class="normal">2686</span>
<span class="normal">2687</span>
<span class="normal">2688</span>
<span class="normal">2689</span>
<span class="normal">2690</span>
<span class="normal">2691</span>
<span class="normal">2692</span>
<span class="normal">2693</span>
<span class="normal">2694</span>
<span class="normal">2695</span>
<span class="normal">2696</span>
<span class="normal">2697</span>
<span class="normal">2698</span>
<span class="normal">2699</span>
<span class="normal">2700</span>
<span class="normal">2701</span>
<span class="normal">2702</span>
<span class="normal">2703</span>
<span class="normal">2704</span>
<span class="normal">2705</span>
<span class="normal">2706</span>
<span class="normal">2707</span>
<span class="normal">2708</span>
<span class="normal">2709</span>
<span class="normal">2710</span>
<span class="normal">2711</span>
<span class="normal">2712</span>
<span class="normal">2713</span>
<span class="normal">2714</span>
<span class="normal">2715</span>
<span class="normal">2716</span>
<span class="normal">2717</span>
<span class="normal">2718</span>
<span class="normal">2719</span>
<span class="normal">2720</span>
<span class="normal">2721</span>
<span class="normal">2722</span>
<span class="normal">2723</span>
<span class="normal">2724</span>
<span class="normal">2725</span>
<span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">beam_search</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">beam_scorer</span><span class="p">:</span> <span class="n">CpmBeeBeamSearchScorer</span><span class="p">,</span>
    <span class="n">repetition_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict_in_generate</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">synced_gpus</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Override the beam_search for CPMBee.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># init values</span>
    <span class="n">logits_processor</span> <span class="o">=</span> <span class="n">logits_processor</span> <span class="k">if</span> <span class="n">logits_processor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>
    <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">pad_token_id</span> <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">pad_token_id</span>
    <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span> <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span>
    <span class="n">bos_token_id</span> <span class="o">=</span> <span class="n">bos_token_id</span> <span class="k">if</span> <span class="n">bos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">bos_token_id</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="k">if</span> <span class="n">vocab_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">vocab_size</span>
    <span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span> <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_new_tokens</span>
    <span class="n">output_scores</span> <span class="o">=</span> <span class="n">output_scores</span> <span class="k">if</span> <span class="n">output_scores</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">output_scores</span>
    <span class="n">output_attentions</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">output_attentions</span>
    <span class="p">)</span>
    <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">output_hidden_states</span>
    <span class="p">)</span>
    <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">return_dict_in_generate</span>
        <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">return_dict_in_generate</span>
    <span class="p">)</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">_beam_hyps</span><span class="p">)</span>
    <span class="n">num_beams</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span>

    <span class="n">batch_beam_size</span><span class="p">,</span> <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

    <span class="k">if</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">!=</span> <span class="n">batch_beam_size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Batch dimension of `input_ids` should be </span><span class="si">{</span><span class="n">num_beams</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">batch_size</span><span class="si">}</span><span class="s2">, but is </span><span class="si">{</span><span class="n">batch_beam_size</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>

    <span class="c1"># init attention / hidden states / scores tuples</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">beam_indices</span> <span class="o">=</span> <span class="p">(</span>
        <span class="nb">tuple</span><span class="p">(()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_beam_size</span><span class="p">))</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="p">)</span>
    <span class="n">decoder_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">cross_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">decoder_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_hidden_states</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="c1"># initialise score of first beam with 0 and the rest with -1e9. This makes sure that only tokens</span>
    <span class="c1"># of the first beam are considered to avoid sampling the exact same tokens across all beams.</span>
    <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">beam_scores</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e9</span>
    <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_scores</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,))</span>

    <span class="c1"># init inference</span>
    <span class="n">model_inputs</span><span class="p">,</span> <span class="n">input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>
    <span class="n">pred_start_index</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span>
        <span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># update model_kwargs</span>
    <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;buffer_position&quot;</span><span class="p">:</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">],</span>
        <span class="s2">&quot;buffer_context&quot;</span><span class="p">:</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">],</span>
        <span class="s2">&quot;buffer_sample_ids&quot;</span><span class="p">:</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;sample_ids&quot;</span><span class="p">],</span>
        <span class="s2">&quot;buffer_num_segments&quot;</span><span class="p">:</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;num_segments&quot;</span><span class="p">],</span>
        <span class="s2">&quot;buffer_segments&quot;</span><span class="p">:</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;segment&quot;</span><span class="p">],</span>
        <span class="s2">&quot;buffer&quot;</span><span class="p">:</span> <span class="n">outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_beam_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">batch_beam_size</span><span class="p">,</span> <span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;sample_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_beam_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">batch_beam_size</span><span class="p">,</span> <span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;num_segments&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;num_segments&quot;</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
    <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;segment_rel_offset&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;segment_rel_offset&quot;</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
    <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">past_key_values</span>

    <span class="n">ext_table_ids_cpu</span> <span class="o">=</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;ext_table_ids&quot;</span><span class="p">]</span>
    <span class="n">ext_table_sub_cpu</span> <span class="o">=</span> <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;ext_table_sub&quot;</span><span class="p">]</span>

    <span class="n">cur_len</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">model_inputs</span><span class="p">,</span> <span class="n">input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">beam_scorer</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span>
        <span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span>
            <span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">_done</span><span class="p">):</span>
            <span class="k">break</span>
        <span class="c1"># hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`</span>
        <span class="c1"># cannot be generated both before and after the `ops.log_softmax` operation.</span>
        <span class="n">next_token_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjust_logits_during_generation</span><span class="p">(</span>
            <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">ext_table_ids_cpu</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span>
        <span class="p">)</span>

        <span class="c1"># repetition_penalty</span>
        <span class="n">beam_scorer</span><span class="o">.</span><span class="n">apply_repetition_penalty</span><span class="p">(</span>
            <span class="n">next_token_logits</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="p">,</span>
            <span class="n">num_beams</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">repetition_penalty</span><span class="p">,</span>
            <span class="n">pred_start_index</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
            <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">_next_token_scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span>
            <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>

        <span class="n">next_token_scores_processed</span> <span class="o">=</span> <span class="n">logits_processor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">_next_token_scores</span><span class="p">)</span>
        <span class="c1"># next_token_scores_processed = _next_token_scores</span>
        <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores_processed</span> <span class="o">+</span> <span class="n">beam_scores</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">_next_token_scores</span><span class="p">)</span>

        <span class="c1"># Store scores, attentions and hidden_states when required</span>
        <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                <span class="n">scores</span> <span class="o">+=</span> <span class="p">(</span><span class="n">next_token_scores_processed</span><span class="p">,)</span>
            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                <span class="n">decoder_attentions</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,)</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                    <span class="n">cross_attentions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,)</span>

            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">decoder_hidden_states</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
                    <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,)</span>
                <span class="p">)</span>

        <span class="c1"># reshape for beam search</span>
        <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Sample 2 next tokens for each beam (so we have some spare tokens and match output of beam search)</span>
        <span class="n">next_token_scores</span><span class="p">,</span> <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span>
            <span class="n">next_token_scores</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">largest</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="n">beam_outputs</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">process</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span>
            <span class="n">cur_len</span><span class="p">,</span>
            <span class="n">_next_token_scores</span><span class="p">,</span>
            <span class="n">next_token_scores</span><span class="p">,</span>
            <span class="n">next_tokens</span><span class="p">,</span>
            <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
            <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
            <span class="n">bos_token_id</span><span class="o">=</span><span class="n">bos_token_id</span><span class="p">,</span>
            <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">ext_table_ids_cpu</span><span class="o">=</span><span class="n">ext_table_ids_cpu</span><span class="p">,</span>
            <span class="n">ext_table_sub_cpu</span><span class="o">=</span><span class="n">ext_table_sub_cpu</span><span class="p">,</span>
            <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">beam_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">beam_idx</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_indices&quot;</span><span class="p">]</span>
        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_scores&quot;</span><span class="p">]</span>

        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">beam_idx</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="p">:]</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">model_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_states&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reorder_cache</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_states&quot;</span><span class="p">],</span> <span class="n">beam_idx</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">:</span>
            <span class="n">beam_indices</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">((</span><span class="n">beam_indices</span><span class="p">[</span><span class="n">beam_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+</span> <span class="p">(</span><span class="n">beam_idx</span><span class="p">[</span><span class="n">i</span><span class="p">],)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">beam_indices</span><span class="p">))))</span>

        <span class="n">cur_len</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">is_done</span> <span class="ow">or</span> <span class="n">cur_len</span> <span class="o">==</span> <span class="n">max_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">synced_gpus</span><span class="p">:</span>
                <span class="k">break</span>

    <span class="n">sequence_outputs</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">finalize</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">sequence_outputs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeForCausalLM</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_id_sub</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_segments</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">segment</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">segment_rel_offset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">segment_rel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">span</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ext_table_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ext_table_sub</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<code>CPMBeeTokenizer</code>]. See [<code>PreTrainedTokenizer.encode</code>] and
[<code>PreTrainedTokenizer.__call__</code>] for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_id_sub</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Subscription of input sequence tokens in the vocabulary.</p>
<p>Subscription of normal text will be zero while the special tokens of each group will be the 0, 1, 2,
... <ans_0>, <ans_1>, <ans_2> ... belongs to group <ans>. <mask_0>, <mask_1>, <mask_2> ... belongs to
group <mask>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>length</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The length of sequences in batch.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>context</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether this token id is context or not. If is context, the value is 1. If not, the value is 0. If a
token id is context, it does not need to be predicted.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sample_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Give a sample id to every token id. The token ids with same sample ids belongs to the same sample.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_segments</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Total number of segments in the current input.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>segment</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Give a segment id to every token id. The token ids with same segment ids belongs to the same sample.</p>
<p>Generally, a string key or value in input data will be a segment. For example, input {"input": "hello,
", "<ans>": ""}, the segments includes: "input", "hello, ", "<ans>" and "".</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>segment_rel_offset</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The offset of segment rel.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>segment_rel</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The segment relevance. A relative implementation of measuring the importance of segments.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>span</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Span will record every input_ids shape.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, Union[mindspore.Tensor, List]]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the attentions tensors of all attention layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the hidden states of all layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_key_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dummy arguments for CPMBee. The <code>past_states</code> contains pre-computed hidden-states (key and values in
the self-attention blocks and in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) and other history arguments to speed up sequential decoding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding
(see <code>past_key_values</code>).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>labels</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Labels for computing the masked language modeling loss.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return a [<code>~utils.ModelOutput</code>] instead of a plain tuple.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ext_table_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>ext_table ids for embedding projection.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ext_table_sub</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>ext_table subscriptions for embedding projection.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span>
<span class="normal">2105</span>
<span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span>
<span class="normal">2109</span>
<span class="normal">2110</span>
<span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_id_sub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_segments</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">segment</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">segment_rel_offset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">segment_rel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">span</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ext_table_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># (ext_table_size) int32</span>
    <span class="n">ext_table_sub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># (ext_table_size) int32</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">CausalLMOutputWithPast</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        input_ids (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">            Indices can be obtained using [`CPMBeeTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class="sd">            [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">            [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">        input_id_sub (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            Subscription of input sequence tokens in the vocabulary.</span>

<span class="sd">            Subscription of normal text will be zero while the special tokens of each group will be the 0, 1, 2,</span>
<span class="sd">            ... &lt;ans_0&gt;, &lt;ans_1&gt;, &lt;ans_2&gt; ... belongs to group &lt;ans&gt;. &lt;mask_0&gt;, &lt;mask_1&gt;, &lt;mask_2&gt; ... belongs to</span>
<span class="sd">            group &lt;mask&gt;.</span>
<span class="sd">        length (`mindspore.Tensor` of shape `(batch_size)`):</span>
<span class="sd">            The length of sequences in batch.</span>
<span class="sd">        context (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            Whether this token id is context or not. If is context, the value is 1. If not, the value is 0. If a</span>
<span class="sd">            token id is context, it does not need to be predicted.</span>
<span class="sd">        sample_ids (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            Give a sample id to every token id. The token ids with same sample ids belongs to the same sample.</span>
<span class="sd">        num_segments (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            Total number of segments in the current input.</span>
<span class="sd">        segment (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            Give a segment id to every token id. The token ids with same segment ids belongs to the same sample.</span>

<span class="sd">            Generally, a string key or value in input data will be a segment. For example, input {&quot;input&quot;: &quot;hello,</span>
<span class="sd">            &quot;, &quot;&lt;ans&gt;&quot;: &quot;&quot;}, the segments includes: &quot;input&quot;, &quot;hello, &quot;, &quot;&lt;ans&gt;&quot; and &quot;&quot;.</span>
<span class="sd">        segment_rel_offset (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            The offset of segment rel.</span>
<span class="sd">        segment_rel (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            The segment relevance. A relative implementation of measuring the importance of segments.</span>
<span class="sd">        span (`Dict[str, Union[mindspore.Tensor, List]]`):</span>
<span class="sd">            Span will record every input_ids shape.</span>
<span class="sd">        output_attentions (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the attentions tensors of all attention layers.</span>
<span class="sd">        output_hidden_states (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the hidden states of all layers.</span>
<span class="sd">        past_key_values (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>
<span class="sd">            A dummy arguments for CPMBee. The `past_states` contains pre-computed hidden-states (key and values in</span>
<span class="sd">            the self-attention blocks and in the cross-attention blocks) that can be used (see `past_key_values`</span>
<span class="sd">            input) and other history arguments to speed up sequential decoding.</span>
<span class="sd">        use_cache (`bool`, *optional*):</span>
<span class="sd">            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding</span>
<span class="sd">            (see `past_key_values`).</span>
<span class="sd">        labels (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Labels for computing the masked language modeling loss.</span>
<span class="sd">        return_dict (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class="sd">        ext_table_ids (`mindspore.Tensor`, *optional*):</span>
<span class="sd">            ext_table ids for embedding projection.</span>
<span class="sd">        ext_table_sub (`mindspore.Tensor`, *optional*):</span>
<span class="sd">            ext_table subscriptions for embedding projection.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

    <span class="n">model_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">input_id_sub</span><span class="p">,</span>
        <span class="n">length</span><span class="p">,</span>
        <span class="n">context</span><span class="p">,</span>
        <span class="n">sample_ids</span><span class="p">,</span>
        <span class="n">num_segments</span><span class="p">,</span>
        <span class="n">segment</span><span class="p">,</span>
        <span class="n">segment_rel_offset</span><span class="p">,</span>
        <span class="n">segment_rel</span><span class="p">,</span>
        <span class="n">span</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model_output</span><span class="o">.</span><span class="n">last_hidden_state</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="k">else</span> <span class="n">model_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">ext_table_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ext_table</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">input_embedding</span><span class="p">(</span><span class="n">ext_table_ids</span><span class="p">,</span> <span class="n">ext_table_sub</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ext_table</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">input_embedding</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">ext_table</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">labels</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">model_output</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

    <span class="k">return</span> <span class="n">CausalLMOutputWithPast</span><span class="p">(</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
        <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_output</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">model_output</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attentions</span><span class="o">=</span><span class="n">model_output</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.generate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeForCausalLM</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.generate" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Override the generate for CPMBee. It will accept dict or list(dict) as input and returns dict or list(dict)
with <code>&lt;ans&gt;</code> filled.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>data_list</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sequence used as a prompt for the generation or as model inputs to the encoder. If dict, data_list
will be wrapped as a list.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict` or `list(dict)`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tokenizer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>CpmBeeTokenizer</code>):
The tokenizer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.tokenization_cpmbee.CpmBeeTokenizer">CpmBeeTokenizer</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>generation_config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The generation configuration to be used as base parametrization for the generation call. <code>**kwargs</code>
passed to generate matching the attributes of <code>generation_config</code> will override them. If
<code>generation_config</code> is not provided, the default will be used, which had the following loading
priority: 1) from the <code>generation_config.json</code> model file, if it exists; 2) from the model
configuration. Please note that unspecified parameters will inherit [<code>~generation.GenerationConfig</code>]'s
default values, whose documentation should be checked to parameterize generation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`~generation.GenerationConfig`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2915</span>
<span class="normal">2916</span>
<span class="normal">2917</span>
<span class="normal">2918</span>
<span class="normal">2919</span>
<span class="normal">2920</span>
<span class="normal">2921</span>
<span class="normal">2922</span>
<span class="normal">2923</span>
<span class="normal">2924</span>
<span class="normal">2925</span>
<span class="normal">2926</span>
<span class="normal">2927</span>
<span class="normal">2928</span>
<span class="normal">2929</span>
<span class="normal">2930</span>
<span class="normal">2931</span>
<span class="normal">2932</span>
<span class="normal">2933</span>
<span class="normal">2934</span>
<span class="normal">2935</span>
<span class="normal">2936</span>
<span class="normal">2937</span>
<span class="normal">2938</span>
<span class="normal">2939</span>
<span class="normal">2940</span>
<span class="normal">2941</span>
<span class="normal">2942</span>
<span class="normal">2943</span>
<span class="normal">2944</span>
<span class="normal">2945</span>
<span class="normal">2946</span>
<span class="normal">2947</span>
<span class="normal">2948</span>
<span class="normal">2949</span>
<span class="normal">2950</span>
<span class="normal">2951</span>
<span class="normal">2952</span>
<span class="normal">2953</span>
<span class="normal">2954</span>
<span class="normal">2955</span>
<span class="normal">2956</span>
<span class="normal">2957</span>
<span class="normal">2958</span>
<span class="normal">2959</span>
<span class="normal">2960</span>
<span class="normal">2961</span>
<span class="normal">2962</span>
<span class="normal">2963</span>
<span class="normal">2964</span>
<span class="normal">2965</span>
<span class="normal">2966</span>
<span class="normal">2967</span>
<span class="normal">2968</span>
<span class="normal">2969</span>
<span class="normal">2970</span>
<span class="normal">2971</span>
<span class="normal">2972</span>
<span class="normal">2973</span>
<span class="normal">2974</span>
<span class="normal">2975</span>
<span class="normal">2976</span>
<span class="normal">2977</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">data_list</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]],</span>
    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">CpmBeeTokenizer</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Override the generate for CPMBee. It will accept dict or list(dict) as input and returns dict or list(dict)</span>
<span class="sd">    with `&lt;ans&gt;` filled.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        data_list (`dict` or `list(dict)`):</span>
<span class="sd">            The sequence used as a prompt for the generation or as model inputs to the encoder. If dict, data_list</span>
<span class="sd">            will be wrapped as a list.</span>
<span class="sd">        tokenizer: (`CpmBeeTokenizer`):</span>
<span class="sd">            The tokenizer.</span>
<span class="sd">        generation_config (`~generation.GenerationConfig`, *optional*):</span>
<span class="sd">            The generation configuration to be used as base parametrization for the generation call. `**kwargs`</span>
<span class="sd">            passed to generate matching the attributes of `generation_config` will override them. If</span>
<span class="sd">            `generation_config` is not provided, the default will be used, which had the following loading</span>
<span class="sd">            priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model</span>
<span class="sd">            configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]&#39;s</span>
<span class="sd">            default values, whose documentation should be checked to parameterize generation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">data_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">data_list</span><span class="p">]</span>
    <span class="n">input_encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;ms&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">input_encoded</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">input_encoded</span><span class="p">[</span><span class="s2">&quot;vocab_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span>

    <span class="n">decode_res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_encoded</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">sent_id</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">decode_res</span><span class="p">):</span>
        <span class="n">ans_result_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">raw_word_id</span><span class="p">,</span> <span class="n">ans_id</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">ans_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ans_result_map</span><span class="p">:</span>
                <span class="n">ans_result_map</span><span class="p">[</span><span class="n">ans_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">ans_result_map</span><span class="p">[</span><span class="n">ans_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">raw_word_id</span><span class="p">)</span>

        <span class="n">answer_placeholders</span> <span class="o">=</span> <span class="n">input_encoded</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;answer_placeholders&quot;</span><span class="p">]</span>
        <span class="n">ext_table</span> <span class="o">=</span> <span class="n">input_encoded</span><span class="p">[</span><span class="s2">&quot;other_info&quot;</span><span class="p">][</span><span class="n">sent_id</span><span class="p">][</span><span class="s2">&quot;ext_table&quot;</span><span class="p">]</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">data_list</span><span class="p">[</span><span class="n">sent_id</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">ans_id</span><span class="p">,</span> <span class="n">token_ids</span> <span class="ow">in</span> <span class="n">ans_result_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">token_ids</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">:</span>
                <span class="n">token_ids</span> <span class="o">=</span> <span class="n">token_ids</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">ext_table</span><span class="p">)</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">answer_placeholders</span><span class="p">[</span><span class="n">ans_id</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">path</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;&lt;ans&gt;&quot;</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="n">path</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                    <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">part</span><span class="p">]</span>
                <span class="n">p</span><span class="p">[</span><span class="n">path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">text</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">data</span><span class="p">[</span><span class="s2">&quot;&lt;ans&gt;&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text</span>
        <span class="k">for</span> <span class="n">ans_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">answer_placeholders</span><span class="p">)):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">ans_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ans_result_map</span><span class="p">:</span>
                <span class="n">path</span> <span class="o">=</span> <span class="n">answer_placeholders</span><span class="p">[</span><span class="n">ans_id</span><span class="p">]</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;&lt;ans&gt;&quot;</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="n">path</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                    <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">part</span><span class="p">]</span>
                <span class="n">p</span><span class="p">[</span><span class="n">path</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">data_list</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.get_input_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeForCausalLM</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.get_input_embeddings" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method retrieves the input embeddings from the CpmBeeForCausalLM object.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CpmBeeForCausalLM class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM">CpmBeeForCausalLM</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_embedding</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>This method returns the input embeddings, which are of type None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2250</span>
<span class="normal">2251</span>
<span class="normal">2252</span>
<span class="normal">2253</span>
<span class="normal">2254</span>
<span class="normal">2255</span>
<span class="normal">2256</span>
<span class="normal">2257</span>
<span class="normal">2258</span>
<span class="normal">2259</span>
<span class="normal">2260</span>
<span class="normal">2261</span>
<span class="normal">2262</span>
<span class="normal">2263</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method retrieves the input embeddings from the CpmBeeForCausalLM object.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CpmBeeForCausalLM): The instance of the CpmBeeForCausalLM class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        input_embedding: This method returns the input embeddings, which are of type None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">input_embedding</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.get_output_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeForCausalLM</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.get_output_embeddings" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Returns the output embeddings for the CpmBeeForCausalLM model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeForCausalLM class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2281</span>
<span class="normal">2282</span>
<span class="normal">2283</span>
<span class="normal">2284</span>
<span class="normal">2285</span>
<span class="normal">2286</span>
<span class="normal">2287</span>
<span class="normal">2288</span>
<span class="normal">2289</span>
<span class="normal">2290</span>
<span class="normal">2291</span>
<span class="normal">2292</span>
<span class="normal">2293</span>
<span class="normal">2294</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the output embeddings for the CpmBeeForCausalLM model.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: An instance of the CpmBeeForCausalLM class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.inference" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeForCausalLM</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_id_sub</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_segments</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">segment</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">segment_rel_offset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">segment_rel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ext_table_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ext_table_sub</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.inference" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<code>CPMBeeTokenizer</code>]. See [<code>PreTrainedTokenizer.encode</code>] and
[<code>PreTrainedTokenizer.__call__</code>] for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_id_sub</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Subscription of input sequence tokens in the vocabulary.</p>
<p>Subscription of normal text will be zero while the special tokens of each group will be the 0, 1, 2,
... <ans_0>, <ans_1>, <ans_2> ... belongs to group <ans>. <mask_0>, <mask_1>, <mask_2> ... belongs to
group <mask>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The position of input sequence tokens in the vocabulary for each segment. if segment1 is 0, 1, 2 and
segment2 is 0, 1, 2, 3, the position will be 0, 1, 2, 0, 1, 2, 3</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>context</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether this token id is context or not. If is context, the value is 1. If not, the value is 0. If a
token id is context, it does not need to be predicted.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sample_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Give a sample id to every token id. The token ids with same sample ids belongs to the same sample.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_segments</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Total number of segments in the current input.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>segment</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Give a segment id to every token id. The token ids with same segment ids belongs to the same sample.</p>
<p>Generally, a string key or value in input data will be a segment. For example, input {"input": "hello,
", "<ans>": ""}, the segments includes: "input", "hello, ", "<ans>" and "".</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>segment_rel_offset</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The offset of segment rel.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>segment_rel</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The segment relevance. A relative implementation of measuring the importance of segments.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, seq_len)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Store the history information including position, context, sample_ids, num_segments, segment and
past_key_values.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, Union[mindspore.Tensor, List]]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the attentions tensors of all attention layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the hidden states of all layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_key_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dummy arguments for CPMBee. The <code>past_states</code> contains pre-computed hidden-states (key and values in
the self-attention blocks and in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) and other history arguments to speed up sequential decoding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding
(see <code>past_key_values</code>).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>labels</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Labels for computing the masked language modeling loss.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return a [<code>~utils.ModelOutput</code>] instead of a plain tuple.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ext_table_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>ext_table ids for embedding projection.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ext_table_sub</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>ext_table subscriptions for embedding projection.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="example" open>
  <summary>Example</summary>
  <p>Text Generation with CpmBeeForCausalLM.
<div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">CpmBeeTokenizer</span><span class="p">,</span> <span class="n">CpmBeeForCausalLM</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">texts</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;今天天气不错，&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;ans&gt;&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">CpmBeeForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openbmb/cpm-bee-10b&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CPMBeeTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openbmb/cpm-bee-10b&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">output_texts</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">({</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;今天天气不错，&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;ans&gt;&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">},</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">output_texts</span><span class="p">)</span>
<span class="p">{</span><span class="s1">&#39;input&#39;</span><span class="p">:</span> <span class="s1">&#39;今天天气不错，&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;ans&gt;&#39;</span><span class="p">:</span> <span class="s1">&#39;适合睡觉。&#39;</span><span class="p">}</span>
</code></pre></div></p>
</details>
            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span>
<span class="normal">2141</span>
<span class="normal">2142</span>
<span class="normal">2143</span>
<span class="normal">2144</span>
<span class="normal">2145</span>
<span class="normal">2146</span>
<span class="normal">2147</span>
<span class="normal">2148</span>
<span class="normal">2149</span>
<span class="normal">2150</span>
<span class="normal">2151</span>
<span class="normal">2152</span>
<span class="normal">2153</span>
<span class="normal">2154</span>
<span class="normal">2155</span>
<span class="normal">2156</span>
<span class="normal">2157</span>
<span class="normal">2158</span>
<span class="normal">2159</span>
<span class="normal">2160</span>
<span class="normal">2161</span>
<span class="normal">2162</span>
<span class="normal">2163</span>
<span class="normal">2164</span>
<span class="normal">2165</span>
<span class="normal">2166</span>
<span class="normal">2167</span>
<span class="normal">2168</span>
<span class="normal">2169</span>
<span class="normal">2170</span>
<span class="normal">2171</span>
<span class="normal">2172</span>
<span class="normal">2173</span>
<span class="normal">2174</span>
<span class="normal">2175</span>
<span class="normal">2176</span>
<span class="normal">2177</span>
<span class="normal">2178</span>
<span class="normal">2179</span>
<span class="normal">2180</span>
<span class="normal">2181</span>
<span class="normal">2182</span>
<span class="normal">2183</span>
<span class="normal">2184</span>
<span class="normal">2185</span>
<span class="normal">2186</span>
<span class="normal">2187</span>
<span class="normal">2188</span>
<span class="normal">2189</span>
<span class="normal">2190</span>
<span class="normal">2191</span>
<span class="normal">2192</span>
<span class="normal">2193</span>
<span class="normal">2194</span>
<span class="normal">2195</span>
<span class="normal">2196</span>
<span class="normal">2197</span>
<span class="normal">2198</span>
<span class="normal">2199</span>
<span class="normal">2200</span>
<span class="normal">2201</span>
<span class="normal">2202</span>
<span class="normal">2203</span>
<span class="normal">2204</span>
<span class="normal">2205</span>
<span class="normal">2206</span>
<span class="normal">2207</span>
<span class="normal">2208</span>
<span class="normal">2209</span>
<span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span>
<span class="normal">2219</span>
<span class="normal">2220</span>
<span class="normal">2221</span>
<span class="normal">2222</span>
<span class="normal">2223</span>
<span class="normal">2224</span>
<span class="normal">2225</span>
<span class="normal">2226</span>
<span class="normal">2227</span>
<span class="normal">2228</span>
<span class="normal">2229</span>
<span class="normal">2230</span>
<span class="normal">2231</span>
<span class="normal">2232</span>
<span class="normal">2233</span>
<span class="normal">2234</span>
<span class="normal">2235</span>
<span class="normal">2236</span>
<span class="normal">2237</span>
<span class="normal">2238</span>
<span class="normal">2239</span>
<span class="normal">2240</span>
<span class="normal">2241</span>
<span class="normal">2242</span>
<span class="normal">2243</span>
<span class="normal">2244</span>
<span class="normal">2245</span>
<span class="normal">2246</span>
<span class="normal">2247</span>
<span class="normal">2248</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">inference</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_id_sub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_segments</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">segment</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">segment_rel_offset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">segment_rel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">past_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ext_table_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># (ext_table_size) int32</span>
    <span class="n">ext_table_sub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># (ext_table_size) int32</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">CausalLMOutputWithPast</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        input_ids (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            Indices of input sequence tokens in the vocabulary.</span>

<span class="sd">            Indices can be obtained using [`CPMBeeTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class="sd">            [`PreTrainedTokenizer.__call__`] for details.</span>

<span class="sd">            [What are input IDs?](../glossary#input-ids)</span>
<span class="sd">        input_id_sub (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            Subscription of input sequence tokens in the vocabulary.</span>

<span class="sd">            Subscription of normal text will be zero while the special tokens of each group will be the 0, 1, 2,</span>
<span class="sd">            ... &lt;ans_0&gt;, &lt;ans_1&gt;, &lt;ans_2&gt; ... belongs to group &lt;ans&gt;. &lt;mask_0&gt;, &lt;mask_1&gt;, &lt;mask_2&gt; ... belongs to</span>
<span class="sd">            group &lt;mask&gt;.</span>
<span class="sd">        position (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            The position of input sequence tokens in the vocabulary for each segment. if segment1 is 0, 1, 2 and</span>
<span class="sd">            segment2 is 0, 1, 2, 3, the position will be 0, 1, 2, 0, 1, 2, 3</span>
<span class="sd">        context (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            Whether this token id is context or not. If is context, the value is 1. If not, the value is 0. If a</span>
<span class="sd">            token id is context, it does not need to be predicted.</span>
<span class="sd">        sample_ids (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            Give a sample id to every token id. The token ids with same sample ids belongs to the same sample.</span>
<span class="sd">        num_segments (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            Total number of segments in the current input.</span>
<span class="sd">        segment (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            Give a segment id to every token id. The token ids with same segment ids belongs to the same sample.</span>

<span class="sd">            Generally, a string key or value in input data will be a segment. For example, input {&quot;input&quot;: &quot;hello,</span>
<span class="sd">            &quot;, &quot;&lt;ans&gt;&quot;: &quot;&quot;}, the segments includes: &quot;input&quot;, &quot;hello, &quot;, &quot;&lt;ans&gt;&quot; and &quot;&quot;.</span>
<span class="sd">        segment_rel_offset (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            The offset of segment rel.</span>
<span class="sd">        segment_rel (`mindspore.Tensor` of shape `(batch_size, seq_len)`):</span>
<span class="sd">            The segment relevance. A relative implementation of measuring the importance of segments.</span>
<span class="sd">        past_states (`Dict[str, Union[mindspore.Tensor, List]]`):</span>
<span class="sd">            Store the history information including position, context, sample_ids, num_segments, segment and</span>
<span class="sd">            past_key_values.</span>
<span class="sd">        output_attentions (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the attentions tensors of all attention layers.</span>
<span class="sd">        output_hidden_states (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the hidden states of all layers.</span>
<span class="sd">        past_key_values (`tuple(tuple(mindspore.Tensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>
<span class="sd">            A dummy arguments for CPMBee. The `past_states` contains pre-computed hidden-states (key and values in</span>
<span class="sd">            the self-attention blocks and in the cross-attention blocks) that can be used (see `past_key_values`</span>
<span class="sd">            input) and other history arguments to speed up sequential decoding.</span>
<span class="sd">        use_cache (`bool`, *optional*):</span>
<span class="sd">            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding</span>
<span class="sd">            (see `past_key_values`).</span>
<span class="sd">        labels (`mindspore.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class="sd">            Labels for computing the masked language modeling loss.</span>
<span class="sd">        return_dict (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class="sd">        ext_table_ids (`mindspore.Tensor`, *optional*):</span>
<span class="sd">            ext_table ids for embedding projection.</span>
<span class="sd">        ext_table_sub (`mindspore.Tensor`, *optional*):</span>
<span class="sd">            ext_table subscriptions for embedding projection.</span>

<span class="sd">    Example:</span>
<span class="sd">        Text Generation with CpmBeeForCausalLM.</span>
<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; from transformers import CpmBeeTokenizer, CpmBeeForCausalLM</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; texts = {&quot;input&quot;: &quot;今天天气不错，&quot;, &quot;&lt;ans&gt;&quot;: &quot;&quot;}</span>
<span class="sd">        &gt;&gt;&gt; model = CpmBeeForCausalLM.from_pretrained(&quot;openbmb/cpm-bee-10b&quot;)</span>
<span class="sd">        &gt;&gt;&gt; tokenizer = CPMBeeTokenizer.from_pretrained(&quot;openbmb/cpm-bee-10b&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output_texts = model.generate({&quot;input&quot;: &quot;今天天气不错，&quot;, &quot;&lt;ans&gt;&quot;: &quot;&quot;}, tokenizer)</span>
<span class="sd">        &gt;&gt;&gt; print(output_texts)</span>
<span class="sd">        {&#39;input&#39;: &#39;今天天气不错，&#39;, &#39;&lt;ans&gt;&#39;: &#39;适合睡觉。&#39;}</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

    <span class="n">model_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">input_id_sub</span><span class="p">,</span>
        <span class="n">position</span><span class="p">,</span>
        <span class="n">context</span><span class="p">,</span>
        <span class="n">sample_ids</span><span class="p">,</span>
        <span class="n">num_segments</span><span class="p">,</span>
        <span class="n">segment</span><span class="p">,</span>
        <span class="n">segment_rel_offset</span><span class="p">,</span>
        <span class="n">segment_rel</span><span class="p">,</span>
        <span class="n">past_states</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model_output</span><span class="o">.</span><span class="n">last_hidden_state</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="k">else</span> <span class="n">model_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">ext_table_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="mi">0</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ext_table_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="n">ext_table</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">input_embedding</span><span class="p">(</span><span class="n">ext_table_ids</span><span class="p">,</span> <span class="n">ext_table_sub</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ext_table</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">input_embedding</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">ext_table</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">model_output</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

    <span class="k">return</span> <span class="n">CausalLMOutputWithPast</span><span class="p">(</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
        <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="n">model_output</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">model_output</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attentions</span><span class="o">=</span><span class="n">model_output</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.prepare_inputs_for_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeForCausalLM</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">beam_scorer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_id_subs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_pos</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">segment_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_ext_table_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_ext_table_sub</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">other_info</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.prepare_inputs_for_generation" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Choose the current input according to beam states.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2316</span>
<span class="normal">2317</span>
<span class="normal">2318</span>
<span class="normal">2319</span>
<span class="normal">2320</span>
<span class="normal">2321</span>
<span class="normal">2322</span>
<span class="normal">2323</span>
<span class="normal">2324</span>
<span class="normal">2325</span>
<span class="normal">2326</span>
<span class="normal">2327</span>
<span class="normal">2328</span>
<span class="normal">2329</span>
<span class="normal">2330</span>
<span class="normal">2331</span>
<span class="normal">2332</span>
<span class="normal">2333</span>
<span class="normal">2334</span>
<span class="normal">2335</span>
<span class="normal">2336</span>
<span class="normal">2337</span>
<span class="normal">2338</span>
<span class="normal">2339</span>
<span class="normal">2340</span>
<span class="normal">2341</span>
<span class="normal">2342</span>
<span class="normal">2343</span>
<span class="normal">2344</span>
<span class="normal">2345</span>
<span class="normal">2346</span>
<span class="normal">2347</span>
<span class="normal">2348</span>
<span class="normal">2349</span>
<span class="normal">2350</span>
<span class="normal">2351</span>
<span class="normal">2352</span>
<span class="normal">2353</span>
<span class="normal">2354</span>
<span class="normal">2355</span>
<span class="normal">2356</span>
<span class="normal">2357</span>
<span class="normal">2358</span>
<span class="normal">2359</span>
<span class="normal">2360</span>
<span class="normal">2361</span>
<span class="normal">2362</span>
<span class="normal">2363</span>
<span class="normal">2364</span>
<span class="normal">2365</span>
<span class="normal">2366</span>
<span class="normal">2367</span>
<span class="normal">2368</span>
<span class="normal">2369</span>
<span class="normal">2370</span>
<span class="normal">2371</span>
<span class="normal">2372</span>
<span class="normal">2373</span>
<span class="normal">2374</span>
<span class="normal">2375</span>
<span class="normal">2376</span>
<span class="normal">2377</span>
<span class="normal">2378</span>
<span class="normal">2379</span>
<span class="normal">2380</span>
<span class="normal">2381</span>
<span class="normal">2382</span>
<span class="normal">2383</span>
<span class="normal">2384</span>
<span class="normal">2385</span>
<span class="normal">2386</span>
<span class="normal">2387</span>
<span class="normal">2388</span>
<span class="normal">2389</span>
<span class="normal">2390</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">beam_scorer</span><span class="p">:</span> <span class="n">CpmBeeBeamSearchScorer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_id_subs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">input_pos</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">segment_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">batch_ext_table_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">batch_ext_table_sub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">other_info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Choose the current input according to beam states.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># init preparation</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;context&quot;</span><span class="p">)</span>
    <span class="n">sample_ids</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;sample_ids&quot;</span><span class="p">)</span>
    <span class="n">segment_rel_offset</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;segment_rel_offset&quot;</span><span class="p">)</span>
    <span class="n">num_segments</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;num_segments&quot;</span><span class="p">)</span>
    <span class="n">segment_rel</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;segment_rel&quot;</span><span class="p">)</span>
    <span class="n">past_states</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_states&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">_input_ids</span> <span class="o">=</span> <span class="n">input_ids</span>

    <span class="c1"># update input in generation</span>
    <span class="k">if</span> <span class="n">beam_scorer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">tmp_input</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">tmp_input_sub</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">tmp_position</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">tmp_segment</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">sent_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">beam_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span><span class="p">):</span>
                <span class="n">tmp_input</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">beam_states</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="n">beam_id</span><span class="p">][</span><span class="s2">&quot;nx_token_id&quot;</span><span class="p">])</span>
                <span class="n">tmp_input_sub</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">beam_states</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="n">beam_id</span><span class="p">][</span><span class="s2">&quot;nx_token_sub&quot;</span><span class="p">])</span>
                <span class="n">tmp_position</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">beam_states</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="n">beam_id</span><span class="p">][</span><span class="s2">&quot;nx_position&quot;</span><span class="p">])</span>
                <span class="n">tmp_segment</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">beam_states</span><span class="p">[</span><span class="n">sent_id</span><span class="p">][</span><span class="n">beam_id</span><span class="p">][</span><span class="s2">&quot;nx_segment_id&quot;</span><span class="p">])</span>

        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;input_id_subs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_id_subs</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="n">tmp_input_sub</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;input_pos&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_pos</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="n">tmp_position</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;segment_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">segment_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="n">tmp_segment</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tmp_input</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                    <span class="n">batch_size</span> <span class="o">*</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span>
                <span class="p">),</span>
            <span class="p">],</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">_input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">_input_ids</span><span class="p">,</span>
        <span class="s2">&quot;input_id_sub&quot;</span><span class="p">:</span> <span class="n">input_id_subs</span><span class="p">,</span>
        <span class="s2">&quot;position&quot;</span><span class="p">:</span> <span class="n">input_pos</span><span class="p">,</span>
        <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">context</span><span class="p">,</span>
        <span class="s2">&quot;sample_ids&quot;</span><span class="p">:</span> <span class="n">sample_ids</span><span class="p">,</span>
        <span class="s2">&quot;segment_rel_offset&quot;</span><span class="p">:</span> <span class="n">segment_rel_offset</span><span class="p">,</span>
        <span class="s2">&quot;segment&quot;</span><span class="p">:</span> <span class="n">segment_ids</span><span class="p">,</span>
        <span class="s2">&quot;num_segments&quot;</span><span class="p">:</span> <span class="n">num_segments</span><span class="p">,</span>
        <span class="s2">&quot;segment_rel&quot;</span><span class="p">:</span> <span class="n">segment_rel</span><span class="p">,</span>
        <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
        <span class="s2">&quot;ext_table_ids&quot;</span><span class="p">:</span> <span class="n">batch_ext_table_ids</span><span class="p">,</span>
        <span class="s2">&quot;ext_table_sub&quot;</span><span class="p">:</span> <span class="n">batch_ext_table_sub</span><span class="p">,</span>
        <span class="s2">&quot;past_states&quot;</span><span class="p">:</span> <span class="n">past_states</span><span class="p">,</span>
    <span class="p">},</span> <span class="n">input_ids</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.set_input_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeForCausalLM</span><span class="o">.</span><span class="n">set_input_embeddings</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.set_input_embeddings" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Sets the input embeddings for the CpmBeeForCausalLM class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CpmBeeForCausalLM class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM">CpmBeeForCausalLM</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input embeddings to be set for the CpmBeeForCausalLM instance.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2265</span>
<span class="normal">2266</span>
<span class="normal">2267</span>
<span class="normal">2268</span>
<span class="normal">2269</span>
<span class="normal">2270</span>
<span class="normal">2271</span>
<span class="normal">2272</span>
<span class="normal">2273</span>
<span class="normal">2274</span>
<span class="normal">2275</span>
<span class="normal">2276</span>
<span class="normal">2277</span>
<span class="normal">2278</span>
<span class="normal">2279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets the input embeddings for the CpmBeeForCausalLM class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CpmBeeForCausalLM): The instance of the CpmBeeForCausalLM class.</span>
<span class="sd">        embeddings: The input embeddings to be set for the CpmBeeForCausalLM instance.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">embeddings</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.set_output_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeForCausalLM</span><span class="o">.</span><span class="n">set_output_embeddings</span><span class="p">(</span><span class="n">new_embeddings</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM.set_output_embeddings" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Sets the output embeddings for the CpmBeeForCausalLM model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CpmBeeForCausalLM class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeForCausalLM">CpmBeeForCausalLM</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>new_embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The new embeddings to be set as the output embeddings.
This should be a tensor or an object that can be converted to a tensor.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <p>This method sets the output embeddings of the CpmBeeForCausalLM model to the provided new embeddings.
The new embeddings are assigned to the 'lm_head' attribute of the model object.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2296</span>
<span class="normal">2297</span>
<span class="normal">2298</span>
<span class="normal">2299</span>
<span class="normal">2300</span>
<span class="normal">2301</span>
<span class="normal">2302</span>
<span class="normal">2303</span>
<span class="normal">2304</span>
<span class="normal">2305</span>
<span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span>
<span class="normal">2313</span>
<span class="normal">2314</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets the output embeddings for the CpmBeeForCausalLM model.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CpmBeeForCausalLM): The instance of the CpmBeeForCausalLM class.</span>
<span class="sd">        new_embeddings: The new embeddings to be set as the output embeddings.</span>
<span class="sd">            This should be a tensor or an object that can be converted to a tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>

<span class="sd">    This method sets the output embeddings of the CpmBeeForCausalLM model to the provided new embeddings.</span>
<span class="sd">    The new embeddings are assigned to the &#39;lm_head&#39; attribute of the model object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>We use Root Mean Square (RMS) Layer Normalization, please see https://arxiv.org/abs/1910.07467 for details."</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeLayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    We use Root Mean Square (RMS) Layer Normalization, please see https://arxiv.org/abs/1910.07467 for details.&quot;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a CpmBeeLayerNorm object with the provided configuration.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the CpmBeeLayerNorm class.</span>
<span class="sd">            config (CpmBeeConfig):</span>
<span class="sd">                An instance of the CpmBeeConfig class containing the configuration parameters.</span>

<span class="sd">                - config.eps (float): The value for epsilon used in normalization.</span>
<span class="sd">                - config.hidden_size (int): The dimension of the hidden size.</span>
<span class="sd">                - config.ms_dtype (str): The data type for the weight parameter.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim_norm</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_states (`mindspore.Tensor` of shape `(batch, seq_len, dim_in)`)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_norm</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;hidden_states.shape[-1] != self.dim_norm&quot;</span><span class="p">)</span>
        <span class="n">old_dtype</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">variance</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">old_dtype</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeLayerNorm</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a CpmBeeLayerNorm object with the provided configuration.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CpmBeeLayerNorm class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeConfig class containing the configuration parameters.</p>
<ul>
<li>config.eps (float): The value for epsilon used in normalization.</li>
<li>config.hidden_size (int): The dimension of the hidden size.</li>
<li>config.ms_dtype (str): The data type for the weight parameter.</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig">CpmBeeConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a CpmBeeLayerNorm object with the provided configuration.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the CpmBeeLayerNorm class.</span>
<span class="sd">        config (CpmBeeConfig):</span>
<span class="sd">            An instance of the CpmBeeConfig class containing the configuration parameters.</span>

<span class="sd">            - config.eps (float): The value for epsilon used in normalization.</span>
<span class="sd">            - config.hidden_size (int): The dimension of the hidden size.</span>
<span class="sd">            - config.ms_dtype (str): The data type for the weight parameter.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">eps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dim_norm</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeLayerNorm</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        hidden_states (`mindspore.Tensor` of shape `(batch, seq_len, dim_in)`)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_norm</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;hidden_states.shape[-1] != self.dim_norm&quot;</span><span class="p">)</span>
    <span class="n">old_dtype</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">old_dtype</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
    <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Linear">Linear</span></code></p>


        <p>This class represents a linear layer with a scale operation for CPMBee. It is a subclass of the nn.Linear class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear.dim_in">dim_in</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The input dimension of the linear layer.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear.dim_out">dim_out</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The output dimension of the linear layer.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear.weight">weight</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The weight parameter of the linear layer.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Parameter">Parameter</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear.__init__" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Construct a linear layer for CPMBee with a scale operation.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear.forward" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Apply the linear transformation to the input tensor.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span>
<span class="normal">97</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeLinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents a linear layer with a scale operation for CPMBee. It is a subclass of the nn.Linear class.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        dim_in (int): The input dimension of the linear layer.</span>
<span class="sd">        dim_out (int): The output dimension of the linear layer.</span>
<span class="sd">        weight (mindspore.Parameter): The weight parameter of the linear layer.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__(self, dim_in, dim_out, dtype):</span>
<span class="sd">            Construct a linear layer for CPMBee with a scale operation.</span>

<span class="sd">        forward(self, x):</span>
<span class="sd">            Apply the linear transformation to the input tensor.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Construct a linear for CPMBee. It contains a scale operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim_in</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">dim_in</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">dim_out</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">dim_in</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            x (`mindspore.Tensor` of shape `(batch, seq_len, dim_in)`): The input of linear layer</span>

<span class="sd">        Returns:</span>
<span class="sd">            `mindspore.Tensor` of shape `(batch, seq_len, dim_out)`: The output of the linear transform y.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_in</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeLinear</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Construct a linear for CPMBee. It contains a scale operation.</p>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct a linear for CPMBee. It contains a scale operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dim_in</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">dim_in</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dim_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">dim_out</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">dim_in</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeLinear</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLinear.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input of linear layer</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch, seq_len, dim_in)`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>mindspore.Tensor</code> of shape <code>(batch, seq_len, dim_out)</code>: The output of the linear transform y.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span>
<span class="normal">97</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Args:</span>
<span class="sd">        x (`mindspore.Tensor` of shape `(batch, seq_len, dim_in)`): The input of linear layer</span>

<span class="sd">    Returns:</span>
<span class="sd">        `mindspore.Tensor` of shape `(batch, seq_len, dim_out)`: The output of the linear transform y.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_in</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeePreTrainedModel" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeePreTrainedModel">CpmBeePreTrainedModel</a></code></p>


        <p>CpmBeeModel</p>
<p>This class represents a CpmBee model for natural language processing tasks.
It is a subclass of CpmBeePreTrainedModel and inherits all the functionality from it.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.encoder">encoder</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of CpmBeeEncoder, responsible for encoding the input sequences.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.input_embedding">input_embedding</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of CpmBeeEmbeddingExt, used for embedding the input sequences.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.position_bias">position_bias</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An instance of CpmBeeBucketPositionBias, used for calculating the position bias.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.vocab_size">vocab_size</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>An integer representing the size of the vocabulary.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.__init__" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the CpmBeeModel instance with the given configuration.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.get_input_embeddings" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.get_input_embeddings">get_input_embeddings</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Returns the input embedding instance.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.set_input_embeddings" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.set_input_embeddings">set_input_embeddings</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Sets the input embeddings to the given value.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.forward" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the CpmBee model with the provided input and configuration.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.inference" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.inference">inference</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Performs inference using the CpmBee model with the provided input and configuration.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span>
<span class="normal">1445</span>
<span class="normal">1446</span>
<span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeModel</span><span class="p">(</span><span class="n">CpmBeePreTrainedModel</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    CpmBeeModel</span>

<span class="sd">    This class represents a CpmBee model for natural language processing tasks.</span>
<span class="sd">    It is a subclass of CpmBeePreTrainedModel and inherits all the functionality from it.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        encoder: An instance of CpmBeeEncoder, responsible for encoding the input sequences.</span>
<span class="sd">        input_embedding: An instance of CpmBeeEmbeddingExt, used for embedding the input sequences.</span>
<span class="sd">        position_bias: An instance of CpmBeeBucketPositionBias, used for calculating the position bias.</span>
<span class="sd">        vocab_size: An integer representing the size of the vocabulary.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__: Initializes the CpmBeeModel instance with the given configuration.</span>
<span class="sd">        get_input_embeddings: Returns the input embedding instance.</span>
<span class="sd">        set_input_embeddings: Sets the input embeddings to the given value.</span>
<span class="sd">        forward: Constructs the CpmBee model with the provided input and configuration.</span>
<span class="sd">        inference: Performs inference using the CpmBee model with the provided input and configuration.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an instance of the CpmBeeModel class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The object instance.</span>
<span class="sd">            config (CpmBeeConfig):</span>
<span class="sd">                The configuration object that contains the model settings.</span>

<span class="sd">                - type: CpmBeeConfig</span>
<span class="sd">                - purpose: Specifies the model configuration.</span>
<span class="sd">                - restrictions: Must be an instance of CpmBeeConfig.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">half</span><span class="p">:</span>
            <span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">CpmBeeEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">CpmBeeEmbeddingExt</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_bias</span> <span class="o">=</span> <span class="n">CpmBeeBucketPositionBias</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method retrieves the input embeddings for the CpmBeeModel.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeModel): The instance of the CpmBeeModel class.</span>
<span class="sd">                It is used to access the input embeddings for the model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            input_embedding: The method returns the input embedding associated with the CpmBeeModel instance.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_embedding</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method sets the input embeddings for the CpmBeeModel.</span>

<span class="sd">        Args:</span>
<span class="sd">            embeddings (object): The input embeddings to be set for the model.</span>
<span class="sd">                It can be of any type and should contain the necessary information for input embeddings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">embeddings</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">input_id_sub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_segments</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">segment</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">segment_rel_offset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">segment_rel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">span</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs the CpmBeeModel.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The object itself.</span>
<span class="sd">            input_ids (mindspore.Tensor): The input tensor of shape (batch, seq_length) containing the input IDs.</span>
<span class="sd">            input_id_sub (Optional[mindspore.Tensor], optional):</span>
<span class="sd">                The optional input tensor of shape (batch, seq_length) containing the sub input IDs. Defaults to None.</span>
<span class="sd">            length (Optional[mindspore.Tensor], optional):</span>
<span class="sd">                The optional input tensor of shape (batch,) containing the length of the input sequences.</span>
<span class="sd">                Defaults to None.</span>
<span class="sd">            context (Optional[mindspore.Tensor], optional):</span>
<span class="sd">                The optional input tensor of shape (batch, seq_length) containing the context. Defaults to None.</span>
<span class="sd">            sample_ids (Optional[mindspore.Tensor], optional):</span>
<span class="sd">                The optional input tensor of shape (batch, seq_length) containing the sample IDs. Defaults to None.</span>
<span class="sd">            num_segments (Optional[mindspore.Tensor], optional):</span>
<span class="sd">                The optional input tensor of shape (batch, seq_length) containing the number of segments.</span>
<span class="sd">                Defaults to None.</span>
<span class="sd">            segment (Optional[mindspore.Tensor], optional):</span>
<span class="sd">                The optional input tensor of shape (batch, seq_length) containing the segments. Defaults to None.</span>
<span class="sd">            segment_rel_offset (Optional[mindspore.Tensor], optional):</span>
<span class="sd">                The optional input tensor of shape (batch, seq_length) containing the segment relative offset.</span>
<span class="sd">                Defaults to None.</span>
<span class="sd">            segment_rel (Optional[mindspore.Tensor], optional):</span>
<span class="sd">                The optional input tensor of shape (batch, seq_length) containing the segment relative.</span>
<span class="sd">                Defaults to None.</span>
<span class="sd">            span (Optional[Dict], optional):</span>
<span class="sd">                The optional input dictionary containing span information. Defaults to None.</span>
<span class="sd">            output_attentions (Optional[bool], optional):</span>
<span class="sd">                The optional boolean flag indicating whether to output attentions. Defaults to None.</span>
<span class="sd">            output_hidden_states (Optional[bool], optional):</span>
<span class="sd">                The optional boolean flag indicating whether to output hidden states. Defaults to None.</span>
<span class="sd">            past_key_values (Optional[List], optional):</span>
<span class="sd">                The optional list containing past key values. Defaults to None.</span>
<span class="sd">            use_cache (Optional[bool], optional):</span>
<span class="sd">                The optional boolean flag indicating whether to use cache. Defaults to None.</span>
<span class="sd">            return_dict (Optional[bool], optional):</span>
<span class="sd">                The optional boolean flag indicating whether to return a dictionary. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>

        <span class="c1"># dummy setting for common tests</span>
        <span class="k">if</span> <span class="n">input_id_sub</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">dtype</span>
            <span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">segment</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">input_ids</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">input_id_sub</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">segment_rel_offset</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">segment_rel</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">num_segments</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">sample_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="n">batch</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">seqlen</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># calc segment bucket</span>
        <span class="n">segment_rel_2d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
            <span class="n">segment</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_segments</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="o">+</span> <span class="n">segment</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
            <span class="o">+</span> <span class="n">segment_rel_offset</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">],</span>
            <span class="o">~</span><span class="p">(</span>
                <span class="p">(</span><span class="n">sample_ids</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">sample_ids</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
                <span class="o">&amp;</span> <span class="p">(</span><span class="n">span</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">==</span> <span class="n">span</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="p">),</span>  <span class="c1"># not in the same span or sample</span>
            <span class="mi">0</span><span class="p">,</span>  <span class="c1"># avoid torch.gather overflow</span>
        <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seqlen</span> <span class="o">*</span> <span class="n">seqlen</span><span class="p">)</span>

        <span class="n">segment_bucket</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="n">segment_rel</span><span class="p">,</span>
            <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">index</span><span class="o">=</span><span class="n">segment_rel_2d</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span>
        <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">)</span>

        <span class="n">segment_bucket</span> <span class="o">=</span> <span class="n">segment_bucket</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
            <span class="o">~</span><span class="p">(</span>
                <span class="p">(</span><span class="n">sample_ids</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">sample_ids</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
                <span class="o">&amp;</span> <span class="p">(</span><span class="n">span</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">==</span> <span class="n">span</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="p">),</span>  <span class="c1"># not in the same span or sample</span>
            <span class="mi">1</span><span class="p">,</span>  <span class="c1"># bucket is used for in-context samples</span>
        <span class="p">)</span>

        <span class="c1"># directional mask</span>
        <span class="n">directional_mask_2d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seqlen</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
            <span class="n">seqlen</span>
        <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># sample mask</span>
        <span class="n">sample_mask_2d</span> <span class="o">=</span> <span class="p">(</span><span class="n">sample_ids</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span>
            <span class="n">sample_ids</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">sample_ids</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="c1"># context mask</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">context</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">|</span> <span class="p">(</span>
            <span class="n">context</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">logical_not</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">directional_mask_2d</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># span mask</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">attention_mask</span> <span class="o">&amp;</span> <span class="n">sample_mask_2d</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">span</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">==</span> <span class="n">span</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span>
        <span class="p">)</span>
        <span class="c1"># length mask</span>
        <span class="n">mask_1d</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seqlen</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">mask_1d</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">mask_1d</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">attention_mask</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seqlen</span><span class="p">),</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">))</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_embedding</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">input_id_sub</span><span class="p">)</span>
        <span class="n">position_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_bias</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">position</span><span class="p">,</span> <span class="n">segment_bucket</span><span class="p">)</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_key_values</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_attentions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">position_bias</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span>
                <span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_key_values</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_attentions</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">BaseModelOutputWithPast</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">present_key_values</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">all_attentions</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">inference</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">input_id_sub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_segments</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">segment</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">segment_rel_offset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">segment_rel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Perform inference using the CpmBeeModel.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeModel): An instance of the CpmBeeModel class.</span>
<span class="sd">            input_ids (mindspore.Tensor):</span>
<span class="sd">                The input tensor of shape (batch, seq_length) containing the input IDs.</span>
<span class="sd">            input_id_sub (Optional[mindspore.Tensor]):</span>
<span class="sd">                The optional input tensor of shape (batch, seq_length) containing the sub input IDs. Default is None.</span>
<span class="sd">            position (Optional[mindspore.Tensor]):</span>
<span class="sd">                The optional input tensor of shape (batch, seq_length) containing the position information.</span>
<span class="sd">                Default is None.</span>
<span class="sd">            context (Optional[mindspore.Tensor]):</span>
<span class="sd">                The optional input tensor of shape (batch, seq_length) containing the context information.</span>
<span class="sd">                Default is None.</span>
<span class="sd">            sample_ids (Optional[mindspore.Tensor]):</span>
<span class="sd">                The optional input tensor of shape (batch, seq_length) containing the sample IDs. Default is None.</span>
<span class="sd">            num_segments (Optional[mindspore.Tensor]):</span>
<span class="sd">                The optional input tensor of shape (batch, seq_length) containing the number of segments.</span>
<span class="sd">                Default is None.</span>
<span class="sd">            segment (Optional[mindspore.Tensor]):</span>
<span class="sd">                The optional input tensor of shape (batch, seq_length) containing the segment information.</span>
<span class="sd">                Default is None.</span>
<span class="sd">            segment_rel_offset (Optional[mindspore.Tensor]):</span>
<span class="sd">                The optional input tensor of shape (batch, seq_length) containing the segment relative offset.</span>
<span class="sd">                Default is None.</span>
<span class="sd">            segment_rel (Optional[mindspore.Tensor]):</span>
<span class="sd">                The optional input tensor of shape (batch, seq_length) containing the segment relative information.</span>
<span class="sd">                Default is None.</span>
<span class="sd">            past_states (Optional[Dict]):</span>
<span class="sd">                The optional dictionary containing the past states. Default is None.</span>
<span class="sd">            output_attentions (Optional[bool]):</span>
<span class="sd">                Whether to output attentions. If None, it uses the output_attentions from the model configuration.</span>
<span class="sd">                Default is None.</span>
<span class="sd">            output_hidden_states (Optional[bool]):</span>
<span class="sd">                Whether to output hidden states. If None, it uses the output_hidden_states from the model configuration.</span>
<span class="sd">                Default is None.</span>
<span class="sd">            past_key_values (Optional[List]): The optional list containing the past key values. Default is None.</span>
<span class="sd">            use_cache (Optional[bool]):</span>
<span class="sd">                Whether to use cache. If None, it uses the use_cache from the model configuration. Default is None.</span>
<span class="sd">            return_dict (Optional[bool]):</span>
<span class="sd">                Whether to return a dictionary. If None, it uses the use_return_dict from the model configuration.</span>
<span class="sd">                Default is None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            BaseModelOutputWithPast: An instance of BaseModelOutputWithPast containing the last hidden state,</span>
<span class="sd">                past key values, hidden states, and attentions.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>

        <span class="c1"># dummy setting for common tests</span>
        <span class="k">if</span> <span class="n">input_id_sub</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">dtype</span>
            <span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">segment</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">input_ids</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">input_id_sub</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">segment_rel_offset</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">segment_rel</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">num_segments</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">sample_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">past_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">present_position</span> <span class="o">=</span> <span class="n">position</span>
            <span class="n">present_context</span> <span class="o">=</span> <span class="n">context</span>
            <span class="n">present_sample_ids</span> <span class="o">=</span> <span class="n">sample_ids</span>
            <span class="n">present_num_segments</span> <span class="o">=</span> <span class="n">num_segments</span>
            <span class="n">present_segments</span> <span class="o">=</span> <span class="n">segment</span>
            <span class="n">present_buffer</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">present_position</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_states</span><span class="p">[</span><span class="s2">&quot;buffer_position&quot;</span><span class="p">],</span> <span class="n">position</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">present_context</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_states</span><span class="p">[</span><span class="s2">&quot;buffer_context&quot;</span><span class="p">],</span> <span class="n">context</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">present_sample_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_states</span><span class="p">[</span><span class="s2">&quot;buffer_sample_ids&quot;</span><span class="p">],</span> <span class="n">sample_ids</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">present_num_segments</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_states</span><span class="p">[</span><span class="s2">&quot;buffer_num_segments&quot;</span><span class="p">],</span> <span class="n">num_segments</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">present_segments</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_states</span><span class="p">[</span><span class="s2">&quot;buffer_segments&quot;</span><span class="p">],</span> <span class="n">segment</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">present_buffer</span> <span class="o">=</span> <span class="n">past_states</span><span class="p">[</span><span class="s2">&quot;buffer&quot;</span><span class="p">]</span>

        <span class="n">batch</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">len_q</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">len_buffer</span> <span class="o">=</span> <span class="n">present_position</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">segment_rel_2d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
            <span class="n">segment</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_segments</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="o">+</span> <span class="n">present_segments</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
            <span class="o">+</span> <span class="n">segment_rel_offset</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">],</span>
            <span class="o">~</span><span class="p">((</span><span class="n">sample_ids</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">present_sample_ids</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:])),</span>  <span class="c1"># not in the same sample</span>
            <span class="mi">0</span><span class="p">,</span>  <span class="c1"># avoid torch.gather overflow</span>
        <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">len_q</span> <span class="o">*</span> <span class="n">len_buffer</span><span class="p">)</span>

        <span class="n">segment_bucket</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="n">segment_rel</span><span class="p">,</span>
            <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">index</span><span class="o">=</span><span class="n">segment_rel_2d</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span>
        <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="n">len_buffer</span><span class="p">)</span>

        <span class="n">segment_bucket</span> <span class="o">=</span> <span class="n">segment_bucket</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
            <span class="o">~</span><span class="p">((</span><span class="n">sample_ids</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">present_sample_ids</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:])),</span>  <span class="c1"># not in the same span or sample</span>
            <span class="mi">1</span><span class="p">,</span>  <span class="c1"># bucket is used for in-context samples</span>
        <span class="p">)</span>

        <span class="c1"># directional mask</span>
        <span class="n">directional_mask_2d</span> <span class="o">=</span> <span class="n">present_position</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&lt;=</span> <span class="n">position</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="c1"># sample mask</span>
        <span class="n">sample_mask_2d</span> <span class="o">=</span> <span class="p">(</span><span class="n">sample_ids</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">sample_ids</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">present_sample_ids</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:])</span>
        <span class="c1"># context mask</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">present_context</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">|</span> <span class="p">(</span>
            <span class="n">context</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">logical_not</span><span class="p">()</span> <span class="o">&amp;</span> <span class="n">directional_mask_2d</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="n">len_buffer</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># span mask</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span> <span class="o">&amp;</span> <span class="n">sample_mask_2d</span>
        <span class="c1"># length mask</span>
        <span class="n">mask_1d</span> <span class="o">=</span> <span class="n">present_num_segments</span> <span class="o">!=</span> <span class="mi">0</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">mask_1d</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">len_buffer</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">attention_mask</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_embedding</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">input_id_sub</span><span class="p">)</span>
        <span class="n">position_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_bias</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">present_position</span><span class="p">,</span> <span class="n">segment_bucket</span><span class="p">)</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_key_values</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_attentions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">position_bias</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">present_buffer</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span>
                <span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_key_values</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_attentions</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">BaseModelOutputWithPast</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">present_key_values</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">all_attentions</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeModel</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes an instance of the CpmBeeModel class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The object instance.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object that contains the model settings.</p>
<ul>
<li>type: CpmBeeConfig</li>
<li>purpose: Specifies the model configuration.</li>
<li>restrictions: Must be an instance of CpmBeeConfig.</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig">CpmBeeConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes an instance of the CpmBeeModel class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The object instance.</span>
<span class="sd">        config (CpmBeeConfig):</span>
<span class="sd">            The configuration object that contains the model settings.</span>

<span class="sd">            - type: CpmBeeConfig</span>
<span class="sd">            - purpose: Specifies the model configuration.</span>
<span class="sd">            - restrictions: Must be an instance of CpmBeeConfig.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">half</span><span class="p">:</span>
        <span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">CpmBeeEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">CpmBeeEmbeddingExt</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">position_bias</span> <span class="o">=</span> <span class="n">CpmBeeBucketPositionBias</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeModel</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">input_id_sub</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_segments</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">segment</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">segment_rel_offset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">segment_rel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">span</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs the CpmBeeModel.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The object itself.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tensor of shape (batch, seq_length) containing the input IDs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_id_sub</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input tensor of shape (batch, seq_length) containing the sub input IDs. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>length</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input tensor of shape (batch,) containing the length of the input sequences.
Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>context</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input tensor of shape (batch, seq_length) containing the context. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sample_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input tensor of shape (batch, seq_length) containing the sample IDs. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_segments</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input tensor of shape (batch, seq_length) containing the number of segments.
Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>segment</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input tensor of shape (batch, seq_length) containing the segments. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>segment_rel_offset</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input tensor of shape (batch, seq_length) containing the segment relative offset.
Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>segment_rel</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input tensor of shape (batch, seq_length) containing the segment relative.
Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>span</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input dictionary containing span information. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional boolean flag indicating whether to output attentions. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[bool]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional boolean flag indicating whether to output hidden states. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[bool]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_key_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional list containing past key values. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional boolean flag indicating whether to use cache. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[bool]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional boolean flag indicating whether to return a dictionary. Defaults to None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[bool]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">input_id_sub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_segments</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">segment</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">segment_rel_offset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">segment_rel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">span</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs the CpmBeeModel.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The object itself.</span>
<span class="sd">        input_ids (mindspore.Tensor): The input tensor of shape (batch, seq_length) containing the input IDs.</span>
<span class="sd">        input_id_sub (Optional[mindspore.Tensor], optional):</span>
<span class="sd">            The optional input tensor of shape (batch, seq_length) containing the sub input IDs. Defaults to None.</span>
<span class="sd">        length (Optional[mindspore.Tensor], optional):</span>
<span class="sd">            The optional input tensor of shape (batch,) containing the length of the input sequences.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        context (Optional[mindspore.Tensor], optional):</span>
<span class="sd">            The optional input tensor of shape (batch, seq_length) containing the context. Defaults to None.</span>
<span class="sd">        sample_ids (Optional[mindspore.Tensor], optional):</span>
<span class="sd">            The optional input tensor of shape (batch, seq_length) containing the sample IDs. Defaults to None.</span>
<span class="sd">        num_segments (Optional[mindspore.Tensor], optional):</span>
<span class="sd">            The optional input tensor of shape (batch, seq_length) containing the number of segments.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        segment (Optional[mindspore.Tensor], optional):</span>
<span class="sd">            The optional input tensor of shape (batch, seq_length) containing the segments. Defaults to None.</span>
<span class="sd">        segment_rel_offset (Optional[mindspore.Tensor], optional):</span>
<span class="sd">            The optional input tensor of shape (batch, seq_length) containing the segment relative offset.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        segment_rel (Optional[mindspore.Tensor], optional):</span>
<span class="sd">            The optional input tensor of shape (batch, seq_length) containing the segment relative.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        span (Optional[Dict], optional):</span>
<span class="sd">            The optional input dictionary containing span information. Defaults to None.</span>
<span class="sd">        output_attentions (Optional[bool], optional):</span>
<span class="sd">            The optional boolean flag indicating whether to output attentions. Defaults to None.</span>
<span class="sd">        output_hidden_states (Optional[bool], optional):</span>
<span class="sd">            The optional boolean flag indicating whether to output hidden states. Defaults to None.</span>
<span class="sd">        past_key_values (Optional[List], optional):</span>
<span class="sd">            The optional list containing past key values. Defaults to None.</span>
<span class="sd">        use_cache (Optional[bool], optional):</span>
<span class="sd">            The optional boolean flag indicating whether to use cache. Defaults to None.</span>
<span class="sd">        return_dict (Optional[bool], optional):</span>
<span class="sd">            The optional boolean flag indicating whether to return a dictionary. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
    <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
    <span class="p">)</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>
    <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>

    <span class="c1"># dummy setting for common tests</span>
    <span class="k">if</span> <span class="n">input_id_sub</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">segment</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">input_ids</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">input_id_sub</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">segment_rel_offset</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">segment_rel</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">num_segments</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">sample_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

    <span class="n">batch</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">seqlen</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># calc segment bucket</span>
    <span class="n">segment_rel_2d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
        <span class="n">segment</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_segments</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="o">+</span> <span class="n">segment</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="o">+</span> <span class="n">segment_rel_offset</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">],</span>
        <span class="o">~</span><span class="p">(</span>
            <span class="p">(</span><span class="n">sample_ids</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">sample_ids</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="o">&amp;</span> <span class="p">(</span><span class="n">span</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">==</span> <span class="n">span</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="p">),</span>  <span class="c1"># not in the same span or sample</span>
        <span class="mi">0</span><span class="p">,</span>  <span class="c1"># avoid torch.gather overflow</span>
    <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seqlen</span> <span class="o">*</span> <span class="n">seqlen</span><span class="p">)</span>

    <span class="n">segment_bucket</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">segment_rel</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">index</span><span class="o">=</span><span class="n">segment_rel_2d</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span>
    <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">)</span>

    <span class="n">segment_bucket</span> <span class="o">=</span> <span class="n">segment_bucket</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
        <span class="o">~</span><span class="p">(</span>
            <span class="p">(</span><span class="n">sample_ids</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">sample_ids</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="o">&amp;</span> <span class="p">(</span><span class="n">span</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">==</span> <span class="n">span</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="p">),</span>  <span class="c1"># not in the same span or sample</span>
        <span class="mi">1</span><span class="p">,</span>  <span class="c1"># bucket is used for in-context samples</span>
    <span class="p">)</span>

    <span class="c1"># directional mask</span>
    <span class="n">directional_mask_2d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seqlen</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
        <span class="n">seqlen</span>
    <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># sample mask</span>
    <span class="n">sample_mask_2d</span> <span class="o">=</span> <span class="p">(</span><span class="n">sample_ids</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span>
        <span class="n">sample_ids</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">sample_ids</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="c1"># context mask</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">context</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">|</span> <span class="p">(</span>
        <span class="n">context</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">logical_not</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">directional_mask_2d</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="c1"># span mask</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">attention_mask</span> <span class="o">&amp;</span> <span class="n">sample_mask_2d</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">span</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">==</span> <span class="n">span</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span>
    <span class="p">)</span>
    <span class="c1"># length mask</span>
    <span class="n">mask_1d</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seqlen</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">mask_1d</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">mask_1d</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">attention_mask</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seqlen</span><span class="p">),</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">))</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_embedding</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">input_id_sub</span><span class="p">)</span>
    <span class="n">position_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_bias</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">position</span><span class="p">,</span> <span class="n">segment_bucket</span><span class="p">)</span>
    <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_key_values</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_attentions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">position_bias</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_key_values</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_attentions</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">BaseModelOutputWithPast</span><span class="p">(</span>
        <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="n">present_key_values</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
        <span class="n">attentions</span><span class="o">=</span><span class="n">all_attentions</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.get_input_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeModel</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.get_input_embeddings" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method retrieves the input embeddings for the CpmBeeModel.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CpmBeeModel class.
It is used to access the input embeddings for the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel">CpmBeeModel</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_embedding</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>The method returns the input embedding associated with the CpmBeeModel instance.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method retrieves the input embeddings for the CpmBeeModel.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CpmBeeModel): The instance of the CpmBeeModel class.</span>
<span class="sd">            It is used to access the input embeddings for the model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        input_embedding: The method returns the input embedding associated with the CpmBeeModel instance.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_embedding</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.inference" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeModel</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">input_id_sub</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_segments</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">segment</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">segment_rel_offset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">segment_rel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.inference" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Perform inference using the CpmBeeModel.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeModel class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel">CpmBeeModel</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tensor of shape (batch, seq_length) containing the input IDs.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_id_sub</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input tensor of shape (batch, seq_length) containing the sub input IDs. Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input tensor of shape (batch, seq_length) containing the position information.
Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>context</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input tensor of shape (batch, seq_length) containing the context information.
Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sample_ids</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input tensor of shape (batch, seq_length) containing the sample IDs. Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_segments</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input tensor of shape (batch, seq_length) containing the number of segments.
Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>segment</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input tensor of shape (batch, seq_length) containing the segment information.
Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>segment_rel_offset</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input tensor of shape (batch, seq_length) containing the segment relative offset.
Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>segment_rel</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional input tensor of shape (batch, seq_length) containing the segment relative information.
Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="mindspore.Tensor">Tensor</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional dictionary containing the past states. Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to output attentions. If None, it uses the output_attentions from the model configuration.
Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[bool]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to output hidden states. If None, it uses the output_hidden_states from the model configuration.
Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[bool]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_key_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The optional list containing the past key values. Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to use cache. If None, it uses the use_cache from the model configuration. Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[bool]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to return a dictionary. If None, it uses the use_return_dict from the model configuration.
Default is None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[bool]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>BaseModelOutputWithPast</code>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>An instance of BaseModelOutputWithPast containing the last hidden state,
past key values, hidden states, and attentions.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span>
<span class="normal">1445</span>
<span class="normal">1446</span>
<span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">inference</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">input_id_sub</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sample_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_segments</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">segment</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">segment_rel_offset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">segment_rel</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">past_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Perform inference using the CpmBeeModel.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CpmBeeModel): An instance of the CpmBeeModel class.</span>
<span class="sd">        input_ids (mindspore.Tensor):</span>
<span class="sd">            The input tensor of shape (batch, seq_length) containing the input IDs.</span>
<span class="sd">        input_id_sub (Optional[mindspore.Tensor]):</span>
<span class="sd">            The optional input tensor of shape (batch, seq_length) containing the sub input IDs. Default is None.</span>
<span class="sd">        position (Optional[mindspore.Tensor]):</span>
<span class="sd">            The optional input tensor of shape (batch, seq_length) containing the position information.</span>
<span class="sd">            Default is None.</span>
<span class="sd">        context (Optional[mindspore.Tensor]):</span>
<span class="sd">            The optional input tensor of shape (batch, seq_length) containing the context information.</span>
<span class="sd">            Default is None.</span>
<span class="sd">        sample_ids (Optional[mindspore.Tensor]):</span>
<span class="sd">            The optional input tensor of shape (batch, seq_length) containing the sample IDs. Default is None.</span>
<span class="sd">        num_segments (Optional[mindspore.Tensor]):</span>
<span class="sd">            The optional input tensor of shape (batch, seq_length) containing the number of segments.</span>
<span class="sd">            Default is None.</span>
<span class="sd">        segment (Optional[mindspore.Tensor]):</span>
<span class="sd">            The optional input tensor of shape (batch, seq_length) containing the segment information.</span>
<span class="sd">            Default is None.</span>
<span class="sd">        segment_rel_offset (Optional[mindspore.Tensor]):</span>
<span class="sd">            The optional input tensor of shape (batch, seq_length) containing the segment relative offset.</span>
<span class="sd">            Default is None.</span>
<span class="sd">        segment_rel (Optional[mindspore.Tensor]):</span>
<span class="sd">            The optional input tensor of shape (batch, seq_length) containing the segment relative information.</span>
<span class="sd">            Default is None.</span>
<span class="sd">        past_states (Optional[Dict]):</span>
<span class="sd">            The optional dictionary containing the past states. Default is None.</span>
<span class="sd">        output_attentions (Optional[bool]):</span>
<span class="sd">            Whether to output attentions. If None, it uses the output_attentions from the model configuration.</span>
<span class="sd">            Default is None.</span>
<span class="sd">        output_hidden_states (Optional[bool]):</span>
<span class="sd">            Whether to output hidden states. If None, it uses the output_hidden_states from the model configuration.</span>
<span class="sd">            Default is None.</span>
<span class="sd">        past_key_values (Optional[List]): The optional list containing the past key values. Default is None.</span>
<span class="sd">        use_cache (Optional[bool]):</span>
<span class="sd">            Whether to use cache. If None, it uses the use_cache from the model configuration. Default is None.</span>
<span class="sd">        return_dict (Optional[bool]):</span>
<span class="sd">            Whether to return a dictionary. If None, it uses the use_return_dict from the model configuration.</span>
<span class="sd">            Default is None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        BaseModelOutputWithPast: An instance of BaseModelOutputWithPast containing the last hidden state,</span>
<span class="sd">            past key values, hidden states, and attentions.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
    <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
    <span class="p">)</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>
    <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>

    <span class="c1"># dummy setting for common tests</span>
    <span class="k">if</span> <span class="n">input_id_sub</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">segment</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">input_ids</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">input_id_sub</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">segment_rel_offset</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">segment_rel</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">num_segments</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">sample_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">past_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">present_position</span> <span class="o">=</span> <span class="n">position</span>
        <span class="n">present_context</span> <span class="o">=</span> <span class="n">context</span>
        <span class="n">present_sample_ids</span> <span class="o">=</span> <span class="n">sample_ids</span>
        <span class="n">present_num_segments</span> <span class="o">=</span> <span class="n">num_segments</span>
        <span class="n">present_segments</span> <span class="o">=</span> <span class="n">segment</span>
        <span class="n">present_buffer</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">present_position</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_states</span><span class="p">[</span><span class="s2">&quot;buffer_position&quot;</span><span class="p">],</span> <span class="n">position</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">present_context</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_states</span><span class="p">[</span><span class="s2">&quot;buffer_context&quot;</span><span class="p">],</span> <span class="n">context</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">present_sample_ids</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_states</span><span class="p">[</span><span class="s2">&quot;buffer_sample_ids&quot;</span><span class="p">],</span> <span class="n">sample_ids</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">present_num_segments</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_states</span><span class="p">[</span><span class="s2">&quot;buffer_num_segments&quot;</span><span class="p">],</span> <span class="n">num_segments</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">present_segments</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_states</span><span class="p">[</span><span class="s2">&quot;buffer_segments&quot;</span><span class="p">],</span> <span class="n">segment</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">present_buffer</span> <span class="o">=</span> <span class="n">past_states</span><span class="p">[</span><span class="s2">&quot;buffer&quot;</span><span class="p">]</span>

    <span class="n">batch</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">len_q</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">len_buffer</span> <span class="o">=</span> <span class="n">present_position</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">segment_rel_2d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
        <span class="n">segment</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_segments</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="o">+</span> <span class="n">present_segments</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="o">+</span> <span class="n">segment_rel_offset</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">],</span>
        <span class="o">~</span><span class="p">((</span><span class="n">sample_ids</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">present_sample_ids</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:])),</span>  <span class="c1"># not in the same sample</span>
        <span class="mi">0</span><span class="p">,</span>  <span class="c1"># avoid torch.gather overflow</span>
    <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">len_q</span> <span class="o">*</span> <span class="n">len_buffer</span><span class="p">)</span>

    <span class="n">segment_bucket</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">segment_rel</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">index</span><span class="o">=</span><span class="n">segment_rel_2d</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span>
    <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="n">len_buffer</span><span class="p">)</span>

    <span class="n">segment_bucket</span> <span class="o">=</span> <span class="n">segment_bucket</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
        <span class="o">~</span><span class="p">((</span><span class="n">sample_ids</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">present_sample_ids</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:])),</span>  <span class="c1"># not in the same span or sample</span>
        <span class="mi">1</span><span class="p">,</span>  <span class="c1"># bucket is used for in-context samples</span>
    <span class="p">)</span>

    <span class="c1"># directional mask</span>
    <span class="n">directional_mask_2d</span> <span class="o">=</span> <span class="n">present_position</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&lt;=</span> <span class="n">position</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="c1"># sample mask</span>
    <span class="n">sample_mask_2d</span> <span class="o">=</span> <span class="p">(</span><span class="n">sample_ids</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">sample_ids</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">present_sample_ids</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:])</span>
    <span class="c1"># context mask</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">present_context</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">|</span> <span class="p">(</span>
        <span class="n">context</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">logical_not</span><span class="p">()</span> <span class="o">&amp;</span> <span class="n">directional_mask_2d</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="n">len_buffer</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="c1"># span mask</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span> <span class="o">&amp;</span> <span class="n">sample_mask_2d</span>
    <span class="c1"># length mask</span>
    <span class="n">mask_1d</span> <span class="o">=</span> <span class="n">present_num_segments</span> <span class="o">!=</span> <span class="mi">0</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">mask_1d</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">len_buffer</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">attention_mask</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_embedding</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">input_id_sub</span><span class="p">)</span>
    <span class="n">position_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_bias</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">present_position</span><span class="p">,</span> <span class="n">segment_bucket</span><span class="p">)</span>
    <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_key_values</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_attentions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">position_bias</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="n">present_buffer</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_key_values</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_attentions</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">BaseModelOutputWithPast</span><span class="p">(</span>
        <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="n">present_key_values</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
        <span class="n">attentions</span><span class="o">=</span><span class="n">all_attentions</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.set_input_embeddings" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeModel</span><span class="o">.</span><span class="n">set_input_embeddings</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeModel.set_input_embeddings" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>This method sets the input embeddings for the CpmBeeModel.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>embeddings</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input embeddings to be set for the model.
It can be of any type and should contain the necessary information for input embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method sets the input embeddings for the CpmBeeModel.</span>

<span class="sd">    Args:</span>
<span class="sd">        embeddings (object): The input embeddings to be set for the model.</span>
<span class="sd">            It can be of any type and should contain the necessary information for input embeddings.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">embeddings</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>CpmBeeOutput represents a neural network cell for processing hidden states, including dense transformation, dropout, and layer normalization.</p>
<p>This class inherits from nn.Module and provides methods for initializing the cell and forwarding the output based on the given input tensors.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput.dense">dense</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A dense layer for transforming the input hidden states.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.Linear">Linear</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput.LayerNorm">LayerNorm</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A layer normalization module for normalizing the hidden states.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.LayerNorm">LayerNorm</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput.dropout">dropout</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A dropout module for applying dropout to the hidden states.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.Dropout">Dropout</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">METHOD</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput.__init__" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput.__init__">__init__</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Initializes the CpmBeeOutput cell with the given configuration.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput.forward" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput.forward">forward</a></code></td>
              <td class="doc-function-details">
                <div class="doc-md-description">
                  <p>Constructs the output based on the input hidden states and input tensor.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span>
<span class="normal">957</span>
<span class="normal">958</span>
<span class="normal">959</span>
<span class="normal">960</span>
<span class="normal">961</span>
<span class="normal">962</span>
<span class="normal">963</span>
<span class="normal">964</span>
<span class="normal">965</span>
<span class="normal">966</span>
<span class="normal">967</span>
<span class="normal">968</span>
<span class="normal">969</span>
<span class="normal">970</span>
<span class="normal">971</span>
<span class="normal">972</span>
<span class="normal">973</span>
<span class="normal">974</span>
<span class="normal">975</span>
<span class="normal">976</span>
<span class="normal">977</span>
<span class="normal">978</span>
<span class="normal">979</span>
<span class="normal">980</span>
<span class="normal">981</span>
<span class="normal">982</span>
<span class="normal">983</span>
<span class="normal">984</span>
<span class="normal">985</span>
<span class="normal">986</span>
<span class="normal">987</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeOutput</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    CpmBeeOutput represents a neural network cell for processing hidden states, including dense transformation, dropout, and layer normalization.</span>

<span class="sd">    This class inherits from nn.Module and provides methods for initializing the cell and forwarding the output based on the given input tensors.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        dense (nn.Linear): A dense layer for transforming the input hidden states.</span>
<span class="sd">        LayerNorm (nn.LayerNorm): A layer normalization module for normalizing the hidden states.</span>
<span class="sd">        dropout (nn.Dropout): A dropout module for applying dropout to the hidden states.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__: Initializes the CpmBeeOutput cell with the given configuration.</span>
<span class="sd">        forward: Constructs the output based on the input hidden states and input tensor.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a CpmBeeOutput instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeOutput): The instance of the CpmBeeOutput class.</span>
<span class="sd">            config (object):</span>
<span class="sd">                The configuration object containing parameters for the model.</span>

<span class="sd">                - intermediate_size (int): The size of the intermediate layer.</span>
<span class="sd">                - hidden_size (int): The size of the hidden layer.</span>
<span class="sd">                - layer_norm_eps (float): The epsilon value for LayerNorm.</span>
<span class="sd">                - hidden_dropout_prob (float): The dropout probability for the hidden layer.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If the provided config object is not of the expected type.</span>
<span class="sd">            ValueError: If the config object is missing any required parameters.</span>
<span class="sd">            AttributeError: If there is an issue with accessing the attributes of the config object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">LayerNorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs the CpmBeeOutput.</span>

<span class="sd">        This method takes three parameters: self, hidden_states, and input_tensor. It returns a mindspore.Tensor object.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeOutput): An instance of the CpmBeeOutput class.</span>
<span class="sd">            hidden_states (mindspore.Tensor): The hidden states tensor.</span>
<span class="sd">                This tensor contains the hidden states from the previous layer.</span>
<span class="sd">            input_tensor (mindspore.Tensor): The input tensor.</span>
<span class="sd">                This tensor represents the input to the current layer.</span>

<span class="sd">        Returns:</span>
<span class="sd">            mindspore.Tensor: The forwarded tensor.</span>
<span class="sd">                This tensor is the result of applying the CpmBeeOutput layer operations.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_states</span> <span class="o">+</span> <span class="n">input_tensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeOutput</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a CpmBeeOutput instance.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CpmBeeOutput class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput">CpmBeeOutput</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration object containing parameters for the model.</p>
<ul>
<li>intermediate_size (int): The size of the intermediate layer.</li>
<li>hidden_size (int): The size of the hidden layer.</li>
<li>layer_norm_eps (float): The epsilon value for LayerNorm.</li>
<li>hidden_dropout_prob (float): The dropout probability for the hidden layer.</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>object</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>TypeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the provided config object is not of the expected type.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the config object is missing any required parameters.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>AttributeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If there is an issue with accessing the attributes of the config object.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span>
<span class="normal">957</span>
<span class="normal">958</span>
<span class="normal">959</span>
<span class="normal">960</span>
<span class="normal">961</span>
<span class="normal">962</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a CpmBeeOutput instance.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CpmBeeOutput): The instance of the CpmBeeOutput class.</span>
<span class="sd">        config (object):</span>
<span class="sd">            The configuration object containing parameters for the model.</span>

<span class="sd">            - intermediate_size (int): The size of the intermediate layer.</span>
<span class="sd">            - hidden_size (int): The size of the hidden layer.</span>
<span class="sd">            - layer_norm_eps (float): The epsilon value for LayerNorm.</span>
<span class="sd">            - hidden_dropout_prob (float): The dropout probability for the hidden layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the provided config object is not of the expected type.</span>
<span class="sd">        ValueError: If the config object is missing any required parameters.</span>
<span class="sd">        AttributeError: If there is an issue with accessing the attributes of the config object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">LayerNorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_eps</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeOutput</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs the CpmBeeOutput.</p>
<p>This method takes three parameters: self, hidden_states, and input_tensor. It returns a mindspore.Tensor object.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeOutput class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeOutput">CpmBeeOutput</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The hidden states tensor.
This tensor contains the hidden states from the previous layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_tensor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tensor.
This tensor represents the input to the current layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="mindspore.Tensor">Tensor</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>mindspore.Tensor: The forwarded tensor.
This tensor is the result of applying the CpmBeeOutput layer operations.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">964</span>
<span class="normal">965</span>
<span class="normal">966</span>
<span class="normal">967</span>
<span class="normal">968</span>
<span class="normal">969</span>
<span class="normal">970</span>
<span class="normal">971</span>
<span class="normal">972</span>
<span class="normal">973</span>
<span class="normal">974</span>
<span class="normal">975</span>
<span class="normal">976</span>
<span class="normal">977</span>
<span class="normal">978</span>
<span class="normal">979</span>
<span class="normal">980</span>
<span class="normal">981</span>
<span class="normal">982</span>
<span class="normal">983</span>
<span class="normal">984</span>
<span class="normal">985</span>
<span class="normal">986</span>
<span class="normal">987</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs the CpmBeeOutput.</span>

<span class="sd">    This method takes three parameters: self, hidden_states, and input_tensor. It returns a mindspore.Tensor object.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CpmBeeOutput): An instance of the CpmBeeOutput class.</span>
<span class="sd">        hidden_states (mindspore.Tensor): The hidden states tensor.</span>
<span class="sd">            This tensor contains the hidden states from the previous layer.</span>
<span class="sd">        input_tensor (mindspore.Tensor): The input tensor.</span>
<span class="sd">            This tensor represents the input to the current layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        mindspore.Tensor: The forwarded tensor.</span>
<span class="sd">            This tensor is the result of applying the CpmBeeOutput layer operations.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_states</span> <span class="o">+</span> <span class="n">input_tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeePreTrainedModel" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeePreTrainedModel</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeePreTrainedModel" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.modeling_utils.PreTrainedModel" href="../../../../../api/transformers/modeling_utils/#mindnlp.transformers.modeling_utils.PreTrainedModel">PreTrainedModel</a></code></p>


        <p>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
models.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeePreTrainedModel</span><span class="p">(</span><span class="n">PreTrainedModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained</span>
<span class="sd">    models.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">CpmBeeConfig</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;cpmbee&quot;</span>

    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cell</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the weights&quot;&quot;&quot;</span>
        <span class="n">std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">init_std</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="n">std</span><span class="p">),</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">cell</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">cell</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="c1"># still needed</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">CpmBeeEmbeddingExt</span><span class="p">):</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="n">std</span><span class="p">),</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">):</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;ones&#39;</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">CpmBeeLayerNorm</span><span class="p">):</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;ones&#39;</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">CpmBeeBucketPositionBias</span><span class="p">):</span>
            <span class="n">cell</span><span class="o">.</span><span class="n">relative_attention_bias</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span>
                <span class="n">Normal</span><span class="p">(</span><span class="n">std</span><span class="p">),</span> <span class="n">cell</span><span class="o">.</span><span class="n">relative_attention_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cell</span><span class="o">.</span><span class="n">relative_attention_bias</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeRotaryEmbedding" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeRotaryEmbedding</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeRotaryEmbedding" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>RotaryEmbedding embeds the unk token and special token. It will embeds the "...<mask>...<mask>...<unk>...<unk>..."
to "...<mask_0>...<mask_1>...<unk_0>...<unk_1>..."" to help model to specify different special tokens and unk
tokens.</p>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeRotaryEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    RotaryEmbedding embeds the unk token and special token. It will embeds the &quot;...&lt;mask&gt;...&lt;mask&gt;...&lt;unk&gt;...&lt;unk&gt;...&quot;</span>
<span class="sd">    to &quot;...&lt;mask_0&gt;...&lt;mask_1&gt;...&lt;unk_0&gt;...&lt;unk_1&gt;...&quot;&quot; to help model to specify different special tokens and unk</span>
<span class="sd">    tokens.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initializes a new instance of the CpmBeeRotaryEmbedding class.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the CpmBeeRotaryEmbedding class.</span>
<span class="sd">            config (CpmBeeConfig):</span>
<span class="sd">                An instance of the CpmBeeConfig class containing configuration parameters.</span>

<span class="sd">                - Purpose: Represents the configuration for the rotary embedding.</span>
<span class="sd">                - Restrictions: Must be a valid instance of the CpmBeeConfig class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">inv_freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distance_scale</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">distance_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inv_freq</span> <span class="o">=</span> <span class="n">inv_freq</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">x_pos</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructs a rotary embedding for a given input tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            self (CpmBeeRotaryEmbedding): An instance of the CpmBeeRotaryEmbedding class.</span>
<span class="sd">            x (mindspore.Tensor): The input tensor for which the rotary embedding is forwarded.</span>
<span class="sd">            x_pos (mindspore.Tensor): The positional encoding tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Raises:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">inv_freq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inv_freq</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">x_pos</span> <span class="o">=</span> <span class="n">x_pos</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance_scale</span>
        <span class="n">freqs</span> <span class="o">=</span> <span class="n">x_pos</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">inv_freq</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># (..., dim/2)</span>

        <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">freqs</span><span class="p">,</span> <span class="n">freqs</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (..., dim)</span>
        <span class="n">emb_cos</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">cos</span><span class="p">()</span>  <span class="c1"># (..., dim)</span>
        <span class="n">emb_sin</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">sin</span><span class="p">()</span>  <span class="c1"># (..., dim)</span>

        <span class="n">rotate_x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span> <span class="p">:],</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (..., dim)</span>

        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">emb_cos</span> <span class="o">+</span> <span class="n">rotate_x</span> <span class="o">*</span> <span class="n">emb_sin</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeRotaryEmbedding.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeRotaryEmbedding</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeRotaryEmbedding.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a new instance of the CpmBeeRotaryEmbedding class.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CpmBeeRotaryEmbedding class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeConfig class containing configuration parameters.</p>
<ul>
<li>Purpose: Represents the configuration for the rotary embedding.</li>
<li>Restrictions: Must be a valid instance of the CpmBeeConfig class.</li>
</ul>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig">CpmBeeConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Initializes a new instance of the CpmBeeRotaryEmbedding class.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the CpmBeeRotaryEmbedding class.</span>
<span class="sd">        config (CpmBeeConfig):</span>
<span class="sd">            An instance of the CpmBeeConfig class containing configuration parameters.</span>

<span class="sd">            - Purpose: Represents the configuration for the rotary embedding.</span>
<span class="sd">            - Restrictions: Must be a valid instance of the CpmBeeConfig class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="n">inv_freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">distance_scale</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">distance_scale</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inv_freq</span> <span class="o">=</span> <span class="n">inv_freq</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">ms_dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeRotaryEmbedding.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeRotaryEmbedding</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_pos</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeRotaryEmbedding.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Constructs a rotary embedding for a given input tensor.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeRotaryEmbedding class.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeRotaryEmbedding" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeRotaryEmbedding">CpmBeeRotaryEmbedding</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The input tensor for which the rotary embedding is forwarded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>x_pos</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The positional encoding tensor.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindspore.Tensor">Tensor</span></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">x_pos</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a rotary embedding for a given input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        self (CpmBeeRotaryEmbedding): An instance of the CpmBeeRotaryEmbedding class.</span>
<span class="sd">        x (mindspore.Tensor): The input tensor for which the rotary embedding is forwarded.</span>
<span class="sd">        x_pos (mindspore.Tensor): The positional encoding tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Raises:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inv_freq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inv_freq</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">x_pos</span> <span class="o">=</span> <span class="n">x_pos</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance_scale</span>
    <span class="n">freqs</span> <span class="o">=</span> <span class="n">x_pos</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">inv_freq</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># (..., dim/2)</span>

    <span class="n">emb</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">freqs</span><span class="p">,</span> <span class="n">freqs</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (..., dim)</span>
    <span class="n">emb_cos</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">cos</span><span class="p">()</span>  <span class="c1"># (..., dim)</span>
    <span class="n">emb_sin</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">sin</span><span class="p">()</span>  <span class="c1"># (..., dim)</span>

    <span class="n">rotate_x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span> <span class="p">:],</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (..., dim)</span>

    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">emb_cos</span> <span class="o">+</span> <span class="n">rotate_x</span> <span class="o">*</span> <span class="n">emb_sin</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeSelfAttentionBlock" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeSelfAttentionBlock</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeSelfAttentionBlock" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>Represents a self-attention block in the CpmBee model for transformer-based neural network architectures.
This class inherits from <code>nn.Module</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The configuration for the self-attention block.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig">CpmBeeConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the configuration is invalid.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeSelfAttentionBlock.layernorm_before_attention">layernorm_before_attention</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The layer normalization module before the self-attention block.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeLayerNorm">CpmBeeLayerNorm</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeSelfAttentionBlock.self_attention">self_attention</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The self-attention module.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeAttention">CpmBeeAttention</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeSelfAttentionBlock.dropout">dropout</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The dropout layer, if configured.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="mindnlp.core.nn.Dropout">Dropout</span> or None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the input tensors are of invalid shape or type.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Tuple[mindspore.Tensor, mindspore.Tensor, mindspore.Tensor]:
The updated hidden states, attention weights, and current key-value states.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeSelfAttentionBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Represents a self-attention block in the CpmBee model for transformer-based neural network architectures.</span>
<span class="sd">    This class inherits from `nn.Module`.</span>

<span class="sd">    Args:</span>
<span class="sd">        config (CpmBeeConfig): The configuration for the self-attention block.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the configuration is invalid.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        layernorm_before_attention (CpmBeeLayerNorm): The layer normalization module before the self-attention block.</span>
<span class="sd">        self_attention (CpmBeeAttention): The self-attention module.</span>
<span class="sd">        dropout (nn.Dropout or None): The dropout layer, if configured.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the input tensors are of invalid shape or type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[mindspore.Tensor, mindspore.Tensor, mindspore.Tensor]:</span>
<span class="sd">            The updated hidden states, attention weights, and current key-value states.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a CpmBeeSelfAttentionBlock instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The CpmBeeSelfAttentionBlock instance itself.</span>
<span class="sd">            config (CpmBeeConfig): An instance of CpmBeeConfig containing configuration parameters for the</span>
<span class="sd">                self-attention block.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_before_attention</span> <span class="o">=</span> <span class="n">CpmBeeLayerNorm</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">CpmBeeAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">position_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_states (`mindspore.Tensor` of shape `(batch, len_seq, dim_model)`):</span>
<span class="sd">                Input of transformer block(self-attention block). It can be the raw embedding of a batch of sequences.</span>
<span class="sd">            attention_mask (`mindspore.Tensor` of shape `(batch, len_seq, len_seq)`):</span>
<span class="sd">                Avoid invalid areas to participate in the calculation of self-attention.</span>
<span class="sd">            position_bias (`mindspore.Tensor` of shape `(batch, len_seq, len_seq)`):</span>
<span class="sd">                Provide positional information to self-attention block.</span>
<span class="sd">            output_attentions (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers.</span>
<span class="sd">            past_key_values (`Tuple(mindspore.Tensor)`, *optional*):</span>
<span class="sd">                Cached past key and value projection states.</span>
<span class="sd">            use_cache (`bool`, *optional*):</span>
<span class="sd">                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding</span>
<span class="sd">                (see `past_key_values`).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_before_attention</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span>
            <span class="n">outputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">position_bias</span><span class="p">,</span> <span class="n">output_attentions</span><span class="p">,</span> <span class="n">past_key_values</span><span class="p">,</span> <span class="n">use_cache</span>
        <span class="p">)</span>

        <span class="n">outputs</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">current_key_value</span> <span class="o">=</span> <span class="n">outputs</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1.05</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">current_key_value</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeSelfAttentionBlock.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeSelfAttentionBlock</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeSelfAttentionBlock.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p>Initializes a CpmBeeSelfAttentionBlock instance.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The CpmBeeSelfAttentionBlock instance itself.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of CpmBeeConfig containing configuration parameters for the
self-attention block.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig">CpmBeeConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a CpmBeeSelfAttentionBlock instance.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The CpmBeeSelfAttentionBlock instance itself.</span>
<span class="sd">        config (CpmBeeConfig): An instance of CpmBeeConfig containing configuration parameters for the</span>
<span class="sd">            self-attention block.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_before_attention</span> <span class="o">=</span> <span class="n">CpmBeeLayerNorm</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">CpmBeeAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeSelfAttentionBlock.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeSelfAttentionBlock</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">position_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeSelfAttentionBlock.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input of transformer block(self-attention block). It can be the raw embedding of a batch of sequences.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch, len_seq, dim_model)`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Avoid invalid areas to participate in the calculation of self-attention.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch, len_seq, len_seq)`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Provide positional information to self-attention block.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor` of shape `(batch, len_seq, len_seq)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the attentions tensors of all attention layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_key_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Cached past key and value projection states.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple(mindspore.Tensor)`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding
(see <code>past_key_values</code>).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">position_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        hidden_states (`mindspore.Tensor` of shape `(batch, len_seq, dim_model)`):</span>
<span class="sd">            Input of transformer block(self-attention block). It can be the raw embedding of a batch of sequences.</span>
<span class="sd">        attention_mask (`mindspore.Tensor` of shape `(batch, len_seq, len_seq)`):</span>
<span class="sd">            Avoid invalid areas to participate in the calculation of self-attention.</span>
<span class="sd">        position_bias (`mindspore.Tensor` of shape `(batch, len_seq, len_seq)`):</span>
<span class="sd">            Provide positional information to self-attention block.</span>
<span class="sd">        output_attentions (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the attentions tensors of all attention layers.</span>
<span class="sd">        past_key_values (`Tuple(mindspore.Tensor)`, *optional*):</span>
<span class="sd">            Cached past key and value projection states.</span>
<span class="sd">        use_cache (`bool`, *optional*):</span>
<span class="sd">            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding</span>
<span class="sd">            (see `past_key_values`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_before_attention</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">position_bias</span><span class="p">,</span> <span class="n">output_attentions</span><span class="p">,</span> <span class="n">past_key_values</span><span class="p">,</span> <span class="n">use_cache</span>
    <span class="p">)</span>

    <span class="n">outputs</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">current_key_value</span> <span class="o">=</span> <span class="n">outputs</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1.05</span>

    <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">current_key_value</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeTransformerBlock" class="doc doc-heading">
            <code>mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeTransformerBlock</code>


<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeTransformerBlock" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindnlp.core.nn.Module">Module</span></code></p>


        <p>This class represents a transformer block of the CPM-BEE model, which is a neural network architecture used for
natural language processing tasks. The CpmBeeTransformerBlock class inherits from nn.Module and contains
two sub-blocks: a self-attention block and a feed-forward neural network (FFN) block.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeTransformerBlock.config">config</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The configuration object for the CPM-BEE model.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig">CpmBeeConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeTransformerBlock.mask_att">mask_att</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A boolean flag indicating whether to apply masking to the self-attention block.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeTransformerBlock.mask_ffn">mask_ffn</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>A boolean flag indicating whether to apply masking to the feed-forward neural network block.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CpmBeeTransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class represents a transformer block of the CPM-BEE model, which is a neural network architecture used for</span>
<span class="sd">    natural language processing tasks. The CpmBeeTransformerBlock class inherits from nn.Module and contains</span>
<span class="sd">    two sub-blocks: a self-attention block and a feed-forward neural network (FFN) block.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        config (CpmBeeConfig): The configuration object for the CPM-BEE model.</span>
<span class="sd">        mask_att (bool): A boolean flag indicating whether to apply masking to the self-attention block.</span>
<span class="sd">        mask_ffn (bool): A boolean flag indicating whether to apply masking to the feed-forward neural network block.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">,</span> <span class="n">mask_att</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">mask_ffn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __init__</span>

<span class="sd">        Initializes a CpmBeeTransformerBlock instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: The instance of the CpmBeeTransformerBlock class.</span>
<span class="sd">            config (CpmBeeConfig): An instance of the CpmBeeConfig class containing configuration parameters.</span>
<span class="sd">            mask_att (bool, optional): A boolean indicating whether to mask attention. Defaults to False.</span>
<span class="sd">            mask_ffn (bool, optional): A boolean indicating whether to mask feed-forward network. Defaults to False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Raises:</span>
<span class="sd">            None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask_att</span> <span class="o">=</span> <span class="n">mask_att</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask_ffn</span> <span class="o">=</span> <span class="n">mask_ffn</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_att</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">self_att</span> <span class="o">=</span> <span class="n">CpmBeeSelfAttentionBlock</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_ffn</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">CpmBeeFFNBlock</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">position_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_states (`mindspore.Tensor`):</span>
<span class="sd">                Input to the layer of shape `(batch, seq_len, dim_model)`</span>
<span class="sd">            attention_mask (`mindspore.Tensor`):</span>
<span class="sd">                Avoid invalid areas to participate in the calculation of shape `(batch, seq_len, seq_len)`</span>
<span class="sd">            position_bias (`mindspore.Tensor`):</span>
<span class="sd">                Provides position information to attention mechanism of shape `(num_heads, seq_len, seq_len)`</span>
<span class="sd">            output_attentions (`bool`, *optional*):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers.</span>
<span class="sd">            past_key_values (`Tuple[mindspore.Tensor, mindspore.Tensor])`, *optional*):</span>
<span class="sd">                Cached past key and value projection states</span>
<span class="sd">            use_cache (`bool`, *optional*):</span>
<span class="sd">                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding</span>
<span class="sd">                (see `past_key_values`).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_att</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_att</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">position_bias</span><span class="o">=</span><span class="n">position_bias</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
                <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">current_key_value</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attn_weights</span><span class="p">,</span> <span class="n">current_key_value</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_ffn</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">current_key_value</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeTransformerBlock.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTransformerBlock</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">mask_att</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mask_ffn</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeTransformerBlock.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">

        <p><strong>init</strong></p>
<p>Initializes a CpmBeeTransformerBlock instance.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>self</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instance of the CpmBeeTransformerBlock class.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>config</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An instance of the CpmBeeConfig class containing configuration parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><a class="autorefs autorefs-internal" title="mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig" href="../../../../../api/transformers/models/cpmbee/#mindnlp.transformers.models.cpmbee.configuration_cpmbee.CpmBeeConfig">CpmBeeConfig</a></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mask_att</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A boolean indicating whether to mask attention. Defaults to False.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mask_ffn</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A boolean indicating whether to mask feed-forward network. Defaults to False.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>None.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CpmBeeConfig</span><span class="p">,</span> <span class="n">mask_att</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">mask_ffn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    __init__</span>

<span class="sd">    Initializes a CpmBeeTransformerBlock instance.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: The instance of the CpmBeeTransformerBlock class.</span>
<span class="sd">        config (CpmBeeConfig): An instance of the CpmBeeConfig class containing configuration parameters.</span>
<span class="sd">        mask_att (bool, optional): A boolean indicating whether to mask attention. Defaults to False.</span>
<span class="sd">        mask_ffn (bool, optional): A boolean indicating whether to mask feed-forward network. Defaults to False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mask_att</span> <span class="o">=</span> <span class="n">mask_att</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mask_ffn</span> <span class="o">=</span> <span class="n">mask_ffn</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_att</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_att</span> <span class="o">=</span> <span class="n">CpmBeeSelfAttentionBlock</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_ffn</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">CpmBeeFFNBlock</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeTransformerBlock.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindnlp</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">cpmbee</span><span class="o">.</span><span class="n">modeling_cpmbee</span><span class="o">.</span><span class="n">CpmBeeTransformerBlock</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">position_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindnlp.transformers.models.cpmbee.modeling_cpmbee.CpmBeeTransformerBlock.forward" class="headerlink" title="Permanent link">&para;</a></h4>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input to the layer of shape <code>(batch, seq_len, dim_model)</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Avoid invalid areas to participate in the calculation of shape <code>(batch, seq_len, seq_len)</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Provides position information to attention mechanism of shape <code>(num_heads, seq_len, seq_len)</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.Tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_attentions</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return the attentions tensors of all attention layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>past_key_values</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Cached past key and value projection states</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple[mindspore.Tensor, mindspore.Tensor])`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_cache</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding
(see <code>past_key_values</code>).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindnlp\transformers\models\cpmbee\modeling_cpmbee.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">position_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        hidden_states (`mindspore.Tensor`):</span>
<span class="sd">            Input to the layer of shape `(batch, seq_len, dim_model)`</span>
<span class="sd">        attention_mask (`mindspore.Tensor`):</span>
<span class="sd">            Avoid invalid areas to participate in the calculation of shape `(batch, seq_len, seq_len)`</span>
<span class="sd">        position_bias (`mindspore.Tensor`):</span>
<span class="sd">            Provides position information to attention mechanism of shape `(num_heads, seq_len, seq_len)`</span>
<span class="sd">        output_attentions (`bool`, *optional*):</span>
<span class="sd">            Whether or not to return the attentions tensors of all attention layers.</span>
<span class="sd">        past_key_values (`Tuple[mindspore.Tensor, mindspore.Tensor])`, *optional*):</span>
<span class="sd">            Cached past key and value projection states</span>
<span class="sd">        use_cache (`bool`, *optional*):</span>
<span class="sd">            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding</span>
<span class="sd">            (see `past_key_values`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_att</span><span class="p">:</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_att</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">position_bias</span><span class="o">=</span><span class="n">position_bias</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">current_key_value</span> <span class="o">=</span> <span class="n">hidden_states</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">attn_weights</span><span class="p">,</span> <span class="n">current_key_value</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_ffn</span><span class="p">:</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">current_key_value</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="页脚" >
        
          
          <a href="../cpmant/" class="md-footer__link md-footer__link--prev" aria-label="上一页: cpmant">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                上一页
              </span>
              <div class="md-ellipsis">
                cpmant
              </div>
            </div>
          </a>
        
        
          
          <a href="../ctrl/" class="md-footer__link md-footer__link--next" aria-label="下一页: ctrl">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                下一页
              </span>
              <div class="md-ellipsis">
                ctrl
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 - 2024 MindSpore Lab and CQU NLP Team.
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:lvyufeng@cqu.edu.cn" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindnlp" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/lu-yu-feng-46-1" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.indexes", "navigation.top", "navigation.footer", "navigation.path", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>